{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 (Recurrent Neural Networks)\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "循环神经网络(RNN)是一种专门用于处理序列数据的神经网络架构，通过内部的循环连接来保持对历史信息的记忆。\n",
    "\n",
    "### 核心特性\n",
    "\n",
    "1. **时序记忆能力** (Temporal Memory)\n",
    "   - 具有内部状态(隐藏状态)来保存历史信息\n",
    "   - 当前输出依赖于历史输入序列\n",
    "   - 适合处理时间依赖性强的数据\n",
    "\n",
    "2. **参数共享** (Parameter Sharing)\n",
    "   - 同一组权重在所有时间步上复用\n",
    "   - 可以处理任意长度的序列\n",
    "   - 大幅减少参数数量\n",
    "\n",
    "3. **序列到序列映射**\n",
    "   - **多对一**: 序列分类(情感分析)\n",
    "   - **一对多**: 序列生成(图像描述)\n",
    "   - **多对多(同步)**: 序列标注(词性标注)\n",
    "   - **多对多(异步)**: 序列翻译(机器翻译)\n",
    "\n",
    "### 三种主流RNN架构\n",
    "\n",
    "| 架构 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|----------|\n",
    "| **SimpleRNN** | 简单易懂 | 梯度消失严重，无法学习长期依赖 | 短序列、教学演示 |\n",
    "| **LSTM** | 能学习长期依赖，性能稳定 | 参数多，训练慢 | 长序列、复杂任务 |\n",
    "| **GRU** | 参数少于LSTM，训练快 | 性能略低于LSTM | 计算资源受限、实时应用 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备与数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(f\"TensorFlow版本: {tf.__version__}\")\n",
    "print(f\"GPU可用: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. SimpleRNN - 基础循环神经网络\n",
    "\n",
    "### 原理\n",
    "\n",
    "SimpleRNN是最基础的RNN结构，每个时间步的隐藏状态由当前输入和上一时刻隐藏状态共同决定：\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "### 梯度消失问题\n",
    "\n",
    "由于tanh激活函数的导数范围在(0,1)，经过多次连乘后梯度会指数级衰减，导致：\n",
    "- 难以学习长期依赖\n",
    "- 只适合短序列(10-20步)\n",
    "- 训练不稳定\n",
    "\n",
    "### 演示任务：序列分类\n",
    "\n",
    "生成一个简单的二分类任务：判断序列均值是否大于0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_classification_data(n_samples=5000, seq_length=20, n_features=1):\n",
    "    \"\"\"\n",
    "    生成序列分类数据\n",
    "    规则：序列均值 > 0.5 → 标签1，否则 → 标签0\n",
    "    \"\"\"\n",
    "    X = np.random.rand(n_samples, seq_length, n_features)\n",
    "    y = (X.mean(axis=1).flatten() > 0.5).astype(int)\n",
    "    return X, y\n",
    "\n",
    "# 生成数据\n",
    "X_seq, y_seq = generate_sequence_classification_data()\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "print(f\"训练集形状: {X_train_seq.shape}\")  # (4000, 20, 1)\n",
    "print(f\"序列长度: {X_train_seq.shape[1]}\")\n",
    "print(f\"特征维度: {X_train_seq.shape[2]}\")\n",
    "print(f\"类别分布: {np.bincount(y_train_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_rnn(seq_length, n_features, rnn_units=32):\n",
    "    \"\"\"\n",
    "    构建SimpleRNN分类模型\n",
    "    \n",
    "    参数:\n",
    "        seq_length: 序列长度\n",
    "        n_features: 每个时间步的特征数\n",
    "        rnn_units: RNN隐藏单元数\n",
    "    \n",
    "    返回:\n",
    "        编译后的模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='SimpleRNN_Classifier')\n",
    "    \n",
    "    # SimpleRNN层\n",
    "    # return_sequences=False: 只返回最后一个时间步的输出\n",
    "    # 用于序列到单值的映射(序列分类)\n",
    "    model.add(layers.SimpleRNN(\n",
    "        rnn_units,\n",
    "        activation='tanh',\n",
    "        input_shape=(seq_length, n_features),\n",
    "        return_sequences=False  # 多对一映射\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建并编译模型\n",
    "model_simple_rnn = build_simple_rnn(\n",
    "    seq_length=X_train_seq.shape[1],\n",
    "    n_features=X_train_seq.shape[2]\n",
    ")\n",
    "\n",
    "model_simple_rnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "history_simple_rnn = model_simple_rnn.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    batch_size=128,\n",
    "    epochs=100,  # 测试时用2个epoch\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "loss, acc = model_simple_rnn.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "print(f\"\\nSimpleRNN测试精度: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LSTM - 长短期记忆网络\n",
    "\n",
    "### 核心机制\n",
    "\n",
    "LSTM通过引入**门控机制**来解决梯度消失问题，包含三个门：\n",
    "\n",
    "1. **遗忘门 (Forget Gate)**: 决定丢弃哪些历史信息\n",
    "   $$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **输入门 (Input Gate)**: 决定更新哪些信息\n",
    "   $$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$\n",
    "   $$\\tilde{C}_t = \\tanh(W_C [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3. **输出门 (Output Gate)**: 决定输出哪些信息\n",
    "   $$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**细胞状态更新**:\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "### 优势\n",
    "- 可以学习长期依赖(100+时间步)\n",
    "- 梯度流动更稳定\n",
    "- 性能稳定可靠\n",
    "\n",
    "### 演示任务：IMDB情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载IMDB数据集（电影评论情感分类）\n",
    "max_features = 10000  # 词汇表大小\n",
    "maxlen = 200  # 序列最大长度\n",
    "\n",
    "(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "\n",
    "print(f\"训练样本数: {len(X_train_imdb)}\")\n",
    "print(f\"测试样本数: {len(X_test_imdb)}\")\n",
    "print(f\"原始序列长度示例: {len(X_train_imdb[0])}\")\n",
    "\n",
    "# 填充/截断序列到固定长度\n",
    "X_train_imdb = sequence.pad_sequences(X_train_imdb, maxlen=maxlen)\n",
    "X_test_imdb = sequence.pad_sequences(X_test_imdb, maxlen=maxlen)\n",
    "\n",
    "print(f\"填充后形状: {X_train_imdb.shape}\")  # (25000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(max_features, maxlen, lstm_units=64, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    构建LSTM情感分析模型\n",
    "    \n",
    "    参数:\n",
    "        max_features: 词汇表大小\n",
    "        maxlen: 序列长度\n",
    "        lstm_units: LSTM隐藏单元数\n",
    "        embedding_dim: 词嵌入维度\n",
    "    \n",
    "    返回:\n",
    "        编译后的模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='LSTM_Sentiment')\n",
    "    \n",
    "    # Embedding层：将整数序列转为稠密向量\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ))\n",
    "    \n",
    "    # 单层LSTM\n",
    "    model.add(layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=False  # 只需要最后的输出\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建并编译LSTM模型\n",
    "model_lstm = build_lstm_model(max_features, maxlen)\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练LSTM模型（使用简化参数）\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    batch_size=128,\n",
    "    epochs=10,  # 测试时用1个epoch，实际训练建议5-10\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "loss, acc = model_lstm.evaluate(X_test_imdb, y_test_imdb, verbose=0)\n",
    "print(f\"\\nLSTM测试精度: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GRU - 门控循环单元\n",
    "\n",
    "### 核心机制\n",
    "\n",
    "GRU是LSTM的简化版本，将遗忘门和输入门合并为**更新门**，去掉了细胞状态：\n",
    "\n",
    "1. **重置门 (Reset Gate)**: 决定保留多少历史信息\n",
    "   $$r_t = \\sigma(W_r [h_{t-1}, x_t])$$\n",
    "\n",
    "2. **更新门 (Update Gate)**: 决定更新多少新信息\n",
    "   $$z_t = \\sigma(W_z [h_{t-1}, x_t])$$\n",
    "\n",
    "**隐藏状态更新**:\n",
    "$$\\tilde{h}_t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t])$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "### 优势\n",
    "- 参数量约为LSTM的75%\n",
    "- 训练速度更快\n",
    "- 性能接近LSTM\n",
    "- 更适合实时应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(max_features, maxlen, gru_units=64, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    构建GRU情感分析模型\n",
    "    \n",
    "    参数:\n",
    "        max_features: 词汇表大小\n",
    "        maxlen: 序列长度\n",
    "        gru_units: GRU隐藏单元数\n",
    "        embedding_dim: 词嵌入维度\n",
    "    \n",
    "    返回:\n",
    "        编译后的模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='GRU_Sentiment')\n",
    "    \n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ))\n",
    "    \n",
    "    # 单层GRU\n",
    "    model.add(layers.GRU(\n",
    "        gru_units,\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建并编译GRU模型\n",
    "model_gru = build_gru_model(max_features, maxlen)\n",
    "\n",
    "model_gru.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_gru.summary()\n",
    "\n",
    "# 参数量对比\n",
    "lstm_params = model_lstm.count_params()\n",
    "gru_params = model_gru.count_params()\n",
    "print(f\"\\nLSTM参数量: {lstm_params:,}\")\n",
    "print(f\"GRU参数量: {gru_params:,}\")\n",
    "print(f\"参数减少: {(1 - gru_params/lstm_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练GRU模型\n",
    "history_gru = model_gru.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    batch_size=128,\n",
    "    epochs=10,  # 测试时用1个epoch\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "loss, acc = model_gru.evaluate(X_test_imdb, y_test_imdb, verbose=0)\n",
    "print(f\"\\nGRU测试精度: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 堆叠RNN - 多层RNN架构\n",
    "\n",
    "### 关键配置\n",
    "\n",
    "**重要**: 堆叠多层RNN时，**除了最后一层，前面所有层都必须设置 `return_sequences=True`**\n",
    "\n",
    "- 前面的层需要返回完整序列供下一层处理\n",
    "- 最后一层根据任务决定：\n",
    "  - 序列分类: `return_sequences=False`\n",
    "  - 序列标注: `return_sequences=True`\n",
    "\n",
    "### 优势\n",
    "- 增强模型表达能力\n",
    "- 学习更抽象的特征\n",
    "- 提升复杂任务性能\n",
    "\n",
    "### 注意事项\n",
    "- 层数过多容易过拟合\n",
    "- 训练时间显著增加\n",
    "- 通常2-3层已足够"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_lstm(max_features, maxlen, lstm_units=[64, 32], embedding_dim=128):\n",
    "    \"\"\"\n",
    "    构建多层堆叠LSTM模型\n",
    "    \n",
    "    参数:\n",
    "        max_features: 词汇表大小\n",
    "        maxlen: 序列长度\n",
    "        lstm_units: 每层LSTM的单元数列表\n",
    "        embedding_dim: 词嵌入维度\n",
    "    \n",
    "    返回:\n",
    "        编译后的模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Stacked_LSTM')\n",
    "    \n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ))\n",
    "    \n",
    "    # 堆叠多层LSTM\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        # 除了最后一层，其他层都要返回完整序列\n",
    "        return_seq = (i < len(lstm_units) - 1)\n",
    "        model.add(layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=return_seq,\n",
    "            name=f'lstm_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建3层堆叠LSTM\n",
    "model_stacked = build_stacked_lstm(\n",
    "    max_features, \n",
    "    maxlen, \n",
    "    lstm_units=[64, 32, 16]\n",
    ")\n",
    "\n",
    "model_stacked.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_stacked.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练堆叠LSTM\n",
    "history_stacked = model_stacked.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    batch_size=128,\n",
    "    epochs=10,  # 测试时用1个epoch\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "loss, acc = model_stacked.evaluate(X_test_imdb, y_test_imdb, verbose=0)\n",
    "print(f\"\\n堆叠LSTM测试精度: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 双向RNN - Bidirectional RNN\n",
    "\n",
    "### 原理\n",
    "\n",
    "双向RNN同时从前向后和从后向前处理序列，然后合并两个方向的信息：\n",
    "\n",
    "- **前向RNN**: $\\overrightarrow{h}_t = f(\\overrightarrow{h}_{t-1}, x_t)$\n",
    "- **后向RNN**: $\\overleftarrow{h}_t = f(\\overleftarrow{h}_{t+1}, x_t)$\n",
    "- **输出**: $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$ (拼接)\n",
    "\n",
    "### 优势\n",
    "- 同时利用过去和未来的上下文\n",
    "- 提升序列理解能力\n",
    "- 特别适合文本分类、命名实体识别等任务\n",
    "\n",
    "### 注意事项\n",
    "- 参数量翻倍\n",
    "- 不适合实时预测（需要完整序列）\n",
    "- 适合离线批处理任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_lstm(max_features, maxlen, lstm_units=64, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    构建双向LSTM模型\n",
    "    \n",
    "    参数:\n",
    "        max_features: 词汇表大小\n",
    "        maxlen: 序列长度\n",
    "        lstm_units: LSTM隐藏单元数\n",
    "        embedding_dim: 词嵌入维度\n",
    "    \n",
    "    返回:\n",
    "        编译后的模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Bidirectional_LSTM')\n",
    "    \n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ))\n",
    "    \n",
    "    # 双向LSTM\n",
    "    # 输出维度会翻倍: 2 * lstm_units\n",
    "    model.add(layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units, return_sequences=False)\n",
    "    ))\n",
    "    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建双向LSTM\n",
    "model_bidirectional = build_bidirectional_lstm(max_features, maxlen)\n",
    "\n",
    "model_bidirectional.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_bidirectional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练双向LSTM\n",
    "history_bidirectional = model_bidirectional.fit(\n",
    "    X_train_imdb, y_train_imdb,\n",
    "    batch_size=128,\n",
    "    epochs=10,  # 测试时用1个epoch\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估\n",
    "loss, acc = model_bidirectional.evaluate(X_test_imdb, y_test_imdb, verbose=0)\n",
    "print(f\"\\n双向LSTM测试精度: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 时间序列预测示例\n",
    "\n",
    "演示RNN在时间序列预测中的应用：使用历史数据预测未来值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_data(n_samples=1000, seq_length=50):\n",
    "    \"\"\"\n",
    "    生成合成时间序列数据：正弦波 + 噪声\n",
    "    \"\"\"\n",
    "    time = np.arange(n_samples + seq_length + 1)\n",
    "    series = np.sin(time * 0.1) + np.random.randn(len(time)) * 0.1\n",
    "    \n",
    "    # 创建输入输出对\n",
    "    X, y = [], []\n",
    "    for i in range(n_samples):\n",
    "        X.append(series[i:i+seq_length])\n",
    "        y.append(series[i+seq_length])  # 预测下一个值\n",
    "    \n",
    "    return np.array(X).reshape(-1, seq_length, 1), np.array(y)\n",
    "\n",
    "# 生成数据\n",
    "X_ts, y_ts = generate_time_series_data()\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(\n",
    "    X_ts, y_ts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"训练集形状: {X_train_ts.shape}\")\n",
    "print(f\"目标形状: {y_train_ts.shape}\")\n",
    "\n",
    "# 可视化部分序列\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(X_train_ts[0].flatten(), label='输入序列')\n",
    "plt.axvline(x=len(X_train_ts[0])-1, color='r', linestyle='--')\n",
    "plt.scatter([len(X_train_ts[0])], [y_train_ts[0]], color='r', s=100, label='预测目标')\n",
    "plt.xlabel('时间步')\n",
    "plt.ylabel('值')\n",
    "plt.title('时间序列预测任务示例')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_forecaster(seq_length, n_features=1, lstm_units=50):\n",
    "    \"\"\"\n",
    "    构建LSTM时间序列预测模型\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='LSTM_Forecaster')\n",
    "    \n",
    "    model.add(layers.LSTM(\n",
    "        lstm_units,\n",
    "        return_sequences=True,\n",
    "        input_shape=(seq_length, n_features)\n",
    "    ))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.LSTM(lstm_units, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(25, activation='relu'))\n",
    "    model.add(layers.Dense(1))  # 回归任务，无激活函数\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建并训练预测模型\n",
    "model_forecaster = build_lstm_forecaster(seq_length=X_train_ts.shape[1])\n",
    "\n",
    "model_forecaster.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "history_forecaster = model_forecaster.fit(\n",
    "    X_train_ts, y_train_ts,\n",
    "    batch_size=32,\n",
    "    epochs=100,  # 测试时用2个epoch\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 预测并可视化\n",
    "y_pred_ts = model_forecaster.predict(X_test_ts[:100], verbose=0).flatten()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y_test_ts[:100], label='真实值', alpha=0.7)\n",
    "plt.plot(y_pred_ts, label='预测值', alpha=0.7)\n",
    "plt.xlabel('样本')\n",
    "plt.ylabel('值')\n",
    "plt.title('时间序列预测结果对比')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 关键知识总结\n",
    "\n",
    "### RNN架构对比\n",
    "\n",
    "| 特性 | SimpleRNN | LSTM | GRU |\n",
    "|------|-----------|------|-----|\n",
    "| **参数量** | 小 | 大(4倍) | 中(3倍) |\n",
    "| **训练速度** | 快 | 慢 | 中 |\n",
    "| **长期依赖** | 差 | 优秀 | 良好 |\n",
    "| **适用序列长度** | <20 | 100+ | 50+ |\n",
    "| **门控数量** | 0 | 3 | 2 |\n",
    "| **推荐场景** | 教学/短序列 | 复杂任务 | 性能/速度平衡 |\n",
    "\n",
    "### return_sequences参数说明\n",
    "\n",
    "| 配置 | 输出形状 | 适用场景 |\n",
    "|------|---------|----------|\n",
    "| `return_sequences=False` | `(batch, units)` | 序列分类、序列回归 |\n",
    "| `return_sequences=True` | `(batch, timesteps, units)` | 序列标注、堆叠RNN、Seq2Seq |\n",
    "\n",
    "**关键规则**:\n",
    "- 堆叠RNN时，除最后一层外都必须 `return_sequences=True`\n",
    "- 序列到序列任务(如翻译)，编码器最后一层 `False`，解码器所有层 `True`\n",
    "\n",
    "### 输入数据格式\n",
    "\n",
    "RNN要求输入为3D张量: `(batch_size, timesteps, features)`\n",
    "\n",
    "- `batch_size`: 批量大小\n",
    "- `timesteps`: 序列长度(时间步数)\n",
    "- `features`: 每个时间步的特征维度\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "# 100个样本，每个序列50个时间步，每个时间步1个特征\n",
    "X = np.random.randn(100, 50, 1)\n",
    "```\n",
    "\n",
    "### 常见应用场景\n",
    "\n",
    "1. **自然语言处理**\n",
    "   - 情感分析\n",
    "   - 机器翻译\n",
    "   - 文本生成\n",
    "   - 命名实体识别\n",
    "\n",
    "2. **时间序列分析**\n",
    "   - 股票价格预测\n",
    "   - 天气预报\n",
    "   - 能源负载预测\n",
    "   - 异常检测\n",
    "\n",
    "3. **语音处理**\n",
    "   - 语音识别\n",
    "   - 语音合成\n",
    "   - 说话人识别\n",
    "\n",
    "4. **其他**\n",
    "   - 视频分析\n",
    "   - 手势识别\n",
    "   - 音乐生成\n",
    "\n",
    "### 优化技巧\n",
    "\n",
    "1. **梯度裁剪** (Gradient Clipping)\n",
    "   ```python\n",
    "   optimizer = keras.optimizers.Adam(clipnorm=1.0)\n",
    "   ```\n",
    "\n",
    "2. **学习率调度**\n",
    "   ```python\n",
    "   callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)]\n",
    "   ```\n",
    "\n",
    "3. **正则化**\n",
    "   - Dropout (0.2-0.5)\n",
    "   - Recurrent Dropout\n",
    "   - 层归一化\n",
    "\n",
    "4. **批处理**\n",
    "   - 使用较大batch_size (64-256)\n",
    "   - 序列填充/截断到固定长度\n",
    "\n",
    "---\n",
    "\n",
    "## 完整训练配置模板\n",
    "\n",
    "```python\n",
    "# 实际项目中建议使用的完整配置\n",
    "\n",
    "# 优化器配置\n",
    "# optimizer = keras.optimizers.Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     clipnorm=1.0  # 梯度裁剪\n",
    "# )\n",
    "\n",
    "# 回调函数\n",
    "# callbacks = [\n",
    "#     keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_loss',\n",
    "#         patience=10,\n",
    "#         restore_best_weights=True\n",
    "#     ),\n",
    "#     keras.callbacks.ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=0.5,\n",
    "#         patience=5,\n",
    "#         min_lr=1e-7\n",
    "#     ),\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         'best_rnn_model.h5',\n",
    "#         monitor='val_accuracy',\n",
    "#         save_best_only=True\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# 训练\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=128,\n",
    "#     epochs=50,\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=callbacks,\n",
    "#     verbose=1\n",
    "# )\n",
    "```\n",
    "\n",
    "### 性能对比结论\n",
    "\n",
    "在大多数任务中：\n",
    "- **首选LSTM**: 复杂任务、长序列、追求最佳性能\n",
    "- **次选GRU**: 资源受限、实时应用、性能要求适中\n",
    "- **避免SimpleRNN**: 仅用于教学或极短序列"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}