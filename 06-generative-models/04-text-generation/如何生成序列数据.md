# 序列文本生成的采样策略

## 1. 采样方法概述

在语言模型生成文本时，模型会输出一个概率分布 P(w|context)，表示在给定上下文的情况下，每个词出现的概率。如何从这个概率分布中选择下一个词，直接影响生成文本的质量和多样性。

## 2. 基础采样策略

### 2.1 贪婪采样 (Greedy Sampling)

**原理**：每次选择概率最高的词作为输出。

**数学表示**：
```
w_t = argmax P(w|w_1, w_2, ..., w_{t-1})
```

**特点**：
- **优点**：计算效率高，输出确定性强
- **缺点**：容易陷入重复模式，缺乏多样性，可能产生"退化"文本
- **适用场景**：需要高度准确性的任务（如机器翻译、摘要生成）

### 2.2 随机采样 (Random Sampling)

**原理**：根据模型输出的概率分布进行随机采样。

**数学表示**：
```
w_t ~ P(w|w_1, w_2, ..., w_{t-1})
```

**特点**：
- **优点**：生成多样性高，避免重复
- **缺点**：可能采样到低概率词，导致文本不连贯或出现语法错误
- **适用场景**：创意写作、对话生成

## 3. 温度采样 (Temperature Sampling)

### 3.1 原理

温度参数 T 用于重新调整概率分布的"尖锐度"，控制采样的随机性。

**数学表示**：
```
P_T(w_i) = exp(logits_i / T) / Σ_j exp(logits_j / T)
```

其中：
- logits_i 是模型输出的未归一化对数概率
- T 是温度参数
- P_T(w_i) 是经过温度调整后的概率

### 3.2 温度参数的影响

**T → 0（低温）**：
- 概率分布变得尖锐，接近one-hot分布
- 行为接近贪婪采样
- 生成文本更加保守、可预测、连贯
- 适合需要高精确度的任务

**T = 1（标准温度）**：
- 保持模型原始输出的概率分布
- 标准的随机采样

**T > 1（高温）**：
- 概率分布变得平滑，各词概率趋于均匀
- 增加低概率词的采样机会
- 生成文本更加多样化、创造性
- 但可能降低连贯性和语法正确性

**T → ∞（极高温）**：
- 概率分布趋近均匀分布
- 接近完全随机采样

### 3.3 实践经验

- **T = 0.5-0.7**：适合大多数生成任务，平衡连贯性和多样性
- **T = 0.3-0.5**：适合需要高度准确性的任务（翻译、代码生成）
- **T = 0.8-1.2**：适合创意写作、故事生成
- **T > 1.5**：通常不推荐，文本质量显著下降

## 4. 高级采样策略

### 4.1 Top-k 采样

**原理**：只从概率最高的 k 个词中进行采样，过滤掉长尾低概率词。

**算法**：
1. 对概率分布排序，选择前 k 个词
2. 重新归一化这 k 个词的概率
3. 从重新归一化的分布中采样

**特点**：
- 动态过滤不相关的词
- k 值通常设置为 30-50
- 避免采样到完全不合理的词

### 4.2 核采样 (Nucleus Sampling / Top-p)

**原理**：选择累积概率超过阈值 p 的最小词集合进行采样。

**算法**：
1. 对概率分布降序排列
2. 计算累积概率，找到 Σ P(w_i) ≥ p 的最小词集
3. 从该词集中重新归一化并采样

**特点**：
- 动态调整候选词数量
- p 通常设置为 0.9-0.95
- 比 Top-k 更加灵活，适应不同的概率分布形状
- 是目前最先进的采样方法之一

### 4.3 混合策略

实践中常将温度采样与 Top-k/Top-p 结合使用：
1. 先用温度参数调整分布
2. 再用 Top-k 或 Top-p 过滤候选词
3. 从过滤后的分布中采样

## 5. 信息论视角

从信息论角度，温度参数控制的是概率分布的**熵**：

```
H(P_T) = -Σ P_T(w_i) log P_T(w_i)
```

- 低温（T → 0）：低熵，确定性高，"惊喜"少
- 高温（T → ∞）：高熵，不确定性高，"惊喜"多

生成文本的质量往往需要在**连贯性**（低熵）和**创造性**（高熵）之间取得平衡。

## 6. 工程实践建议

1. **任务类型决定策略**：
   - 事实性任务（翻译、QA）→ 低温度或贪婪采样
   - 创造性任务（写作、对话）→ 中等温度 + Nucleus采样

2. **参数调优**：
   - 从 T=0.7, p=0.9 开始实验
   - 根据生成结果调整参数
   - 使用验证集评估困惑度和生成质量

3. **避免退化**：
   - 使用重复惩罚（repetition penalty）
   - 限制n-gram重复
   - 设置最小/最大生成长度

4. **计算效率**：
   - 贪婪采样速度最快
   - Top-k 需要排序，但k值不大时效率仍高
   - Top-p 需要动态计算累积和，略慢但可接受
