{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 深度卷积生成对抗网络（DCGAN）实现\n\n## 理论基础\n\n生成对抗网络（GAN）由Ian Goodfellow于2014年提出，包含两个相互竞争的神经网络：\n\n1. **生成器（Generator）**：从潜在空间的随机噪声生成逼真的样本\n2. **判别器（Discriminator）**：判断输入样本是真实样本还是生成样本\n\n### 数学原理\n\nGAN的训练目标是最小化生成器的损失，同时最大化判别器的损失，形成一个极小极大博弈：\n\n$$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1-D(G(z)))]$$\n\n其中：\n- $D(x)$：判别器对真实样本的输出概率\n- $G(z)$：生成器从噪声$z$生成的样本\n- $p_{data}$：真实数据分布\n- $p_z$：潜在空间噪声分布（通常为高斯分布）\n\n### DCGAN架构特点\n\nDCGAN（Deep Convolutional GAN）是GAN的改进版本，主要特点：\n- 使用卷积和转置卷积替代全连接层\n- 取消池化层，使用步进卷积进行上/下采样\n- 使用Batch Normalization（除生成器输出层和判别器输入层外）\n- 生成器使用ReLU激活（输出层使用tanh）\n- 判别器使用LeakyReLU激活",
   "id": "c28dfcafdc883af1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T14:26:13.721812Z",
     "start_time": "2025-09-27T14:26:13.662814Z"
    }
   },
   "cell_type": "code",
   "source": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 设置随机种子以确保结果可复现\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# ============================================================================\n# 超参数配置\n# ============================================================================\nlatent_dim = 32  # 潜在空间维度：控制生成器输入的噪声向量维度\nheight, width, channels = 32, 32, 3  # 生成图像尺寸（与CIFAR-10一致）\n\n# ============================================================================\n# 生成器网络架构\n# ============================================================================\n# 生成器的作用：将低维潜在空间的随机噪声映射到高维图像空间\n# 架构设计遵循DCGAN原则：全卷积网络，使用LeakyReLU和转置卷积\n\ngenerator_input = keras.Input(shape=(latent_dim,), name='generator_input')\n\n# 第一层：全连接层扩展维度\n# 将32维噪声向量扩展到16x16x128的特征图\nx = layers.Dense(128 * 16 * 16, name='dense_expand')(generator_input)\nx = layers.LeakyReLU(negative_slope=0.2, name='leaky_relu_1')(x)\nx = layers.Reshape((16, 16, 128), name='reshape')(x)\n\n# 第二层：卷积层特征提取\n# 保持空间尺寸不变，增加特征通道数\nx = layers.Conv2D(256, kernel_size=5, padding='same', name='conv2d_1')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='leaky_relu_2')(x)\n\n# 第三层：转置卷积上采样\n# 将16x16上采样到32x32（步长为2）\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', name='conv2d_transpose')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='leaky_relu_3')(x)\n\n# 第四、五层：卷积层深度特征提取\n# 保持32x32尺寸，进一步提炼特征\nx = layers.Conv2D(256, kernel_size=5, padding='same', name='conv2d_2')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='leaky_relu_4')(x)\nx = layers.Conv2D(256, kernel_size=5, padding='same', name='conv2d_3')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='leaky_relu_5')(x)\n\n# 输出层：生成RGB图像\n# 使用tanh激活函数将输出限制在[-1, 1]范围（与数据归一化对应）\nx = layers.Conv2D(channels, kernel_size=7, activation='tanh', padding='same', name='output')(x)\n\n# 实例化生成器模型\ngenerator = keras.models.Model(generator_input, x, name='generator')\ngenerator.summary()",
   "id": "ea1a4d44c6405e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 判别器网络架构\n\n判别器是一个二分类器，用于区分真实图像和生成图像。\n\n**设计原则：**\n- 使用步进卷积进行下采样（替代池化层）\n- 使用LeakyReLU激活函数（避免梯度消失）\n- 添加Dropout防止过拟合\n- 输出单个概率值（sigmoid激活）",
   "id": "feed6876935d098b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T14:26:13.847322Z",
     "start_time": "2025-09-27T14:26:13.784320Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================================\n# 判别器网络架构\n# ============================================================================\n# 判别器的作用：将高维图像空间映射到[0,1]的概率值\n# 输出接近1表示真实图像，接近0表示生成图像\n\ndiscriminator_input = layers.Input(shape=(height, width, channels), name='discriminator_input')\n\n# 第一层：初始卷积特征提取\n# 32x32x3 -> 30x30x128\nx = layers.Conv2D(128, kernel_size=3, name='disc_conv2d_1')(discriminator_input)\nx = layers.LeakyReLU(negative_slope=0.2, name='disc_leaky_relu_1')(x)\n\n# 第二层：步进卷积下采样\n# 30x30x128 -> 14x14x128\nx = layers.Conv2D(128, kernel_size=4, strides=2, name='disc_conv2d_2')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='disc_leaky_relu_2')(x)\n\n# 第三层：步进卷积下采样\n# 14x14x128 -> 6x6x128\nx = layers.Conv2D(128, kernel_size=4, strides=2, name='disc_conv2d_3')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='disc_leaky_relu_3')(x)\n\n# 第四层：步进卷积下采样\n# 6x6x128 -> 2x2x128\nx = layers.Conv2D(128, kernel_size=4, strides=2, name='disc_conv2d_4')(x)\nx = layers.LeakyReLU(negative_slope=0.2, name='disc_leaky_relu_4')(x)\n\n# 展平特征\nx = layers.Flatten(name='flatten')(x)\n\n# Dropout层：随机丢弃30%的神经元，防止判别器过拟合\n# 过强的判别器会导致生成器难以学习\nx = layers.Dropout(0.3, name='dropout')(x)\n\n# 输出层：二分类概率\n# sigmoid输出范围[0,1]，表示输入为真实图像的概率\nx = layers.Dense(1, activation='sigmoid', name='disc_output')(x)\n\n# 实例化判别器模型\ndiscriminator = keras.models.Model(discriminator_input, x, name='discriminator')\ndiscriminator.summary()\n\n# ============================================================================\n# 判别器优化器配置\n# ============================================================================\n# RMSprop优化器：适合处理非平稳目标（GAN训练的特点）\n# - learning_rate: 学习率，控制参数更新步长\n# - clipvalue: 梯度裁剪，防止梯度爆炸\n\ndiscriminator_optimizer = keras.optimizers.RMSprop(\n    learning_rate=0.0008,\n    clipvalue=1.0\n)\n\n# 编译判别器\n# 使用二元交叉熵损失，因为这是一个二分类问题\ndiscriminator.compile(\n    optimizer=discriminator_optimizer,\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)",
   "id": "1700443a185b8bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建对抗网络（GAN）\n\n对抗网络将生成器和判别器串联，用于训练生成器。\n\n**训练策略：**\n1. 固定判别器权重（`trainable=False`）\n2. 生成器生成假样本\n3. 判别器评估假样本\n4. 通过反向传播更新生成器权重\n5. 目标：让判别器将生成样本误判为真实样本\n\n**交替训练：**\n- 训练判别器时：使用真实和生成样本，更新判别器权重\n- 训练生成器时：通过GAN模型，固定判别器权重，只更新生成器权重",
   "id": "f2f6c759e71c0275"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T14:26:13.887962Z",
     "start_time": "2025-09-27T14:26:13.854910Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================================\n# 构建对抗网络（GAN）\n# ============================================================================\n# GAN模型用于训练生成器：\n# 输入：潜在空间噪声 -> 生成器 -> 生成图像 -> 判别器 -> 真假概率\n# 训练时判别器权重被冻结，只更新生成器权重\n\n# 冻结判别器权重\n# 这样在训练GAN时，只有生成器的权重会被更新\ndiscriminator.trainable = False\n\n# 构建GAN模型\ngan_input = keras.Input(shape=(latent_dim,), name='gan_input')\ngan_output = discriminator(generator(gan_input))\ngan = keras.models.Model(gan_input, gan_output, name='gan')\n\n# ============================================================================\n# GAN优化器配置\n# ============================================================================\n# 生成器的学习率设置为判别器的一半\n# 这样可以避免生成器学习过快导致训练不稳定\n\ngan_optimizer = keras.optimizers.RMSprop(\n    learning_rate=0.0004,\n    clipvalue=1.0\n)\n\n# 编译GAN模型\n# 生成器的目标：让判别器输出接近1（即将生成图像误判为真实图像）\ngan.compile(\n    optimizer=gan_optimizer,\n    loss='binary_crossentropy'\n)\n\nprint('GAN模型构建完成')\nprint(f'生成器参数量: {generator.count_params():,}')\nprint(f'判别器参数量: {discriminator.count_params():,}')\nprint(f'GAN总参数量: {gan.count_params():,}')",
   "id": "af09fe7ff79b9bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练DCGAN\n\n## 训练流程\n\nGAN的训练是一个交替优化的过程：\n\n### 1. 训练判别器\n- 从数据集采样真实图像，标签为1\n- 生成器生成假图像，标签为0\n- 将真假图像混合，训练判别器区分它们\n\n### 2. 训练生成器\n- 生成假图像\n- 使用标签1（欺骗判别器）\n- 通过冻结的判别器反向传播，只更新生成器权重\n\n## 训练技巧\n\n1. **标签平滑**：给标签添加小噪声，防止判别器过于自信\n2. **标签翻转**：偶尔翻转标签，增加训练稳定性\n3. **学习率平衡**：生成器学习率通常低于判别器\n4. **梯度裁剪**：防止梯度爆炸\n5. **监控损失**：判别器损失趋近0表示过拟合，需要调整\n\n## 数据预处理\n\nCIFAR-10数据集包含60000张32x32的彩色图像，分为10类。\nGAN训练不需要标签，只使用图像数据。\n图像归一化到[-1, 1]范围，与生成器的tanh输出对应。",
   "id": "78a29546eb208587"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T14:26:49.729157Z",
     "start_time": "2025-09-27T14:26:49.226786Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================================\n# 数据加载与预处理\n# ============================================================================\n\n# 加载CIFAR-10数据集\nprint('正在加载CIFAR-10数据集...')\n(x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\n\n# 数据预处理\n# 1. 归一化到[-1, 1]范围（对应生成器tanh输出）\n# 2. 不需要标签，因为GAN是无监督学习\nx_train = x_train.astype('float32')\nx_train = (x_train - 127.5) / 127.5  # 归一化到[-1, 1]\n\nprint(f'训练集形状: {x_train.shape}')\nprint(f'数据范围: [{x_train.min():.2f}, {x_train.max():.2f}]')\n\n# ============================================================================\n# 训练配置\n# ============================================================================\n\n# 训练超参数\niterations = 10000  # 总迭代次数\nbatch_size = 20  # 批次大小\nsave_interval = 100  # 保存间隔\n\n# 创建输出目录\noutput_dir = './gan_output'\nos.makedirs(output_dir, exist_ok=True)\nprint(f'输出目录: {output_dir}')\n\n# ============================================================================\n# 可视化辅助函数\n# ============================================================================\n\ndef save_generated_images(epoch, generator, latent_dim, examples=10, dim=(1, 10), figsize=(20, 2)):\n    \"\"\"\n    生成并保存图像网格\n    \n    参数:\n        epoch: 当前迭代次数\n        generator: 生成器模型\n        latent_dim: 潜在空间维度\n        examples: 生成图像数量\n        dim: 网格维度(行, 列)\n        figsize: 图像尺寸\n    \"\"\"\n    noise = np.random.normal(0, 1, size=(examples, latent_dim))\n    generated_images = generator.predict(noise, verbose=0)\n    \n    # 反归一化到[0, 1]用于显示\n    generated_images = (generated_images + 1) / 2.0\n    generated_images = np.clip(generated_images, 0, 1)\n    \n    plt.figure(figsize=figsize)\n    for i in range(examples):\n        plt.subplot(dim[0], dim[1], i + 1)\n        plt.imshow(generated_images[i])\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/generated_epoch_{epoch}.png', dpi=100, bbox_inches='tight')\n    plt.close()\n\n# ============================================================================\n# 训练循环\n# ============================================================================\n\nprint('\\n开始训练DCGAN...')\nprint(f'总迭代次数: {iterations}')\nprint(f'批次大小: {batch_size}')\nprint('-' * 80)\n\n# 训练历史记录\nd_losses = []\ng_losses = []\nd_accuracies = []\n\nstart_idx = 0\n\nfor step in range(iterations):\n    # ------------------------------------------------------------------------\n    # 阶段1: 训练判别器\n    # ------------------------------------------------------------------------\n    \n    # 解冻判别器权重\n    discriminator.trainable = True\n    \n    # 1.1 采样真实图像\n    stop_idx = start_idx + batch_size\n    real_images = x_train[start_idx:stop_idx]\n    \n    # 1.2 生成假图像\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n    generated_images = generator.predict(random_latent_vectors, verbose=0)\n    \n    # 1.3 合并真假图像\n    combined_images = np.concatenate([real_images, generated_images])\n    \n    # 1.4 创建标签\n    # 真实图像标签为1，生成图像标签为0\n    # 添加标签平滑：给标签加入小噪声，防止判别器过于自信\n    labels = np.concatenate([\n        np.ones((batch_size, 1)),\n        np.zeros((batch_size, 1))\n    ])\n    labels += 0.05 * np.random.random(labels.shape)\n    \n    # 1.5 训练判别器\n    d_loss, d_acc = discriminator.train_on_batch(combined_images, labels)\n    \n    # ------------------------------------------------------------------------\n    # 阶段2: 训练生成器\n    # ------------------------------------------------------------------------\n    \n    # 冻结判别器权重\n    discriminator.trainable = False\n    \n    # 2.1 采样潜在空间噪声\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n    \n    # 2.2 创建欺骗性标签\n    # 目标是让判别器认为生成的图像是真实的（标签为1）\n    misleading_targets = np.ones((batch_size, 1))\n    \n    # 2.3 训练生成器（通过GAN模型）\n    g_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n    \n    # ------------------------------------------------------------------------\n    # 更新训练状态\n    # ------------------------------------------------------------------------\n    \n    # 记录损失\n    d_losses.append(d_loss)\n    g_losses.append(g_loss)\n    d_accuracies.append(d_acc)\n    \n    # 更新数据索引\n    start_idx += batch_size\n    if start_idx > len(x_train) - batch_size:\n        start_idx = 0\n    \n    # ------------------------------------------------------------------------\n    # 定期保存和可视化\n    # ------------------------------------------------------------------------\n    \n    if step % save_interval == 0:\n        # 打印训练进度\n        print(f'Step {step:5d}/{iterations} | '\n              f'D Loss: {d_loss:.4f} | '\n              f'G Loss: {g_loss:.4f} | '\n              f'D Acc: {d_acc:.4f}')\n        \n        # 保存生成的图像\n        save_generated_images(step, generator, latent_dim)\n        \n        # 保存模型权重\n        gan.save_weights(f'{output_dir}/gan_weights_step_{step}.h5')\n        \n        # 判断训练状态\n        if d_acc < 0.5:\n            print('  ⚠️  警告: 判别器准确率过低，可能需要调整学习率')\n        elif d_acc > 0.95:\n            print('  ⚠️  警告: 判别器准确率过高，可能过拟合')\n\nprint('-' * 80)\nprint('训练完成！')\n\n# ============================================================================\n# 训练结果可视化\n# ============================================================================\n\nplt.figure(figsize=(15, 5))\n\n# 绘制损失曲线\nplt.subplot(1, 3, 1)\nplt.plot(d_losses, label='Discriminator Loss', alpha=0.7)\nplt.plot(g_losses, label='Generator Loss', alpha=0.7)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 绘制判别器准确率\nplt.subplot(1, 3, 2)\nplt.plot(d_accuracies, label='Discriminator Accuracy', alpha=0.7, color='green')\nplt.axhline(y=0.5, color='r', linestyle='--', label='Random Guess')\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.title('Discriminator Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# 绘制损失比率\nplt.subplot(1, 3, 3)\nloss_ratio = np.array(g_losses) / (np.array(d_losses) + 1e-8)\nplt.plot(loss_ratio, label='G Loss / D Loss', alpha=0.7, color='purple')\nplt.axhline(y=1, color='r', linestyle='--', label='Equal Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss Ratio')\nplt.title('Generator/Discriminator Loss Ratio')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(f'{output_dir}/training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'\\n所有结果已保存到: {output_dir}')\nprint(f'- 生成图像: generated_epoch_*.png')\nprint(f'- 模型权重: gan_weights_step_*.h5')\nprint(f'- 训练历史: training_history.png')",
   "id": "2f5130c426898d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "vk4kfjijass",
   "source": "# 潜在空间探索与插值\n\n训练完成后，我们可以探索潜在空间的特性：\n- 潜在空间的连续性：相近的噪声向量生成相似的图像\n- 线性插值：在两个噪声向量之间平滑过渡\n- 语义方向：潜在空间中的特定方向可能对应特定的语义属性",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1ydht9zkjtu",
   "source": "# ============================================================================\n# 潜在空间插值实验\n# ============================================================================\n\ndef interpolate_latent_space(generator, latent_dim, n_steps=10):\n    \"\"\"\n    在潜在空间中进行线性插值，生成平滑过渡的图像序列\n    \n    参数:\n        generator: 生成器模型\n        latent_dim: 潜在空间维度\n        n_steps: 插值步数\n    \"\"\"\n    # 生成两个随机噪声向量\n    z1 = np.random.normal(size=(1, latent_dim))\n    z2 = np.random.normal(size=(1, latent_dim))\n    \n    # 线性插值\n    alphas = np.linspace(0, 1, n_steps)\n    interpolated_vectors = np.array([alpha * z1 + (1 - alpha) * z2 for alpha in alphas])\n    interpolated_vectors = interpolated_vectors.reshape(n_steps, latent_dim)\n    \n    # 生成图像\n    generated_images = generator.predict(interpolated_vectors, verbose=0)\n    generated_images = (generated_images + 1) / 2.0\n    generated_images = np.clip(generated_images, 0, 1)\n    \n    # 可视化\n    plt.figure(figsize=(20, 2))\n    for i in range(n_steps):\n        plt.subplot(1, n_steps, i + 1)\n        plt.imshow(generated_images[i])\n        plt.axis('off')\n        if i == 0:\n            plt.title('起点', fontsize=10)\n        elif i == n_steps - 1:\n            plt.title('终点', fontsize=10)\n    \n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/latent_interpolation.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n# 执行插值实验\nprint('潜在空间插值实验：')\ninterpolate_latent_space(generator, latent_dim, n_steps=10)\nprint(f'插值结果已保存到: {output_dir}/latent_interpolation.png')\n\n# ============================================================================\n# 批量生成图像\n# ============================================================================\n\ndef generate_image_grid(generator, latent_dim, rows=5, cols=10):\n    \"\"\"\n    批量生成图像网格\n    \n    参数:\n        generator: 生成器模型\n        latent_dim: 潜在空间维度\n        rows: 行数\n        cols: 列数\n    \"\"\"\n    n_images = rows * cols\n    noise = np.random.normal(0, 1, size=(n_images, latent_dim))\n    generated_images = generator.predict(noise, verbose=0)\n    generated_images = (generated_images + 1) / 2.0\n    generated_images = np.clip(generated_images, 0, 1)\n    \n    plt.figure(figsize=(cols * 2, rows * 2))\n    for i in range(n_images):\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(generated_images[i])\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/generated_grid.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\nprint('\\n生成图像网格：')\ngenerate_image_grid(generator, latent_dim, rows=5, cols=10)\nprint(f'图像网格已保存到: {output_dir}/generated_grid.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}