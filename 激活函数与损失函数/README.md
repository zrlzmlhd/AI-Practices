# æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°

æ·±åº¦å­¦ä¹ æ ¸å¿ƒç»„ä»¶çš„è¯¦ç»†å‚è€ƒèµ„æ–™ã€‚

<div align="center">

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg)](https://www.tensorflow.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

</div>

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Activation

# æ¿€æ´»å‡½æ•°ä½¿ç”¨
x = Dense(64, activation='relu')(inputs)          # ReLU
x = Dense(64, activation='leaky_relu')(x)         # Leaky ReLU
outputs = Dense(10, activation='softmax')(x)       # Softmaxè¾“å‡º

# æŸå¤±å‡½æ•°ä½¿ç”¨
model.compile(
    loss='sparse_categorical_crossentropy',  # å¤šåˆ†ç±»
    optimizer='adam',
    metrics=['accuracy']
)
```

## ğŸ“š æ¨¡å—ç®€ä»‹

æœ¬æ¨¡å—æä¾›äº†æ·±åº¦å­¦ä¹ ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªç»„ä»¶çš„è¯¦ç»†è¯´æ˜ï¼š**æ¿€æ´»å‡½æ•°**å’Œ**æŸå¤±å‡½æ•°**ã€‚è¿™äº›æ˜¯æ„å»ºç¥ç»ç½‘ç»œçš„åŸºç¡€æ¨¡å—ï¼Œç†è§£å®ƒä»¬å¯¹äºè®¾è®¡å’Œä¼˜åŒ–æ¨¡å‹è‡³å…³é‡è¦ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… ç†è§£å„ç§æ¿€æ´»å‡½æ•°çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
- âœ… æŒæ¡ä¸åŒæŸå¤±å‡½æ•°çš„æ•°å­¦åŸç†
- âœ… å­¦ä¼šä¸ºç‰¹å®šä»»åŠ¡é€‰æ‹©åˆé€‚çš„å‡½æ•°
- âœ… äº†è§£å‡½æ•°é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

## ğŸ“‚ å†…å®¹ç»“æ„

```
æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°/
â”œâ”€â”€ å¸¸è§æ¿€æ´»å‡½æ•°åŠå…¶å›¾åƒ/
â”‚   â””â”€â”€ æ¿€æ´»å‡½æ•°å¯è§†åŒ–.ipynb
â””â”€â”€ æŸå¤±å‡½æ•°/
    â””â”€â”€ æŸå¤±å‡½æ•°.md
```

## âš¡ æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•°ä¸ºç¥ç»ç½‘ç»œå¼•å…¥éçº¿æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚

### å¸¸è§æ¿€æ´»å‡½æ•°

#### 1. Sigmoid (Ïƒ)

**æ•°å­¦å…¬å¼ï¼š**
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**ç‰¹ç‚¹ï¼š**
- è¾“å‡ºèŒƒå›´ï¼š(0, 1)
- å¹³æ»‘çš„Så‹æ›²çº¿
- å¯ä»¥è§£é‡Šä¸ºæ¦‚ç‡

**ä¼˜ç‚¹ï¼š**
- è¾“å‡ºæœ‰ç•Œ
- å¹³æ»‘å¯å¯¼

**ç¼ºç‚¹ï¼š**
- æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- è¾“å‡ºä¸ä»¥é›¶ä¸ºä¸­å¿ƒ
- è®¡ç®—å¼€é”€è¾ƒå¤§

**é€‚ç”¨åœºæ™¯ï¼š**
- äºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚
- éœ€è¦æ¦‚ç‡è¾“å‡ºçš„åœºæ™¯

---

#### 2. Tanh (åŒæ›²æ­£åˆ‡)

**æ•°å­¦å…¬å¼ï¼š**
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**ç‰¹ç‚¹ï¼š**
- è¾“å‡ºèŒƒå›´ï¼š(-1, 1)
- é›¶ä¸ºä¸­å¿ƒ
- æ¯”Sigmoidæ›´é™¡å³­

**ä¼˜ç‚¹ï¼š**
- é›¶ä¸ºä¸­å¿ƒï¼Œæ”¶æ•›æ›´å¿«
- æ¢¯åº¦æ›´å¼º

**ç¼ºç‚¹ï¼š**
- ä»æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- è®¡ç®—å¼€é”€è¾ƒå¤§

**é€‚ç”¨åœºæ™¯ï¼š**
- éšè—å±‚
- RNN/LSTM

---

#### 3. ReLU (Rectified Linear Unit)

**æ•°å­¦å…¬å¼ï¼š**
$$\text{ReLU}(x) = \max(0, x)$$

**ç‰¹ç‚¹ï¼š**
- æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°
- è®¡ç®—ç®€å•é«˜æ•ˆ
- ä¸é¥±å’Œ

**ä¼˜ç‚¹ï¼š**
- è®¡ç®—æ•ˆç‡é«˜
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- åŠ é€Ÿæ”¶æ•›

**ç¼ºç‚¹ï¼š**
- ç¥ç»å…ƒ"æ­»äº¡"é—®é¢˜
- è¾“å‡ºä¸ä»¥é›¶ä¸ºä¸­å¿ƒ

**é€‚ç”¨åœºæ™¯ï¼š**
- CNNå·ç§¯å±‚
- å…¨è¿æ¥å±‚
- å¤§å¤šæ•°æ·±åº¦ç½‘ç»œ

---

#### 4. Leaky ReLU

**æ•°å­¦å…¬å¼ï¼š**
$$\text{Leaky ReLU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}$$

å…¶ä¸­ Î± é€šå¸¸æ˜¯ 0.01

**ç‰¹ç‚¹ï¼š**
- è§£å†³ReLUçš„"æ­»äº¡"é—®é¢˜
- å…è®¸è´Ÿå€¼æ¢¯åº¦

**ä¼˜ç‚¹ï¼š**
- æ‰€æœ‰è¾“å…¥éƒ½æœ‰æ¢¯åº¦
- ä¿ç•™ReLUçš„ä¼˜ç‚¹

**é€‚ç”¨åœºæ™¯ï¼š**
- æ›¿ä»£ReLU
- æ·±å±‚ç½‘ç»œ

---

#### 5. ELU (Exponential Linear Unit)

**æ•°å­¦å…¬å¼ï¼š**
$$\text{ELU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \leq 0
\end{cases}$$

**ç‰¹ç‚¹ï¼š**
- è´Ÿå€¼è¾“å‡º
- å¹³æ»‘æ€§å¥½

**ä¼˜ç‚¹ï¼š**
- åŠ é€Ÿå­¦ä¹ 
- è¾“å‡ºæ¥è¿‘é›¶å‡å€¼
- é¿å…ç¥ç»å…ƒ"æ­»äº¡"

**ç¼ºç‚¹ï¼š**
- è®¡ç®—å¼€é”€è¾ƒå¤§

---

#### 6. Swish / SiLU

**æ•°å­¦å…¬å¼ï¼š**
$$\text{Swish}(x) = x \cdot \sigma(x)$$

**ç‰¹ç‚¹ï¼š**
- Googleæå‡ºçš„æ–°æ¿€æ´»å‡½æ•°
- å¹³æ»‘éå•è°ƒ

**ä¼˜ç‚¹ï¼š**
- æŸäº›ä»»åŠ¡ä¸Šä¼˜äºReLU
- è‡ªé€‚åº”æ€§å¼º

---

#### 7. Softmax

**æ•°å­¦å…¬å¼ï¼š**
$$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$

**ç‰¹ç‚¹ï¼š**
- å¤šåˆ†ç±»é—®é¢˜ä¸“ç”¨
- è¾“å‡ºå’Œä¸º1
- å¯è§£é‡Šä¸ºæ¦‚ç‡åˆ†å¸ƒ

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚

---

### æ¿€æ´»å‡½æ•°é€‰æ‹©æŒ‡å—

| å±‚ç±»å‹ | æ¨èæ¿€æ´»å‡½æ•° | åŸå›  |
|-------|------------|------|
| å·ç§¯å±‚ | ReLU, Leaky ReLU | è®¡ç®—æ•ˆç‡é«˜ï¼Œæ€§èƒ½å¥½ |
| å…¨è¿æ¥éšè—å±‚ | ReLU, ELU | è®­ç»ƒå¿«ï¼Œæ•ˆæœå¥½ |
| RNN/LSTM | Tanh, Sigmoid | RNNæ ‡å‡†é…ç½® |
| äºŒåˆ†ç±»è¾“å‡ºå±‚ | Sigmoid | è¾“å‡ºæ¦‚ç‡ |
| å¤šåˆ†ç±»è¾“å‡ºå±‚ | Softmax | è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ |
| å›å½’è¾“å‡ºå±‚ | Linear (æ— æ¿€æ´») | ä¸é™åˆ¶è¾“å‡ºèŒƒå›´ |
| GANåˆ¤åˆ«å™¨ | Leaky ReLU | é¿å…æ¢¯åº¦é—®é¢˜ |

---

## ğŸ“‰ æŸå¤±å‡½æ•°

æŸå¤±å‡½æ•°è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼çš„å·®è·ï¼ŒæŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–æ–¹å‘ã€‚

### åˆ†ç±»ä»»åŠ¡æŸå¤±å‡½æ•°

#### 1. äºŒåˆ†ç±»äº¤å‰ç†µ (Binary Cross-Entropy)

**æ•°å­¦å…¬å¼ï¼š**
$$BCE = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

**é€‚ç”¨åœºæ™¯ï¼š**
- äºŒåˆ†ç±»é—®é¢˜
- å¤šæ ‡ç­¾åˆ†ç±»

**Keraså®ç°ï¼š**
```python
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

**PyTorchå®ç°ï¼š**
```python
criterion = nn.BCELoss()
# æˆ–é…åˆsigmoid
criterion = nn.BCEWithLogitsLoss()
```

---

#### 2. å¤šåˆ†ç±»äº¤å‰ç†µ (Categorical Cross-Entropy)

**æ•°å­¦å…¬å¼ï¼š**
$$CCE = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})$$

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤šåˆ†ç±»é—®é¢˜ï¼ˆäº’æ–¥ç±»åˆ«ï¼‰
- ç±»åˆ«éœ€è¦one-hotç¼–ç 

**Keraså®ç°ï¼š**
```python
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

---

#### 3. ç¨€ç–åˆ†ç±»äº¤å‰ç†µ (Sparse Categorical Cross-Entropy)

**ç‰¹ç‚¹ï¼š**
- ä¸categorical_crossentropyç›¸åŒ
- ä½†æ ‡ç­¾æ˜¯æ•´æ•°è€Œéone-hot

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤šåˆ†ç±»é—®é¢˜
- æ ‡ç­¾æ˜¯æ•´æ•°æ ¼å¼

**Keraså®ç°ï¼š**
```python
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

---

### å›å½’ä»»åŠ¡æŸå¤±å‡½æ•°

#### 1. å‡æ–¹è¯¯å·® (MSE - Mean Squared Error)

**æ•°å­¦å…¬å¼ï¼š**
$$MSE = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2$$

**ç‰¹ç‚¹ï¼š**
- æœ€å¸¸ç”¨çš„å›å½’æŸå¤±
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
- è¯¯å·®è¢«å¹³æ–¹æ”¾å¤§

**é€‚ç”¨åœºæ™¯ï¼š**
- æ ‡å‡†å›å½’é—®é¢˜
- é¢„æµ‹è¿ç»­å€¼

**å®ç°ï¼š**
```python
# Keras
model.compile(loss='mse', optimizer='adam')

# PyTorch
criterion = nn.MSELoss()
```

---

#### 2. å¹³å‡ç»å¯¹è¯¯å·® (MAE - Mean Absolute Error)

**æ•°å­¦å…¬å¼ï¼š**
$$MAE = \frac{1}{N}\sum_{i=1}^{N}|y_i - \hat{y}_i|$$

**ç‰¹ç‚¹ï¼š**
- å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
- æ¢¯åº¦æ’å®š

**é€‚ç”¨åœºæ™¯ï¼š**
- æœ‰å¼‚å¸¸å€¼çš„æ•°æ®
- éœ€è¦é²æ£’æ€§

**å®ç°ï¼š**
```python
# Keras
model.compile(loss='mae', optimizer='adam')

# PyTorch
criterion = nn.L1Loss()
```

---

#### 3. HuberæŸå¤±

**æ•°å­¦å…¬å¼ï¼š**
$$L_\delta(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}$$

**ç‰¹ç‚¹ï¼š**
- ç»“åˆMSEå’ŒMAEä¼˜ç‚¹
- å°è¯¯å·®ç”¨MSEï¼Œå¤§è¯¯å·®ç”¨MAE
- å¯¹å¼‚å¸¸å€¼é²æ£’

**é€‚ç”¨åœºæ™¯ï¼š**
- æœ‰å¼‚å¸¸å€¼çš„å›å½’é—®é¢˜

**å®ç°ï¼š**
```python
# Keras
model.compile(loss='huber', optimizer='adam')

# PyTorch
criterion = nn.SmoothL1Loss()
```

---

### ç‰¹æ®Šä»»åŠ¡æŸå¤±å‡½æ•°

#### 1. Hinge Loss

**æ•°å­¦å…¬å¼ï¼š**
$$L = \max(0, 1 - y \cdot \hat{y})$$

**é€‚ç”¨åœºæ™¯ï¼š**
- SVM
- äºŒåˆ†ç±»é—®é¢˜ï¼ˆæ ‡ç­¾ä¸ºÂ±1ï¼‰

---

#### 2. KLæ•£åº¦ (Kullback-Leibler Divergence)

**æ•°å­¦å…¬å¼ï¼š**
$$D_{KL}(P||Q) = \sum_i P(i) \log\frac{P(i)}{Q(i)}$$

**é€‚ç”¨åœºæ™¯ï¼š**
- VAE
- åˆ†å¸ƒåŒ¹é…
- çŸ¥è¯†è’¸é¦

---

#### 3. Focal Loss

**æ•°å­¦å…¬å¼ï¼š**
$$FL = -\alpha_t(1-p_t)^\gamma \log(p_t)$$

**ç‰¹ç‚¹ï¼š**
- è§£å†³ç±»åˆ«ä¸å¹³è¡¡
- å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬

**é€‚ç”¨åœºæ™¯ï¼š**
- ç›®æ ‡æ£€æµ‹
- ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

---

## ğŸ¯ é€‰æ‹©æŒ‡å—

### æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©

| ä»»åŠ¡ç±»å‹ | è¾“å‡ºæ¿€æ´»å‡½æ•° | æŸå¤±å‡½æ•° |
|---------|------------|---------|
| äºŒåˆ†ç±» | Sigmoid | Binary Cross-Entropy |
| å¤šåˆ†ç±»ï¼ˆäº’æ–¥ï¼‰ | Softmax | Categorical Cross-Entropy |
| å¤šæ ‡ç­¾åˆ†ç±» | Sigmoid | Binary Cross-Entropy |
| å›å½’ | Linear | MSE / MAE |
| åºåˆ—ç”Ÿæˆ | Softmax | Sparse Categorical CE |

### æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©

| æ•°æ®ç‰¹ç‚¹ | æ¨èæŸå¤±å‡½æ•° |
|---------|------------|
| æœ‰å¼‚å¸¸å€¼ | MAE, Huber |
| ç±»åˆ«ä¸å¹³è¡¡ | Weighted CE, Focal Loss |
| æ ‡å‡†æ•°æ® | MSE, CE |

## ğŸ“Š å¯è§†åŒ–å¯¹æ¯”

æŸ¥çœ‹notebookè·å–å„æ¿€æ´»å‡½æ•°çš„å¯è§†åŒ–å¯¹æ¯”ï¼š

- å‡½æ•°æ›²çº¿
- å¯¼æ•°æ›²çº¿
- è¾“å‡ºåˆ†å¸ƒ
- æ¢¯åº¦æµåŠ¨

## ğŸ’¡ æœ€ä½³å®è·µ

### æ¿€æ´»å‡½æ•°

1. **é»˜è®¤é€‰æ‹©ReLU**
   - å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½
   - è®¡ç®—é«˜æ•ˆ

2. **é‡åˆ°é—®é¢˜æ—¶å°è¯•å˜ä½“**
   - ç¥ç»å…ƒ"æ­»äº¡" â†’ Leaky ReLU, ELU
   - éœ€è¦è´Ÿå€¼ â†’ Leaky ReLU, ELU
   - GAN â†’ Leaky ReLU

3. **è¾“å‡ºå±‚è¦æ…é‡**
   - äºŒåˆ†ç±» â†’ Sigmoid
   - å¤šåˆ†ç±» â†’ Softmax
   - å›å½’ â†’ Linear

### æŸå¤±å‡½æ•°

1. **åŒ¹é…ä»»åŠ¡ç±»å‹**
   - åˆ†ç±»ç”¨äº¤å‰ç†µ
   - å›å½’ç”¨MSE/MAE

2. **è€ƒè™‘æ•°æ®ç‰¹ç‚¹**
   - æœ‰å¼‚å¸¸å€¼ç”¨MAE
   - ç±»åˆ«ä¸å¹³è¡¡ç”¨åŠ æƒæŸå¤±

3. **å¯ä»¥ç»„åˆä½¿ç”¨**
   ```python
   total_loss = Î± * loss1 + Î² * loss2
   ```

## ğŸ” è°ƒè¯•æŠ€å·§

### æ¿€æ´»å‡½æ•°é—®é¢˜

**ç—‡çŠ¶ï¼š** æ¢¯åº¦æ¶ˆå¤±
**è§£å†³ï¼š**
- æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Sigmoid/Tanhåœ¨æ·±å±‚ç½‘ç»œ
- å°è¯•ReLUç³»åˆ—

**ç—‡çŠ¶ï¼š** ç¥ç»å…ƒ"æ­»äº¡"
**è§£å†³ï¼š**
- é™ä½å­¦ä¹ ç‡
- ä½¿ç”¨Leaky ReLU
- æ£€æŸ¥åˆå§‹åŒ–

### æŸå¤±å‡½æ•°é—®é¢˜

**ç—‡çŠ¶ï¼š** æŸå¤±çˆ†ç‚¸/NaN
**è§£å†³ï¼š**
- æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
- ä½¿ç”¨LogSoftmax + NLLLoss
- æ¢¯åº¦è£å‰ª

**ç—‡çŠ¶ï¼š** è®­ç»ƒä¸æ”¶æ•›
**è§£å†³ï¼š**
- æ£€æŸ¥æŸå¤±å‡½æ•°æ˜¯å¦åŒ¹é…ä»»åŠ¡
- æ£€æŸ¥æ ‡ç­¾æ ¼å¼
- è°ƒæ•´å­¦ä¹ ç‡

## ğŸ“š å‚è€ƒèµ„æ–™

### ä¼˜è´¨å­¦ä¹ èµ„æº

| èµ„æºç±»å‹ | åç§° | é“¾æ¥ |
|---------|------|------|
| GitHub | PyTorchæ¿€æ´»å‡½æ•°å®ç° | [pytorch/pytorch](https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/activation.py) |
| GitHub | TensorFlowæ¿€æ´»å‡½æ•° | [tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/activations.py) |
| GitHub | è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç¤ºä¾‹ | [keras-team/keras-io](https://github.com/keras-team/keras-io/blob/master/examples/keras_recipes/creating_tfrecords.py) |
| å®˜æ–¹æ–‡æ¡£ | TensorFlowæ¿€æ´»å‡½æ•° | [tensorflow.org/api_docs/python/tf/keras/activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations) |
| å®˜æ–¹æ–‡æ¡£ | PyTorchæŸå¤±å‡½æ•° | [pytorch.org/docs/stable/nn](https://pytorch.org/docs/stable/nn.html#loss-functions) |

### è®ºæ–‡
- Glorot & Bengio (2010): Understanding the difficulty of training deep feedforward neural networks
- He et al. (2015): Delving Deep into Rectifiers (PReLU)
- Ramachandran et al. (2017): Searching for Activation Functions (Swish)

### æ–‡æ¡£
- [TensorFlowæ¿€æ´»å‡½æ•°](https://www.tensorflow.org/api_docs/python/tf/keras/activations)
- [PyTorchæ¿€æ´»å‡½æ•°](https://pytorch.org/docs/stable/nn.html#non-linear-activations)
- [PyTorchæŸå¤±å‡½æ•°](https://pytorch.org/docs/stable/nn.html#loss-functions)

### åšå®¢
- [CS231n: Neural Networks Part 1](http://cs231n.github.io/neural-networks-1/)
- [Activation Functions Explained](https://mlfromscratch.com/activation-functions-explained/)

## ğŸ”¬ å®éªŒå»ºè®®

1. **æ¯”è¾ƒä¸åŒæ¿€æ´»å‡½æ•°**
   - åœ¨åŒä¸€æ¨¡å‹ä¸Šæµ‹è¯•
   - è®°å½•æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½

2. **å¯è§†åŒ–æ¿€æ´»åˆ†å¸ƒ**
   - ä½¿ç”¨TensorBoard
   - æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡æ­»äº¡ç¥ç»å…ƒ

3. **æŸå¤±å‡½æ•°å¯¹æ¯”**
   - MSE vs MAE
   - ä¸åŒæƒé‡çš„ç»„åˆ

## ğŸ¤ è´¡çŒ®

å‘ç°é”™è¯¯æˆ–æƒ³æ·»åŠ æ–°å†…å®¹ï¼Ÿæ¬¢è¿è´¡çŒ®ï¼

æŸ¥çœ‹[è´¡çŒ®æŒ‡å—](../CONTRIBUTING.md)

---

æŒæ¡æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼

[è¿”å›ä¸»é¡µ](../README.md)
