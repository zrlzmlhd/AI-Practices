{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下载并解析原始数据文本",
   "id": "59329ac186b916db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T09:27:56.890106Z",
     "start_time": "2025-09-27T09:27:56.882917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "path = keras.utils.get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path, encoding='utf-8').read().lower()\n",
    "print(f'文本长度:{len(text)}个字符')"
   ],
   "id": "9cfd3dae2d0f9cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本长度:600893个字符\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 将字符序列向量化",
   "id": "31bfad76ec414157"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T09:27:58.864037Z",
     "start_time": "2025-09-27T09:27:56.899627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "maxlen = 60 # 每个序列的长度\n",
    "step = 3 # 每隔三个字符采样一个序列\n",
    "sentences = [] # 用来保存提取的序列\n",
    "next_chars = [] # 用来保存序列对应的下一个字符\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print(f\"序列数量:{len(sentences)}, 下一个字符数量:{len(next_chars)}\")\n",
    "\n",
    "chars = sorted(list(set(text))) # sorted()函数对字符进行排序 set()函数去除重复字符\n",
    "print(f\"唯一字符数量:{len(chars)}\")\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # 将唯一字符映射到它chars中的索引位置\n",
    "print('正在向量化')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ],
   "id": "87212baae3ac788e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列数量:200278, 下一个字符数量:200278\n",
      "唯一字符数量:57\n",
      "正在向量化\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20472\\3342090400.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20472\\3342090400.py:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建网络",
   "id": "50cce1efb5426d91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T09:27:59.000725Z",
     "start_time": "2025-09-27T09:27:58.874654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape= (maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer)\n",
    "model.summary()"
   ],
   "id": "4481954d5a9c25f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128)               95232     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 给定模型预测, 采样下一个字符的函数",
   "id": "c3baf5ca887f555d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T09:27:59.041366Z",
     "start_time": "2025-09-27T09:27:59.037097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "    \"\"\"\n",
    "    根据预测概率和温度参数进行采样，返回最可能的字符索引\n",
    "\n",
    "    参数:\n",
    "        preds: 模型输出的预测概率数组\n",
    "        temperature: 温度参数，用于控制采样的随机性，默认值为1.0\n",
    "\n",
    "    返回:\n",
    "        根据采样概率返回最可能的字符索引\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')  # 将输入转换为float64类型的numpy数组\n",
    "    preds = np.log(preds) / temperature  # 对预测概率取对数并除以温度参数\n",
    "    exp_preds = np.exp(preds)  # 对处理后的值取指数\n",
    "    preds = exp_preds / np.sum(exp_preds)  # 归一化，使概率总和为1\n",
    "    probas = np.random.multinomial(1, preds, 1)  # 进行多项式分布采样\n",
    "    return np.argmax(probas) # 返回概率最大的字符索引 argmax()函数返回最大值的索引"
   ],
   "id": "5265016a5396c742",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 文本生成循环",
   "id": "c171647fcdb9db88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T09:31:51.456213Z",
     "start_time": "2025-09-27T09:27:59.044870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print(f'开始第{epoch}轮')\n",
    "    model.fit(x, y, batch_size=128, epochs=1) # 将模型在数据上拟合一次\n",
    "    start_index = random.randint(0, len(text) - maxlen -1) # 随机选择一个起始点\n",
    "    generated_text = text[start_index: start_index + maxlen] # 从起始点开始生成文本\n",
    "    print(f'---生成文本---\\n {generated_text}', end='')\n",
    "    # 采用一系列不同的温度值来生成文本 对比在不同温度下的文本差异以及连贯性\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(f'---在温度{temperature}下生成文本---')\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        for i in range(400):\n",
    "            # 对当前生成的文本进行向量化\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            sys.stdout.write(next_char)"
   ],
   "id": "333cef8e63360e9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第1轮\n",
      "1565/1565 [==============================] - 13s 6ms/step - loss: 1.9704\n",
      "---生成文本---\n",
      "  the lofty independent\n",
      "spirituality, the will to stand alone---在温度0.2下生成文本---\n",
      " the lofty independent\n",
      "spirituality, the will to stand alone the present in the some the sensies of the same the some to the serience of the same and the same the distury the such a still the same the sense of the same the sensies the some the sense of the same the selfice of the present and the sensies of the same the consequents the may and the madity of the sense of the sense of the same to the serience of the section of the same the constraing to the s---在温度0.5下生成文本---\n",
      " serience of the section of the same the constraing to the sen the est such it with the same suit a compirious and the some the its most soue one a compartious are souch of the seed to the same of one one to the dignest that the enviluation of the incertaint, which all the self are meant to fartion of disture the world as a to in the sectiation of the person the sertion in the moral on fulther the contrad become some can means of the had in a still and the---在温度1.0下生成文本---\n",
      " contrad become some can means of the had in a still and the science in sugriestarres, which foht was readly and an, e\n",
      "srue before it is abygatiral-sowaker of hing in thit edo extemst of the sciensilitious of the \"divelady\n",
      "on saymses, not higherhes, histure. that it eibled\n",
      "is why the not a, occumanquer faghing would meanzey, as a knoway certains for leavine crudess-ally inter age, a higners doasc, alook serfection, bken poward him atay to courtrment is a u---在温度1.2下生成文本---\n",
      " alook serfection, bken poward him atay to courtrment is a unxomeroust, toor\n",
      " schopenhadicm of hinge the ghearerity wort\" at\n",
      " duse spector which i. braatey? denatest-anfore onarl),\n",
      "who com\n",
      "uty ar the ranquesl:-i dor ane h=we zealistnor, unves: which secre or dinf-rigkamicy, fitgy in\n",
      "oness vidwowqulen. bee? act cate\"\n",
      "kpersows-bach be-nally is sevols, in mucher aginate thated of s\" things \"poist\n",
      "aeritubly\" othys, shield,\n",
      "imlis ethatingable.\n",
      "\n",
      "\n",
      "faid als but no开始第2轮\n",
      "1565/1565 [==============================] - 10s 6ms/step - loss: 1.6150\n",
      "---生成文本---\n",
      " . that as men of the \"historical sense\" we have\n",
      "our virtues,---在温度0.2下生成文本---\n",
      ". that as men of the \"historical sense\" we have\n",
      "our virtues, the stronger to the sensible to the extent of the stice as a perhaps as all the most would be the stronger that the self-clease and the most consequently to the sensible as a still to the self-such a still to the same of the sensible as the self-superticism and the southess of the such a sensible as the sensible of the sensible to the sensible to the sensible to the stronger to the still to the s---在温度0.5下生成文本---\n",
      "nsible to the sensible to the stronger to the still to the sensible in the incention of a good that one of the racally to strong to inversent and should be the same as mankers from the refulters with the will one of a \"thing in not of the forther so the man\n",
      "out of the result in a believe of the generate and that he which is the her and concerded to be all that in the bolly been the same recompurion of the self--most consequently to something the stilling a---在温度1.0下生成文本---\n",
      "n of the self--most consequently to something the stilling as one that toichly be have men something\n",
      "that does can wishen, a deged and to as lamac \"troukerapes, justic without it of a bliefful to very in, is taking\n",
      "and the good besist primive however. the enchices better intell meforully yon\n",
      ", with the licbibleneps it soot that detmist\n",
      "doest. they really peressions, he certainty, upon ode of no one have in the . begn discigguish acbnevoonmble, to abistally---在温度1.2下生成文本---\n",
      "e have in the . begn discigguish acbnevoonmble, to abistally best impoutur socisedpresing\n",
      "or to howrgein of that fanrionoc! davether. being.\n",
      "\n",
      "juuss of lixer, but thing fun of the wa.cility from\n",
      "arion at\n",
      "in quices, ble.\n",
      "\n",
      "18. to it, to toaksery king must\n",
      "aingm contendent lampled, you net the self-del to apprype encistncuss; but be holait,\n",
      "in the coming! but out of lish,\n",
      "estimere \"gial not comes \"nohelly of our that be elrefightengeally\n",
      "a wyodans!\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "geared开始第3轮\n",
      "1565/1565 [==============================] - 17s 11ms/step - loss: 1.5258\n",
      "---生成文本---\n",
      " always ambiguous, always inscrutable;\n",
      "neither is it his \"wor---在温度0.2下生成文本---\n",
      "always ambiguous, always inscrutable;\n",
      "neither is it his \"world of the sense of the greater to the sense of the self-contention of a consequence of the self-considerent and intention of the sense of the sense of the consequence of the still and instincts and the more former and consequence of the consequence of the sense of the self-consimulation of the consequently to the same of the strength of the result and the self-contention of the sense of the same t---在温度0.5下生成文本---\n",
      "he result and the self-contention of the sense of the same the considerence, to the ends the strength in every one of the considerent and the present as this refined of dreams,\n",
      "able to the sense and not and our desire in the philosophy supersestamently and things between for the sense, the problem in free wanters, and one would new consequently an includeness and it, the truth of all problem\n",
      "greater and superstical self-consicals by a consequently in the r---在温度1.0下生成文本---\n",
      "er and superstical self-consicals by a consequently in the rife of a learnen literan allow\n",
      "resultments.\n",
      "\n",
      "      to to a latern suffering exebucate\n",
      "your are not of aughty to a\n",
      "lacks and new mattersly in must as feel for the yet them iffee, and bro-gurating will\n",
      "be impul, long of morality--insupicully\n",
      "pleasuenessover of old conductifest our one which far the metaphysicrely endead. the ser\n",
      "time, reflic\n",
      "in distrulty hand such as some pureds,\n",
      "the self-regard nec---在温度1.2下生成文本---\n",
      "c\n",
      "in distrulty hand such as some pureds,\n",
      "the self-regard necessable? and wong one effectaliss\n",
      "to\n",
      "-action the refarence,\". codbite, iffac\n",
      "infagurence.--he usevers moralies of gorcaven\n",
      "and\n",
      "obselves. indifferetatereblies of encwar\n",
      "have the\n",
      "butica.  over hearthood if the power? is more remignent quitaxely imb-int is when-forthunest it of \"sub; with hel, to cax allittoy tanks in viftiess prsiota-led, but\n",
      "heirarifing to vengest of\n",
      "spirit of vomially\n",
      "intek-lojuat开始第4轮\n",
      "1565/1565 [==============================] - 20s 13ms/step - loss: 1.4797\n",
      "---生成文本---\n",
      " counter to our vanity.\n",
      "\n",
      "177. with regard to what \"truthfulne---在温度0.2下生成文本---\n",
      "counter to our vanity.\n",
      "\n",
      "177. with regard to what \"truthfulness and souls, and the soul, and the surrempons and the soul, and the souls and the strength the soul and conscience of the means and souls, and the same and the conscience of the soul, and consequent in the same and the sense of the present and the present the sense of the world, and and the soul, and the sense and the strength and the soul and constitute of the father and consequent and the soul,---在温度0.5下生成文本---\n",
      "ul and constitute of the father and consequent and the soul, the constitute and the constitute wh"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generated_text):\n\u001b[0;32m     19\u001b[0m     sampled[\u001b[38;5;241m0\u001b[39m, t, char_indices[char]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m---> 21\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m next_index \u001b[38;5;241m=\u001b[39m sample(preds, temperature)\n\u001b[0;32m     23\u001b[0m next_char \u001b[38;5;241m=\u001b[39m chars[next_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\engine\\data_adapter.py:349\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    347\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 349\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\keras\\engine\\data_adapter.py:390\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[0;32m    388\u001b[0m     )\n\u001b[1;32m--> 390\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    394\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2204\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2204\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5456\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   5454\u001b[0m     num_parallel_calls, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m-> 5456\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m   5457\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   5459\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m   5460\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m   5461\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic,\n\u001b[0;32m   5462\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m   5463\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5464\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m   5465\u001b[0m \u001b[38;5;28msuper\u001b[39m(ParallelMapDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLearning-py310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:6023\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   6021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   6022\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6023\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6024\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParallelMapDatasetV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6025\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6026\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6027\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeterministic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6028\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6030\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}