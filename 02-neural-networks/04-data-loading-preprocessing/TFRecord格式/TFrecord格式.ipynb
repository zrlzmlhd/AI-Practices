{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# TFRecord 数据格式详解\n",
    "\n",
    "TFRecord 是 TensorFlow 生态系统中用于高效存储和读取大规模数据集的二进制文件格式。本教程从原理到实践系统讲解 TFRecord 的核心概念。\n",
    "\n",
    "## 核心知识点\n",
    "\n",
    "1. TFRecord 格式的设计原理与性能优势\n",
    "2. 基于 Protocol Buffers 的数据序列化机制\n",
    "3. 压缩策略与 I/O 性能权衡\n",
    "4. 生产环境中的分片与并行读取策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. TFRecord 设计原理\n",
    "\n",
    "### 1.1 解决的核心问题\n",
    "\n",
    "在大规模机器学习场景中，数据 I/O 常成为训练瓶颈。考虑 ImageNet 数据集：\n",
    "\n",
    "- **120 万张图像**意味着 120 万次文件系统调用\n",
    "- 每次 `open() -> read() -> close()` 都涉及磁盘寻道\n",
    "- 机械硬盘寻道时间约 10ms，SSD 约 0.1ms，但频繁小文件访问仍是瓶颈\n",
    "\n",
    "TFRecord 的解决方案：**将大量小文件合并为少量大文件**。\n",
    "\n",
    "### 1.2 文件结构\n",
    "\n",
    "TFRecord 文件由连续的记录（Record）组成，每条记录的二进制结构：\n",
    "\n",
    "```\n",
    "[length: uint64][length_crc: uint32][data: byte[length]][data_crc: uint32]\n",
    "```\n",
    "\n",
    "- `length`: 数据长度（8 字节）\n",
    "- `length_crc`: 长度字段的 CRC32 校验（4 字节）\n",
    "- `data`: 实际数据内容\n",
    "- `data_crc`: 数据的 CRC32 校验（4 字节）\n",
    "\n",
    "CRC 校验确保数据在存储和传输过程中的完整性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# 环境信息\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")\n",
    "print(f\"Eager 模式: {tf.executing_eagerly()}\")\n",
    "\n",
    "# 创建临时工作目录\n",
    "WORK_DIR = Path(tempfile.mkdtemp(prefix=\"tfrecord_demo_\"))\n",
    "print(f\"工作目录: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## 2. TFRecord 基础读写\n",
    "\n",
    "### 2.1 写入原始字节数据\n",
    "\n",
    "`TFRecordWriter` 接受任意字节序列。在底层，它会自动添加长度前缀和 CRC 校验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础写入示例\n",
    "basic_tfrecord = WORK_DIR / \"basic_example.tfrecord\"\n",
    "\n",
    "# 准备测试数据\n",
    "records = [\n",
    "    b\"TFRecord stores arbitrary binary data\",\n",
    "    b\"Each record is length-prefixed with CRC32 checksum\",\n",
    "    b\"Sequential access pattern optimizes disk I/O\",\n",
    "]\n",
    "\n",
    "# 使用上下文管理器确保资源正确释放\n",
    "with tf.io.TFRecordWriter(str(basic_tfrecord)) as writer:\n",
    "    for record in records:\n",
    "        writer.write(record)\n",
    "\n",
    "# 验证写入结果\n",
    "file_size = basic_tfrecord.stat().st_size\n",
    "raw_size = sum(len(r) for r in records)\n",
    "overhead = file_size - raw_size\n",
    "\n",
    "print(f\"原始数据: {raw_size} bytes\")\n",
    "print(f\"文件大小: {file_size} bytes\")\n",
    "print(f\"格式开销: {overhead} bytes ({overhead / len(records):.1f} bytes/record)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "### 2.2 读取 TFRecord\n",
    "\n",
    "`TFRecordDataset` 返回 `tf.data.Dataset` 对象，可无缝集成到训练流水线中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集对象\n",
    "dataset = tf.data.TFRecordDataset(str(basic_tfrecord))\n",
    "\n",
    "# 检查数据集规格\n",
    "print(f\"元素类型: {dataset.element_spec.dtype}\")\n",
    "print(f\"元素形状: {dataset.element_spec.shape}\")\n",
    "print()\n",
    "\n",
    "# 遍历读取\n",
    "for idx, raw_record in enumerate(dataset):\n",
    "    # 返回的是 tf.Tensor，dtype=tf.string（表示字节序列）\n",
    "    content = raw_record.numpy().decode(\"utf-8\")\n",
    "    print(f\"[{idx}] {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 3. 压缩 TFRecord\n",
    "\n",
    "### 3.1 压缩的适用场景\n",
    "\n",
    "| 场景 | 推荐策略 |\n",
    "|------|----------|\n",
    "| 网络传输（云存储） | GZIP，最大化压缩比 |\n",
    "| 本地 SSD | 不压缩或 ZLIB，避免 CPU 成为瓶颈 |\n",
    "| 高重复数据（如文本） | GZIP，压缩收益大 |\n",
    "| 已压缩数据（如 JPEG） | 不压缩，二次压缩收益小 |\n",
    "\n",
    "### 3.2 压缩格式对比\n",
    "\n",
    "- **GZIP**: 压缩比高（通常 60-80%），解压较慢\n",
    "- **ZLIB**: 压缩比与 GZIP 接近，解压稍快\n",
    "- **不压缩**: 零 CPU 开销，适合 SSD + 低重复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成具有重复模式的测试数据（模拟文本或结构化数据）\n",
    "test_records = [\n",
    "    b\"feature_vector:\" + np.random.bytes(100) + b\"label:0\" * 50,\n",
    "    b\"feature_vector:\" + np.random.bytes(100) + b\"label:1\" * 50,\n",
    "    b\"feature_vector:\" + np.random.bytes(100) + b\"label:2\" * 50,\n",
    "]\n",
    "\n",
    "# 定义三种写入配置\n",
    "configs = {\n",
    "    \"uncompressed\": None,\n",
    "    \"gzip\": tf.io.TFRecordOptions(compression_type=\"GZIP\"),\n",
    "    \"zlib\": tf.io.TFRecordOptions(compression_type=\"ZLIB\"),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, options in configs.items():\n",
    "    filepath = WORK_DIR / f\"compression_{name}.tfrecord\"\n",
    "    with tf.io.TFRecordWriter(str(filepath), options=options) as writer:\n",
    "        for record in test_records:\n",
    "            writer.write(record)\n",
    "    results[name] = filepath.stat().st_size\n",
    "\n",
    "# 输出对比结果\n",
    "raw_size = sum(len(r) for r in test_records)\n",
    "print(f\"原始数据大小: {raw_size} bytes\\n\")\n",
    "print(\"压缩效果对比:\")\n",
    "print(\"-\" * 45)\n",
    "for name, size in results.items():\n",
    "    ratio = (1 - size / raw_size) * 100\n",
    "    print(f\"{name:15} | {size:6} bytes | 压缩率 {ratio:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取压缩文件时必须指定相同的压缩类型\n",
    "gzip_path = WORK_DIR / \"compression_gzip.tfrecord\"\n",
    "\n",
    "# 正确方式：指定压缩类型\n",
    "compressed_ds = tf.data.TFRecordDataset(\n",
    "    str(gzip_path),\n",
    "    compression_type=\"GZIP\"\n",
    ")\n",
    "\n",
    "print(\"读取 GZIP 压缩文件:\")\n",
    "for idx, record in enumerate(compressed_ds):\n",
    "    print(f\"[{idx}] 长度: {len(record.numpy())} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 4. Protocol Buffers 与 tf.train.Example\n",
    "\n",
    "### 4.1 结构化数据的挑战\n",
    "\n",
    "原始字节流无法表达复杂的数据结构。例如，一个图像分类样本需要存储：\n",
    "\n",
    "- 图像原始像素（或编码后的 JPEG/PNG）\n",
    "- 图像尺寸（高度、宽度、通道数）\n",
    "- 分类标签\n",
    "- 元数据（文件名、采集时间等）\n",
    "\n",
    "**Protocol Buffers** (Protobuf) 是 Google 开发的高效序列化框架，TensorFlow 基于它定义了 `tf.train.Example` 消息类型。\n",
    "\n",
    "### 4.2 tf.train.Feature 数据类型\n",
    "\n",
    "| 类型 | Python 对应 | 典型用途 |\n",
    "|------|-------------|----------|\n",
    "| `BytesList` | `bytes`, `str` | 图像、音频、文本 |\n",
    "| `FloatList` | `float`, `np.float32` | 特征向量、权重 |\n",
    "| `Int64List` | `int`, `np.int64` | 标签、索引、计数 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_feature(value):\n",
    "    \"\"\"将字节或字符串转换为 BytesList Feature\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.encode(\"utf-8\")\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"将浮点数或数组转换为 FloatList Feature\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = value.flatten().tolist()\n",
    "    elif not isinstance(value, (list, tuple)):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"将整数或数组转换为 Int64List Feature\"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = value.flatten().tolist()\n",
    "    elif not isinstance(value, (list, tuple)):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "# 验证辅助函数\n",
    "print(\"BytesList:\", bytes_feature(\"hello\"))\n",
    "print(\"FloatList:\", float_feature([1.0, 2.0, 3.0]))\n",
    "print(\"Int64List:\", int64_feature(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "### 4.3 构建 tf.train.Example\n",
    "\n",
    "`tf.train.Example` 本质上是一个特征字典，键为字符串，值为 `tf.train.Feature`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟图像分类样本\n",
    "np.random.seed(42)\n",
    "\n",
    "sample_image = np.random.randint(0, 256, size=(32, 32, 3), dtype=np.uint8)\n",
    "sample_label = 5\n",
    "sample_id = \"img_00001\"\n",
    "sample_embedding = np.random.randn(128).astype(np.float32)\n",
    "\n",
    "# 构建 Example\n",
    "example = tf.train.Example(\n",
    "    features=tf.train.Features(\n",
    "        feature={\n",
    "            # 图像数据：序列化为字节存储\n",
    "            \"image/encoded\": bytes_feature(sample_image.tobytes()),\n",
    "            \"image/height\": int64_feature(sample_image.shape[0]),\n",
    "            \"image/width\": int64_feature(sample_image.shape[1]),\n",
    "            \"image/channels\": int64_feature(sample_image.shape[2]),\n",
    "            # 标签\n",
    "            \"label\": int64_feature(sample_label),\n",
    "            # 元数据\n",
    "            \"sample_id\": bytes_feature(sample_id),\n",
    "            # 预计算的特征向量\n",
    "            \"embedding\": float_feature(sample_embedding),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# 序列化为字节\n",
    "serialized = example.SerializeToString()\n",
    "print(f\"序列化后大小: {len(serialized)} bytes\")\n",
    "print(f\"原始数据大小: {sample_image.nbytes + sample_embedding.nbytes} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量写入多个样本\n",
    "structured_tfrecord = WORK_DIR / \"structured_samples.tfrecord\"\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "with tf.io.TFRecordWriter(str(structured_tfrecord)) as writer:\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        # 生成随机样本\n",
    "        img = np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8)\n",
    "        label = np.random.randint(0, 10)\n",
    "        emb = np.random.randn(128).astype(np.float32)\n",
    "        \n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"image/encoded\": bytes_feature(img.tobytes()),\n",
    "                    \"image/height\": int64_feature(32),\n",
    "                    \"image/width\": int64_feature(32),\n",
    "                    \"image/channels\": int64_feature(3),\n",
    "                    \"label\": int64_feature(label),\n",
    "                    \"sample_id\": bytes_feature(f\"sample_{i:05d}\"),\n",
    "                    \"embedding\": float_feature(emb),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "file_size_kb = structured_tfrecord.stat().st_size / 1024\n",
    "print(f\"写入 {NUM_SAMPLES} 个样本\")\n",
    "print(f\"文件大小: {file_size_kb:.1f} KB\")\n",
    "print(f\"平均每样本: {file_size_kb * 1024 / NUM_SAMPLES:.0f} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "### 4.4 解析 tf.train.Example\n",
    "\n",
    "解析时需要提供与写入一致的特征描述（Feature Description），描述每个特征的类型和形状。\n",
    "\n",
    "**关键概念**：\n",
    "- `FixedLenFeature`: 固定长度特征，缺失时报错\n",
    "- `VarLenFeature`: 变长特征，返回 SparseTensor\n",
    "- `FixedLenSequenceFeature`: 固定长度的序列特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义特征描述\n",
    "feature_description = {\n",
    "    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/channels\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"sample_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"embedding\": tf.io.FixedLenFeature([128], tf.float32),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    \"\"\"解析单个序列化的 Example\"\"\"\n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "    # 解码图像数据\n",
    "    height = tf.cast(parsed[\"image/height\"], tf.int32)\n",
    "    width = tf.cast(parsed[\"image/width\"], tf.int32)\n",
    "    channels = tf.cast(parsed[\"image/channels\"], tf.int32)\n",
    "    \n",
    "    image = tf.io.decode_raw(parsed[\"image/encoded\"], tf.uint8)\n",
    "    image = tf.reshape(image, [height, width, channels])\n",
    "    \n",
    "    # 归一化到 [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"embedding\": parsed[\"embedding\"],\n",
    "        \"sample_id\": parsed[\"sample_id\"],\n",
    "    }, parsed[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建高效的数据流水线\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = (\n",
    "    tf.data.TFRecordDataset(str(structured_tfrecord))\n",
    "    .map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(buffer_size=100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 验证流水线输出\n",
    "print(\"数据集元素规格:\")\n",
    "print(dataset.element_spec)\n",
    "print()\n",
    "\n",
    "for features, labels in dataset.take(1):\n",
    "    print(f\"图像批次: {features['image'].shape}\")\n",
    "    print(f\"嵌入批次: {features['embedding'].shape}\")\n",
    "    print(f\"标签批次: {labels.shape}\")\n",
    "    print(f\"图像值域: [{features['image'].numpy().min():.3f}, {features['image'].numpy().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "## 5. 生产环境最佳实践：分片存储\n",
    "\n",
    "### 5.1 为什么需要分片\n",
    "\n",
    "单个超大 TFRecord 文件存在以下问题：\n",
    "\n",
    "1. **并行读取受限**: 单文件只能顺序读取\n",
    "2. **故障恢复困难**: 文件损坏可能丢失全部数据\n",
    "3. **分布式训练**: 多 Worker 无法高效分配数据\n",
    "\n",
    "### 5.2 分片策略\n",
    "\n",
    "**推荐配置**：\n",
    "- 每个分片 100-200 MB\n",
    "- 分片数量 >= Worker 数量 × 10（确保负载均衡）\n",
    "- 文件命名: `data-00000-of-00100.tfrecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分片写入示例\n",
    "NUM_SHARDS = 4\n",
    "SAMPLES_PER_SHARD = 25\n",
    "\n",
    "shard_paths = []\n",
    "for shard_id in range(NUM_SHARDS):\n",
    "    shard_name = f\"data-{shard_id:05d}-of-{NUM_SHARDS:05d}.tfrecord\"\n",
    "    shard_path = WORK_DIR / shard_name\n",
    "    shard_paths.append(str(shard_path))\n",
    "    \n",
    "    with tf.io.TFRecordWriter(str(shard_path)) as writer:\n",
    "        for i in range(SAMPLES_PER_SHARD):\n",
    "            global_idx = shard_id * SAMPLES_PER_SHARD + i\n",
    "            img = np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8)\n",
    "            label = np.random.randint(0, 10)\n",
    "            \n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        \"image/encoded\": bytes_feature(img.tobytes()),\n",
    "                        \"image/height\": int64_feature(32),\n",
    "                        \"image/width\": int64_feature(32),\n",
    "                        \"image/channels\": int64_feature(3),\n",
    "                        \"label\": int64_feature(label),\n",
    "                        \"sample_id\": bytes_feature(f\"sample_{global_idx:05d}\"),\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    \n",
    "    print(f\"分片 {shard_id}: {shard_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n共创建 {NUM_SHARDS} 个分片，{NUM_SHARDS * SAMPLES_PER_SHARD} 个样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### 5.3 使用 interleave 并行读取\n",
    "\n",
    "`interleave` 操作从多个数据源交错读取，最大化 I/O 吞吐量。\n",
    "\n",
    "**关键参数**：\n",
    "- `cycle_length`: 同时读取的文件数\n",
    "- `num_parallel_calls`: 并行处理的线程数\n",
    "- `deterministic`: 设为 False 可提升性能（但结果不可复现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化的解析函数（仅用于分片数据）\n",
    "shard_feature_desc = {\n",
    "    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/channels\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"sample_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_shard_example(serialized):\n",
    "    \"\"\"解析分片数据\"\"\"\n",
    "    parsed = tf.io.parse_single_example(serialized, shard_feature_desc)\n",
    "    \n",
    "    image = tf.io.decode_raw(parsed[\"image/encoded\"], tf.uint8)\n",
    "    image = tf.reshape(image, [32, 32, 3])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return image, parsed[\"label\"]\n",
    "\n",
    "\n",
    "# 构建分片读取流水线\n",
    "files_dataset = tf.data.Dataset.from_tensor_slices(shard_paths)\n",
    "\n",
    "# 文件级打乱\n",
    "files_dataset = files_dataset.shuffle(buffer_size=NUM_SHARDS)\n",
    "\n",
    "# 并行交错读取\n",
    "sharded_dataset = files_dataset.interleave(\n",
    "    lambda filepath: tf.data.TFRecordDataset(filepath),\n",
    "    cycle_length=NUM_SHARDS,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False\n",
    ")\n",
    "\n",
    "# 完整流水线\n",
    "sharded_dataset = (\n",
    "    sharded_dataset\n",
    "    .map(parse_shard_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(buffer_size=100)\n",
    "    .batch(8)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 验证\n",
    "total_samples = 0\n",
    "for images, labels in sharded_dataset:\n",
    "    total_samples += images.shape[0]\n",
    "\n",
    "print(f\"从 {NUM_SHARDS} 个分片读取 {total_samples} 个样本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## 6. 实战：MNIST 数据集转换\n",
    "\n",
    "演示完整的数据集转换流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST（使用子集进行演示）\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 取前 200 个样本用于快速演示\n",
    "x_demo, y_demo = x_train[:200], y_train[:200]\n",
    "\n",
    "print(f\"演示数据: {x_demo.shape}, 标签分布: {np.bincount(y_demo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_example(image, label):\n",
    "    \"\"\"创建 MNIST 样本的 tf.train.Example\"\"\"\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"image\": bytes_feature(image.tobytes()),\n",
    "                \"label\": int64_feature(int(label)),\n",
    "                \"height\": int64_feature(image.shape[0]),\n",
    "                \"width\": int64_feature(image.shape[1]),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 写入 TFRecord\n",
    "mnist_tfrecord = WORK_DIR / \"mnist_demo.tfrecord\"\n",
    "\n",
    "with tf.io.TFRecordWriter(str(mnist_tfrecord)) as writer:\n",
    "    for img, lbl in zip(x_demo, y_demo):\n",
    "        example = create_mnist_example(img, lbl)\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "print(f\"MNIST TFRecord: {mnist_tfrecord.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 解析函数\n",
    "mnist_feature_desc = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def parse_mnist(serialized):\n",
    "    \"\"\"解析 MNIST TFRecord\"\"\"\n",
    "    parsed = tf.io.parse_single_example(serialized, mnist_feature_desc)\n",
    "    \n",
    "    image = tf.io.decode_raw(parsed[\"image\"], tf.uint8)\n",
    "    image = tf.reshape(image, [28, 28, 1])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return image, parsed[\"label\"]\n",
    "\n",
    "\n",
    "# 构建训练流水线\n",
    "mnist_dataset = (\n",
    "    tf.data.TFRecordDataset(str(mnist_tfrecord))\n",
    "    .map(parse_mnist, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(200)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 验证\n",
    "for images, labels in mnist_dataset.take(1):\n",
    "    print(f\"图像: {images.shape}, 标签: {labels.shape}\")\n",
    "    print(f\"值域: [{images.numpy().min():.3f}, {images.numpy().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "## 7. 清理资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(WORK_DIR)\n",
    "print(f\"已清理: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **TFRecord 本质**: 长度前缀 + CRC 校验的二进制记录序列\n",
    "2. **Protocol Buffers**: `tf.train.Example` 提供结构化数据的标准表示\n",
    "3. **压缩策略**: 根据数据类型和部署环境选择 GZIP/ZLIB/不压缩\n",
    "4. **分片存储**: 生产环境必备，支持并行读取和故障隔离\n",
    "\n",
    "### 性能优化清单\n",
    "\n",
    "| 优化点 | 方法 |\n",
    "|--------|------|\n",
    "| I/O 瓶颈 | 分片 + `interleave()` 并行读取 |\n",
    "| CPU 瓶颈 | `map(num_parallel_calls=AUTOTUNE)` |\n",
    "| GPU 空闲 | `prefetch(AUTOTUNE)` |\n",
    "| 内存不足 | 流式处理，避免一次性加载 |\n",
    "\n",
    "### 参考文档\n",
    "\n",
    "- [TFRecord 官方指南](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "- [tf.data 性能优化](https://www.tensorflow.org/guide/data_performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
