{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# tf.data API 数据流水线构建\n",
    "\n",
    "`tf.data` 是 TensorFlow 中构建高性能数据输入流水线的核心 API。本教程系统讲解数据集创建、转换操作和性能优化策略。\n",
    "\n",
    "## 核心知识点\n",
    "\n",
    "1. Dataset 对象的创建方法与适用场景\n",
    "2. 链式转换操作的执行语义\n",
    "3. 数据打乱机制与缓冲区策略\n",
    "4. 性能优化：并行化与预取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 固定随机种子确保结果可复现\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Eager 执行: {tf.executing_eagerly()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. 数据集创建\n",
    "\n",
    "### 1.1 from_tensor_slices：内存数据\n",
    "\n",
    "将内存中的张量沿第一维度切分，每个切片成为独立样本。\n",
    "\n",
    "**适用场景**：\n",
    "- 数据量 < 内存容量的 1/4\n",
    "- 快速原型验证\n",
    "- 小规模实验\n",
    "\n",
    "**注意**：此方法会将数据复制到 TensorFlow 内存空间，大数据集应使用流式加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从一维张量创建\n",
    "data = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# 检查数据集规格\n",
    "print(f\"元素规格: {dataset.element_spec}\")\n",
    "print(f\"数据类型: {dataset.element_spec.dtype}\")\n",
    "print(f\"元素形状: {dataset.element_spec.shape}\")\n",
    "print()\n",
    "\n",
    "# 遍历数据集\n",
    "print(\"数据集内容:\", [x.numpy() for x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从特征-标签对创建（监督学习场景）\n",
    "features = np.random.randn(100, 5).astype(np.float32)\n",
    "labels = np.random.randint(0, 3, size=(100,))\n",
    "\n",
    "# 元组形式：(features, labels)\n",
    "paired_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "print(f\"配对数据集规格:\")\n",
    "print(f\"  特征: {paired_dataset.element_spec[0]}\")\n",
    "print(f\"  标签: {paired_dataset.element_spec[1]}\")\n",
    "print()\n",
    "\n",
    "# 取样验证\n",
    "for feat, lbl in paired_dataset.take(2):\n",
    "    print(f\"特征: {feat.numpy()[:3]}..., 标签: {lbl.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从字典创建（多输入模型场景）\n",
    "dict_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"numeric\": np.random.randn(50, 3).astype(np.float32),\n",
    "    \"categorical\": np.random.randint(0, 10, size=(50,)),\n",
    "    \"label\": np.random.randint(0, 2, size=(50,)),\n",
    "})\n",
    "\n",
    "print(\"字典数据集规格:\")\n",
    "for key, spec in dict_dataset.element_spec.items():\n",
    "    print(f\"  {key}: {spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "### 1.2 Dataset.range：整数序列\n",
    "\n",
    "直接创建整数序列，无需先构建张量。常用于索引生成或简单测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类似 Python range()\n",
    "range_ds = tf.data.Dataset.range(5)  # 0, 1, 2, 3, 4\n",
    "print(\"range(5):\", [x.numpy() for x in range_ds])\n",
    "\n",
    "# 指定起止和步长\n",
    "range_ds2 = tf.data.Dataset.range(10, 20, 2)  # 10, 12, 14, 16, 18\n",
    "print(\"range(10, 20, 2):\", [x.numpy() for x in range_ds2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 2. 核心转换操作\n",
    "\n",
    "### 2.1 map：元素级转换\n",
    "\n",
    "`map(func)` 对每个元素应用转换函数。\n",
    "\n",
    "**关键参数**：\n",
    "- `num_parallel_calls`: 并行处理线程数，推荐 `tf.data.AUTOTUNE`\n",
    "- `deterministic`: 是否保证输出顺序（默认 True）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单数值变换\n",
    "def square_plus_one(x):\n",
    "    \"\"\"计算 x^2 + 1\"\"\"\n",
    "    return x ** 2 + 1\n",
    "\n",
    "data = tf.data.Dataset.range(6)\n",
    "mapped = data.map(square_plus_one, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"原始:\", [x.numpy() for x in data])\n",
    "print(\"变换:\", [x.numpy() for x in mapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多步变换链\n",
    "pipeline = (\n",
    "    tf.data.Dataset.range(5)\n",
    "    .map(lambda x: x * 2)                    # 乘以 2\n",
    "    .map(lambda x: tf.cast(x, tf.float32))   # 转浮点\n",
    "    .map(lambda x: x / 10.0)                 # 归一化\n",
    ")\n",
    "\n",
    "print(\"链式 map:\", [f\"{x.numpy():.2f}\" for x in pipeline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理特征-标签对\n",
    "def preprocess(features, label):\n",
    "    \"\"\"特征标准化 + 标签转独热\"\"\"\n",
    "    # 假设特征已知均值和标准差\n",
    "    normalized = (features - 0.0) / 1.0\n",
    "    one_hot = tf.one_hot(label, depth=3)\n",
    "    return normalized, one_hot\n",
    "\n",
    "processed = paired_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "for feat, lbl in processed.take(1):\n",
    "    print(f\"归一化特征: {feat.numpy()[:3]}...\")\n",
    "    print(f\"独热标签: {lbl.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "### 2.2 batch：批处理\n",
    "\n",
    "`batch(batch_size)` 将连续元素组合成批次张量。\n",
    "\n",
    "**参数**：\n",
    "- `batch_size`: 批次大小\n",
    "- `drop_remainder`: 是否丢弃最后不完整批次（默认 False）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(10)\n",
    "\n",
    "# 默认保留不完整批次\n",
    "batched = data.batch(3)\n",
    "print(\"batch(3), drop_remainder=False:\")\n",
    "for batch in batched:\n",
    "    print(f\"  形状 {batch.shape}: {batch.numpy()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 丢弃不完整批次（训练时常用）\n",
    "batched_dropped = data.batch(3, drop_remainder=True)\n",
    "print(\"batch(3), drop_remainder=True:\")\n",
    "for batch in batched_dropped:\n",
    "    print(f\"  形状 {batch.shape}: {batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### 2.3 repeat：数据集重复\n",
    "\n",
    "`repeat(count)` 将数据集重复指定次数。\n",
    "\n",
    "- `count=None` 或不指定：无限重复\n",
    "- `count=n`：重复 n 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = tf.data.Dataset.range(3)\n",
    "\n",
    "# 重复 2 次\n",
    "repeated = base.repeat(2)\n",
    "print(\"repeat(2):\", [x.numpy() for x in repeated])\n",
    "print(f\"总元素数: {sum(1 for _ in repeated)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "### 2.4 操作顺序的影响\n",
    "\n",
    "`repeat` 和 `batch` 的调用顺序会产生不同结果：\n",
    "\n",
    "- **先 repeat 后 batch**：重复的数据会跨批次边界\n",
    "- **先 batch 后 repeat**：每个批次独立重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(5)\n",
    "\n",
    "# 先 repeat 后 batch\n",
    "print(\"repeat(2) -> batch(3):\")\n",
    "result1 = data.repeat(2).batch(3)\n",
    "for i, batch in enumerate(result1):\n",
    "    print(f\"  批次 {i}: {batch.numpy()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 先 batch 后 repeat\n",
    "print(\"batch(3) -> repeat(2):\")\n",
    "result2 = data.batch(3).repeat(2)\n",
    "for i, batch in enumerate(result2):\n",
    "    print(f\"  批次 {i}: {batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "### 2.5 filter：条件过滤\n",
    "\n",
    "`filter(predicate)` 保留满足条件的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(20)\n",
    "\n",
    "# 保留偶数\n",
    "even = data.filter(lambda x: x % 2 == 0)\n",
    "print(\"偶数:\", [x.numpy() for x in even])\n",
    "\n",
    "# 组合条件\n",
    "combined = data.filter(lambda x: (x % 2 == 0) & (x > 10))\n",
    "print(\"偶数且>10:\", [x.numpy() for x in combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### 2.6 unbatch：解除批处理\n",
    "\n",
    "`unbatch()` 将批次拆分为单个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched = tf.data.Dataset.range(10).batch(4)\n",
    "print(\"批处理后:\")\n",
    "for batch in batched:\n",
    "    print(f\"  {batch.numpy()}\")\n",
    "\n",
    "# 解除批处理\n",
    "unbatched = batched.unbatch()\n",
    "print(\"\\n解除批处理:\", [x.numpy() for x in unbatched])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## 3. 数据打乱机制\n",
    "\n",
    "### 3.1 shuffle 工作原理\n",
    "\n",
    "`shuffle(buffer_size)` 使用固定大小缓冲区进行随机打乱：\n",
    "\n",
    "1. 从数据源填充 `buffer_size` 个元素到缓冲区\n",
    "2. 从缓冲区随机选取一个元素输出\n",
    "3. 从数据源取下一个元素填补空位\n",
    "4. 重复直到数据源耗尽\n",
    "\n",
    "**关键洞察**：`buffer_size` 决定打乱程度\n",
    "- `buffer_size=1`：无打乱（顺序输出）\n",
    "- `buffer_size=N`（N=数据集大小）：完全打乱\n",
    "- `buffer_size` 介于两者之间：局部打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(10)\n",
    "print(\"原始:\", [x.numpy() for x in data])\n",
    "\n",
    "# 不同 buffer_size 的效果\n",
    "for buf_size in [1, 3, 5, 10]:\n",
    "    shuffled = data.shuffle(buffer_size=buf_size, seed=42)\n",
    "    result = [x.numpy() for x in shuffled]\n",
    "    print(f\"buffer_size={buf_size:2d}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "### 3.2 reshuffle_each_iteration 参数\n",
    "\n",
    "控制每次遍历数据集时是否重新打乱：\n",
    "\n",
    "- `True`（默认）：每个 epoch 顺序不同\n",
    "- `False`：所有 epoch 顺序相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(5)\n",
    "\n",
    "# 默认：每次迭代重新打乱\n",
    "shuffled_default = data.shuffle(5, seed=42, reshuffle_each_iteration=True).repeat(2)\n",
    "result = [x.numpy() for x in shuffled_default]\n",
    "print(f\"reshuffle=True:\")\n",
    "print(f\"  Epoch 1: {result[:5]}\")\n",
    "print(f\"  Epoch 2: {result[5:]}\")\n",
    "\n",
    "# 固定顺序\n",
    "shuffled_fixed = data.shuffle(5, seed=42, reshuffle_each_iteration=False).repeat(2)\n",
    "result = [x.numpy() for x in shuffled_fixed]\n",
    "print(f\"\\nreshuffle=False:\")\n",
    "print(f\"  Epoch 1: {result[:5]}\")\n",
    "print(f\"  Epoch 2: {result[5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "### 3.3 推荐的操作顺序\n",
    "\n",
    "**标准训练流水线顺序**：\n",
    "\n",
    "```\n",
    "shuffle -> map -> batch -> prefetch\n",
    "```\n",
    "\n",
    "**原因**：\n",
    "1. `shuffle` 在 `batch` 前：确保批次内样本随机\n",
    "2. `map` 在 `batch` 前：逐样本预处理（若可向量化则可后置）\n",
    "3. `prefetch` 在最后：流水线优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准训练流水线示例\n",
    "data = tf.data.Dataset.range(20)\n",
    "\n",
    "train_pipeline = (\n",
    "    data\n",
    "    .shuffle(buffer_size=20, seed=42)\n",
    "    .map(lambda x: tf.cast(x, tf.float32) / 20.0, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(4, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"训练流水线输出:\")\n",
    "for i, batch in enumerate(train_pipeline.take(3)):\n",
    "    print(f\"  批次 {i}: {batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "## 4. 数据集切片与合并\n",
    "\n",
    "### 4.1 take 和 skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.range(20)\n",
    "\n",
    "# take: 取前 n 个\n",
    "first_five = data.take(5)\n",
    "print(\"take(5):\", [x.numpy() for x in first_five])\n",
    "\n",
    "# skip: 跳过前 n 个\n",
    "skip_five = data.skip(5)\n",
    "print(\"skip(5):\", [x.numpy() for x in skip_five])\n",
    "\n",
    "# 组合实现切片\n",
    "middle = data.skip(5).take(5)  # 相当于 [5:10]\n",
    "print(\"skip(5).take(5):\", [x.numpy() for x in middle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "### 4.2 concatenate：数据集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = tf.data.Dataset.range(5)\n",
    "ds2 = tf.data.Dataset.range(5, 10)\n",
    "\n",
    "# 顺序合并\n",
    "combined = ds1.concatenate(ds2)\n",
    "print(\"concatenate:\", [x.numpy() for x in combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f7",
   "metadata": {},
   "source": [
    "### 4.3 zip：多数据集配对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    np.random.randn(5, 3).astype(np.float32)\n",
    ")\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices([0, 1, 0, 1, 0])\n",
    "\n",
    "# 配对\n",
    "zipped = tf.data.Dataset.zip((features_ds, labels_ds))\n",
    "\n",
    "print(\"zip 配对:\")\n",
    "for feat, lbl in zipped:\n",
    "    print(f\"  特征: {feat.numpy()[:2]}..., 标签: {lbl.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "## 5. 性能优化\n",
    "\n",
    "### 5.1 prefetch：预取优化\n",
    "\n",
    "`prefetch(buffer_size)` 允许数据准备和模型计算并行执行：\n",
    "\n",
    "- GPU 处理当前批次时，CPU 同时准备下一批次\n",
    "- 有效隐藏数据加载延迟\n",
    "- 推荐使用 `tf.data.AUTOTUNE` 自动调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整优化流水线\n",
    "def simulate_preprocess(x):\n",
    "    \"\"\"模拟耗时预处理\"\"\"\n",
    "    return tf.cast(x, tf.float32) / 100.0\n",
    "\n",
    "optimized = (\n",
    "    tf.data.Dataset.range(100)\n",
    "    .shuffle(buffer_size=100)\n",
    "    .map(simulate_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(16)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"优化流水线:\")\n",
    "for batch in optimized.take(3):\n",
    "    print(f\"  形状: {batch.shape}, 范围: [{batch.numpy().min():.2f}, {batch.numpy().max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d1",
   "metadata": {},
   "source": [
    "### 5.2 interleave：并行读取多文件\n",
    "\n",
    "从多个数据源交错读取，提高 I/O 吞吐量。\n",
    "\n",
    "**关键参数**：\n",
    "- `cycle_length`: 同时读取的数据源数量\n",
    "- `num_parallel_calls`: 并行处理线程数\n",
    "- `deterministic`: False 可提升性能但结果不可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟多文件读取\n",
    "def generate_file_data(file_id):\n",
    "    \"\"\"模拟从文件读取数据\"\"\"\n",
    "    start = file_id * 100\n",
    "    return tf.data.Dataset.range(start, start + 5)\n",
    "\n",
    "# 3 个\"文件\"\n",
    "file_ids = tf.data.Dataset.range(3)\n",
    "\n",
    "# 交错读取\n",
    "interleaved = file_ids.interleave(\n",
    "    generate_file_data,\n",
    "    cycle_length=3,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"interleave 结果:\", [x.numpy() for x in interleaved])\n",
    "print(\"注意: 元素来自不同数据源交替出现\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f3",
   "metadata": {},
   "source": [
    "## 6. 完整训练流水线示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟分类任务数据\n",
    "np.random.seed(42)\n",
    "NUM_SAMPLES = 1000\n",
    "NUM_FEATURES = 10\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "X_train = np.random.randn(NUM_SAMPLES, NUM_FEATURES).astype(np.float32)\n",
    "y_train = np.random.randint(0, NUM_CLASSES, size=(NUM_SAMPLES,))\n",
    "\n",
    "# 预处理函数\n",
    "def normalize_and_onehot(features, label):\n",
    "    \"\"\"特征标准化 + 标签独热编码\"\"\"\n",
    "    normalized = (features - tf.reduce_mean(features)) / tf.math.reduce_std(features)\n",
    "    one_hot = tf.one_hot(label, depth=NUM_CLASSES)\n",
    "    return normalized, one_hot\n",
    "\n",
    "# 构建完整流水线\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(buffer_size=BUFFER_SIZE)\n",
    "    .map(normalize_and_onehot, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 验证\n",
    "print(\"训练数据集规格:\")\n",
    "print(f\"  特征: {train_dataset.element_spec[0]}\")\n",
    "print(f\"  标签: {train_dataset.element_spec[1]}\")\n",
    "print()\n",
    "\n",
    "for features, labels in train_dataset.take(1):\n",
    "    print(f\"特征批次: {features.shape}\")\n",
    "    print(f\"标签批次: {labels.shape}\")\n",
    "    print(f\"特征统计: 均值={features.numpy().mean():.4f}, 标准差={features.numpy().std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b5",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 核心操作速查\n",
    "\n",
    "| 操作 | 用途 | 关键参数 |\n",
    "|------|------|----------|\n",
    "| `from_tensor_slices()` | 内存数据创建 | data |\n",
    "| `range()` | 整数序列 | start, stop, step |\n",
    "| `map()` | 元素级转换 | `num_parallel_calls=AUTOTUNE` |\n",
    "| `filter()` | 条件过滤 | predicate |\n",
    "| `shuffle()` | 随机打乱 | `buffer_size` |\n",
    "| `batch()` | 批处理 | `drop_remainder` |\n",
    "| `repeat()` | 数据集重复 | count |\n",
    "| `prefetch()` | 预取优化 | `AUTOTUNE` |\n",
    "| `interleave()` | 并行读取 | `cycle_length` |\n",
    "\n",
    "### 最佳实践\n",
    "\n",
    "1. **操作顺序**: `shuffle -> map -> batch -> prefetch`\n",
    "2. **并行化**: 始终使用 `num_parallel_calls=tf.data.AUTOTUNE`\n",
    "3. **预取**: 流水线末尾添加 `prefetch(tf.data.AUTOTUNE)`\n",
    "4. **大数据集**: 使用 `interleave` 并行读取多个分片文件\n",
    "\n",
    "### 参考文档\n",
    "\n",
    "- [tf.data 性能优化指南](https://www.tensorflow.org/guide/data_performance)\n",
    "- [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
