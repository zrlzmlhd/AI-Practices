{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 深度神经网络权重初始化策略\n",
    "\n",
    "## 核心问题：梯度消失与梯度爆炸\n",
    "\n",
    "深度神经网络训练中，权重初始化至关重要。不当的初始化会导致：\n",
    "\n",
    "- **梯度消失**：梯度在反向传播中逐层衰减，深层网络无法有效学习\n",
    "- **梯度爆炸**：梯度指数增长，导致权重更新过大，训练不稳定\n",
    "\n",
    "## 初始化方法演进\n",
    "\n",
    "| 方法 | 提出者 | 适用激活函数 | 方差公式 |\n",
    "|------|--------|--------------|----------|\n",
    "| Xavier/Glorot | Glorot & Bengio (2010) | sigmoid, tanh | σ² = 2/(fan_in + fan_out) |\n",
    "| He | He et al. (2015) | ReLU 及其变体 | σ² = 2/fan_in |\n",
    "| LeCun | LeCun et al. (1998) | SELU | σ² = 1/fan_in |\n",
    "\n",
    "其中 `fan_in` 为输入神经元数量，`fan_out` 为输出神经元数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. Keras 内置初始化器\n",
    "\n",
    "### 1.1 He 初始化（推荐用于 ReLU）\n",
    "\n",
    "He 初始化专为 ReLU 激活函数设计，考虑到 ReLU 会将负值置零，因此需要更大的初始方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He 均匀分布初始化\n",
    "# 权重从 [-limit, limit] 均匀采样，其中 limit = sqrt(6/fan_in)\n",
    "layer_he_uniform = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_uniform',\n",
    "    name='he_uniform_layer'\n",
    ")\n",
    "\n",
    "# He 正态分布初始化\n",
    "# 权重从均值为0、标准差为 sqrt(2/fan_in) 的正态分布采样\n",
    "layer_he_normal = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal',\n",
    "    name='he_normal_layer'\n",
    ")\n",
    "\n",
    "print(\"He 初始化层创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "### 1.2 Glorot/Xavier 初始化（默认初始化器）\n",
    "\n",
    "Glorot 初始化是 Keras Dense 层的默认初始化方式，适用于 sigmoid 和 tanh 激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glorot 均匀分布（Keras Dense 层默认）\n",
    "layer_glorot_uniform = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='tanh',\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    name='glorot_uniform_layer'\n",
    ")\n",
    "\n",
    "# Glorot 正态分布\n",
    "layer_glorot_normal = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='tanh',\n",
    "    kernel_initializer='glorot_normal',\n",
    "    name='glorot_normal_layer'\n",
    ")\n",
    "\n",
    "print(\"Glorot 初始化层创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "### 1.3 LeCun 初始化（用于 SELU）\n",
    "\n",
    "LeCun 初始化与 SELU 激活函数配合使用，可实现自归一化特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeCun 正态分布初始化\n",
    "layer_lecun = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='selu',\n",
    "    kernel_initializer='lecun_normal',\n",
    "    name='lecun_layer'\n",
    ")\n",
    "\n",
    "print(\"LeCun 初始化层创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 2. 自定义初始化器\n",
    "\n",
    "使用 `VarianceScaling` 可以灵活配置初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 VarianceScaling 初始化器\n",
    "# scale: 缩放因子\n",
    "# mode: 'fan_in'（输入）, 'fan_out'（输出）, 'fan_avg'（平均）\n",
    "# distribution: 'uniform'（均匀）或 'truncated_normal'（截断正态）\n",
    "\n",
    "custom_initializer = keras.initializers.VarianceScaling(\n",
    "    scale=2.0,              # He 初始化使用 scale=2\n",
    "    mode='fan_in',          # 基于输入神经元数量\n",
    "    distribution='uniform'  # 均匀分布\n",
    ")\n",
    "\n",
    "layer_custom = keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation='relu',\n",
    "    kernel_initializer=custom_initializer,\n",
    "    name='custom_init_layer'\n",
    ")\n",
    "\n",
    "print(\"自定义初始化器配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 3. 可视化不同初始化方法的权重分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_initializer(initializer, name, input_dim=784, output_dim=256):\n",
    "    \"\"\"\n",
    "    可视化初始化器生成的权重分布\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    initializer : keras.initializers.Initializer\n",
    "        Keras 初始化器实例或字符串名称\n",
    "    name : str\n",
    "        初始化方法名称（用于图表标题）\n",
    "    input_dim : int\n",
    "        输入维度\n",
    "    output_dim : int\n",
    "        输出维度\n",
    "    \"\"\"\n",
    "    # 获取初始化器实例\n",
    "    if isinstance(initializer, str):\n",
    "        init = keras.initializers.get(initializer)\n",
    "    else:\n",
    "        init = initializer\n",
    "    \n",
    "    # 生成权重矩阵\n",
    "    weights = init(shape=(input_dim, output_dim)).numpy()\n",
    "    \n",
    "    # 统计信息\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  均值: {weights.mean():.6f}\")\n",
    "    print(f\"  标准差: {weights.std():.6f}\")\n",
    "    print(f\"  最小值: {weights.min():.6f}\")\n",
    "    print(f\"  最大值: {weights.max():.6f}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# 比较不同初始化方法\n",
    "initializers = {\n",
    "    'He Uniform': 'he_uniform',\n",
    "    'He Normal': 'he_normal',\n",
    "    'Glorot Uniform': 'glorot_uniform',\n",
    "    'Glorot Normal': 'glorot_normal',\n",
    "    'LeCun Normal': 'lecun_normal'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, init) in enumerate(initializers.items()):\n",
    "    weights = visualize_initializer(init, name)\n",
    "    axes[idx].hist(weights.flatten(), bins=50, density=True, alpha=0.7)\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].set_xlabel('权重值')\n",
    "    axes[idx].set_ylabel('密度')\n",
    "    print()\n",
    "\n",
    "# 隐藏多余的子图\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('weight_initialization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 4. 实际应用：比较不同初始化对训练的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(initializer, activation='relu'):\n",
    "    \"\"\"\n",
    "    创建用于测试的深度神经网络\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    initializer : str or keras.initializers.Initializer\n",
    "        权重初始化方法\n",
    "    activation : str\n",
    "        激活函数\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        编译好的模型\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(256, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(128, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(64, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# 使用小数据集快速验证\n",
    "X_train_small = X_train[:5000]\n",
    "y_train_small = y_train[:5000]\n",
    "\n",
    "print(f\"训练集大小: {X_train_small.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较 He 和 Glorot 初始化在 ReLU 网络中的表现\n",
    "results = {}\n",
    "\n",
    "for init_name, initializer in [('He Normal', 'he_normal'), ('Glorot Uniform', 'glorot_uniform')]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"训练使用 {init_name} 初始化的模型\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # 重置随机种子确保公平比较\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    model = create_model(initializer)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_small, y_train_small,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 评估模型\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"测试集准确率: {test_acc:.4f}\")\n",
    "    \n",
    "    results[init_name] = {\n",
    "        'history': history.history,\n",
    "        'test_acc': test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for name, data in results.items():\n",
    "    axes[0].plot(data['history']['accuracy'], label=f'{name} (训练)')\n",
    "    axes[0].plot(data['history']['val_accuracy'], '--', label=f'{name} (验证)')\n",
    "    \n",
    "    axes[1].plot(data['history']['loss'], label=f'{name} (训练)')\n",
    "    axes[1].plot(data['history']['val_loss'], '--', label=f'{name} (验证)')\n",
    "\n",
    "axes[0].set_title('准确率对比')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('损失对比')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('initialization_training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 输出最终结果\n",
    "print(\"\\n最终测试集准确率:\")\n",
    "for name, data in results.items():\n",
    "    print(f\"  {name}: {data['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## 5. 初始化选择指南\n",
    "\n",
    "### 推荐配置\n",
    "\n",
    "| 激活函数 | 推荐初始化 | Keras 参数 |\n",
    "|----------|------------|------------|\n",
    "| ReLU | He | `kernel_initializer='he_normal'` |\n",
    "| Leaky ReLU | He | `kernel_initializer='he_normal'` |\n",
    "| PReLU | He | `kernel_initializer='he_normal'` |\n",
    "| ELU | He | `kernel_initializer='he_normal'` |\n",
    "| SELU | LeCun | `kernel_initializer='lecun_normal'` |\n",
    "| tanh | Glorot | `kernel_initializer='glorot_uniform'` |\n",
    "| sigmoid | Glorot | `kernel_initializer='glorot_uniform'` |\n",
    "| softmax | Glorot | `kernel_initializer='glorot_uniform'` |\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "1. **输出层**：通常使用 Glorot 初始化，与输出激活函数（softmax/sigmoid）匹配\n",
    "2. **批量归一化**：使用 BN 时，初始化方法的影响会减弱\n",
    "3. **残差网络**：建议使用 He 初始化\n",
    "4. **自归一化网络**：SELU + LeCun 初始化 + AlphaDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证所有代码可正常运行\n",
    "print(\"所有单元测试通过！\")\n",
    "print(\"\\n关键要点:\")\n",
    "print(\"1. ReLU 系列激活函数使用 He 初始化\")\n",
    "print(\"2. sigmoid/tanh 使用 Glorot 初始化\")\n",
    "print(\"3. SELU 使用 LeCun 初始化\")\n",
    "print(\"4. 正确的初始化可以加速训练收敛并提高模型性能\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
