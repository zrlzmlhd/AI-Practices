{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Dropout 正则化技术\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "Dropout（Srivastava et al., 2014）是一种简单有效的正则化技术。在训练过程中，随机\"丢弃\"（置零）一部分神经元的输出，迫使网络学习更鲁棒的特征。\n",
    "\n",
    "## 工作原理\n",
    "\n",
    "```\n",
    "训练阶段：\n",
    "- 每个神经元以概率 p 被临时\"关闭\"\n",
    "- 保留的神经元输出需要除以 (1-p) 进行缩放（inverted dropout）\n",
    "\n",
    "推理阶段：\n",
    "- 所有神经元都参与计算\n",
    "- 无需额外缩放（因为训练时已处理）\n",
    "```\n",
    "\n",
    "## 为什么有效\n",
    "\n",
    "| 机制 | 解释 |\n",
    "|------|------|\n",
    "| 集成效果 | 相当于训练多个子网络的集成 |\n",
    "| 减少共适应 | 神经元不能过度依赖其他特定神经元 |\n",
    "| 特征冗余 | 迫使网络学习更分散、更鲁棒的特征表示 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. 基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建带 Dropout 的神经网络\n",
    "model = keras.models.Sequential([\n",
    "    # 输入层展平\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    # 输入层 Dropout（通常使用较小的 rate，如 0.2）\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    \n",
    "    # 第一个隐藏层\n",
    "    keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "    # 隐藏层 Dropout（通常使用 0.2-0.5）\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    \n",
    "    # 第二个隐藏层\n",
    "    keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    \n",
    "    # 第三个隐藏层\n",
    "    keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    \n",
    "    # 输出层（无 Dropout）\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## 2. 可视化 Dropout 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dropout(rate=0.5, input_size=100):\n",
    "    \"\"\"\n",
    "    可视化 Dropout 对神经元的影响\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rate : float\n",
    "        Dropout 比率（0到1之间）\n",
    "    input_size : int\n",
    "        输入维度大小\n",
    "    \"\"\"\n",
    "    dropout_layer = keras.layers.Dropout(rate=rate)\n",
    "    \n",
    "    # 创建输入数据\n",
    "    x = tf.ones((1, input_size))\n",
    "    \n",
    "    # 训练模式下的输出\n",
    "    output_train = dropout_layer(x, training=True)\n",
    "    \n",
    "    # 推理模式下的输出\n",
    "    output_inference = dropout_layer(x, training=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 原始输入\n",
    "    axes[0].bar(range(input_size), x.numpy().flatten(), color='blue', alpha=0.7)\n",
    "    axes[0].set_title('原始输入')\n",
    "    axes[0].set_xlabel('神经元索引')\n",
    "    axes[0].set_ylabel('激活值')\n",
    "    axes[0].set_ylim(0, 2)\n",
    "    \n",
    "    # 训练模式\n",
    "    colors = ['red' if v == 0 else 'green' for v in output_train.numpy().flatten()]\n",
    "    axes[1].bar(range(input_size), output_train.numpy().flatten(), color=colors, alpha=0.7)\n",
    "    axes[1].set_title(f'训练模式 (rate={rate})')\n",
    "    axes[1].set_xlabel('神经元索引')\n",
    "    axes[1].set_ylabel('激活值')\n",
    "    axes[1].set_ylim(0, 2)\n",
    "    \n",
    "    dropped = np.sum(output_train.numpy() == 0)\n",
    "    axes[1].text(0.5, 0.95, f'丢弃: {dropped}/{input_size} ({dropped/input_size*100:.1f}%)',\n",
    "                transform=axes[1].transAxes, ha='center', fontsize=10)\n",
    "    \n",
    "    # 推理模式\n",
    "    axes[2].bar(range(input_size), output_inference.numpy().flatten(), color='blue', alpha=0.7)\n",
    "    axes[2].set_title('推理模式 (无 Dropout)')\n",
    "    axes[2].set_xlabel('神经元索引')\n",
    "    axes[2].set_ylabel('激活值')\n",
    "    axes[2].set_ylim(0, 2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化不同 Dropout 率的效果\n",
    "for rate in [0.2, 0.5]:\n",
    "    print(f\"\\nDropout rate = {rate}\")\n",
    "    visualize_dropout(rate=rate, input_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## 3. 训练与验证对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"训练集: {X_train.shape}\")\n",
    "print(f\"验证集: {X_valid.shape}\")\n",
    "print(f\"测试集: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout_rate=0.0):\n",
    "    \"\"\"\n",
    "    创建带可配置 Dropout 率的模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dropout_rate : float\n",
    "        Dropout 比率，0 表示不使用 Dropout\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        编译好的模型\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    ])\n",
    "    \n",
    "    for units in [256, 128, 64]:\n",
    "        model.add(keras.layers.Dense(units, activation='relu', kernel_initializer='he_normal'))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 创建对比模型\n",
    "model_no_dropout = create_model(dropout_rate=0.0)\n",
    "model_with_dropout = create_model(dropout_rate=0.3)\n",
    "\n",
    "print(f\"无 Dropout 模型参数量: {model_no_dropout.count_params():,}\")\n",
    "print(f\"有 Dropout 模型参数量: {model_with_dropout.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练对比\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"训练无 Dropout 模型...\")\n",
    "history_no_dropout = model_no_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=0\n",
    ")\n",
    "print(\"完成\")\n",
    "\n",
    "print(\"训练有 Dropout 模型...\")\n",
    "history_with_dropout = model_with_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=0\n",
    ")\n",
    "print(\"完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 准确率\n",
    "axes[0].plot(history_no_dropout.history['accuracy'], 'b-', label='无 Dropout (训练)', alpha=0.8)\n",
    "axes[0].plot(history_no_dropout.history['val_accuracy'], 'b--', label='无 Dropout (验证)', alpha=0.8)\n",
    "axes[0].plot(history_with_dropout.history['accuracy'], 'r-', label='有 Dropout (训练)', alpha=0.8)\n",
    "axes[0].plot(history_with_dropout.history['val_accuracy'], 'r--', label='有 Dropout (验证)', alpha=0.8)\n",
    "axes[0].set_title('准确率对比')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 损失\n",
    "axes[1].plot(history_no_dropout.history['loss'], 'b-', label='无 Dropout (训练)', alpha=0.8)\n",
    "axes[1].plot(history_no_dropout.history['val_loss'], 'b--', label='无 Dropout (验证)', alpha=0.8)\n",
    "axes[1].plot(history_with_dropout.history['loss'], 'r-', label='有 Dropout (训练)', alpha=0.8)\n",
    "axes[1].plot(history_with_dropout.history['val_loss'], 'r--', label='有 Dropout (验证)', alpha=0.8)\n",
    "axes[1].set_title('损失对比')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dropout_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 计算过拟合程度\n",
    "gap_no_dropout = history_no_dropout.history['accuracy'][-1] - history_no_dropout.history['val_accuracy'][-1]\n",
    "gap_with_dropout = history_with_dropout.history['accuracy'][-1] - history_with_dropout.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"\\n过拟合程度（训练-验证准确率差）:\")\n",
    "print(f\"无 Dropout: {gap_no_dropout:.4f}\")\n",
    "print(f\"有 Dropout: {gap_with_dropout:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集评估\n",
    "print(\"测试集评估:\")\n",
    "test_loss_no, test_acc_no = model_no_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "test_loss_with, test_acc_with = model_with_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"无 Dropout: 准确率 = {test_acc_no:.4f}, 损失 = {test_loss_no:.4f}\")\n",
    "print(f\"有 Dropout: 准确率 = {test_acc_with:.4f}, 损失 = {test_loss_with:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 4. Dropout 变体\n",
    "\n",
    "### 4.1 空间 Dropout (Spatial Dropout)\n",
    "\n",
    "用于卷积神经网络，丢弃整个特征图而非单个像素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Dropout 示例（用于 CNN）\n",
    "cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    # SpatialDropout2D 丢弃整个特征图\n",
    "    keras.layers.SpatialDropout2D(0.25),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.SpatialDropout2D(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"CNN with Spatial Dropout:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### 4.2 Alpha Dropout（用于 SELU）\n",
    "\n",
    "专为自归一化网络设计，保持输入的均值和方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha Dropout 示例（用于 SELU 激活的自归一化网络）\n",
    "selu_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.AlphaDropout(0.1),  # SELU 专用\n",
    "    keras.layers.Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.AlphaDropout(0.1),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"SELU Network with Alpha Dropout:\")\n",
    "selu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## 5. 使用建议\n",
    "\n",
    "### Dropout 率选择\n",
    "\n",
    "| 层类型 | 推荐 Dropout 率 |\n",
    "|--------|----------------|\n",
    "| 输入层 | 0.1 - 0.2 |\n",
    "| 隐藏层 | 0.2 - 0.5 |\n",
    "| 输出层 | 不使用 Dropout |\n",
    "| CNN 特征提取层 | 0.25 (Spatial Dropout) |\n",
    "| CNN 全连接层 | 0.5 |\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "1. **与 BN 配合**: 通常将 Dropout 放在 BN 之后，或不同时使用\n",
    "2. **大数据集**: 数据量大时，可能不需要 Dropout\n",
    "3. **测试时**: 确保 `training=False`，或使用 `model.predict()`\n",
    "4. **率过高**: Dropout 率过高会导致欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证代码正确性\n",
    "print(\"Dropout 模块测试完成\")\n",
    "print(\"\\n关键要点:\")\n",
    "print(\"1. Dropout 随机丢弃神经元，防止过拟合\")\n",
    "print(\"2. 训练时应用 Dropout，推理时关闭\")\n",
    "print(\"3. 输入层用小 rate (0.1-0.2)，隐藏层用中等 rate (0.2-0.5)\")\n",
    "print(\"4. SELU 激活函数配合 AlphaDropout 使用\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
