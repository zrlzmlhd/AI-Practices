{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 批量归一化 (Batch Normalization)\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "批量归一化（Ioffe & Szegedy, 2015）通过对每一层的输入进行归一化，解决深度网络训练中的**内部协变量偏移 (Internal Covariate Shift)** 问题。\n",
    "\n",
    "## 数学原理\n",
    "\n",
    "对于 mini-batch B = {x₁, x₂, ..., xₘ}：\n",
    "\n",
    "1. **计算均值**: μ_B = (1/m) Σxᵢ\n",
    "2. **计算方差**: σ²_B = (1/m) Σ(xᵢ - μ_B)²\n",
    "3. **归一化**: x̂ᵢ = (xᵢ - μ_B) / √(σ²_B + ε)\n",
    "4. **缩放平移**: yᵢ = γx̂ᵢ + β\n",
    "\n",
    "其中 γ（scale）和 β（shift）是可学习参数，ε 是防止除零的小常数。\n",
    "\n",
    "## 主要优势\n",
    "\n",
    "| 优势 | 说明 |\n",
    "|------|------|\n",
    "| 加速训练 | 允许使用更大的学习率 |\n",
    "| 稳定训练 | 减少梯度消失/爆炸 |\n",
    "| 正则化效果 | 类似轻度 Dropout |\n",
    "| 降低初始化敏感度 | 对权重初始化不那么敏感 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. 基本用法：激活函数后使用 BN\n",
    "\n",
    "传统方式是在激活函数之后应用 BN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建带 BN 的模型（标准方式：激活函数后使用 BN）\n",
    "model_bn_after = keras.models.Sequential([\n",
    "    # 输入层：展平 28x28 图像\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    # 第一个 BN 层：归一化输入数据\n",
    "    keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 第一个隐藏层\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 第二个隐藏层\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 输出层（分类任务）\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bn_after.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看第一个 BN 层的参数\n",
    "bn_layer = model_bn_after.layers[1]  # 第一个 BN 层\n",
    "\n",
    "print(\"BatchNormalization 层参数:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for var in bn_layer.variables:\n",
    "    trainable_status = \"可训练\" if var.trainable else \"不可训练\"\n",
    "    print(f\"{var.name:40s} | {trainable_status}\")\n",
    "\n",
    "print(\"\\n参数说明:\")\n",
    "print(\"- gamma: 缩放因子（可训练）\")\n",
    "print(\"- beta: 偏移量（可训练）\")\n",
    "print(\"- moving_mean: 滑动均值（推理时使用，不可训练）\")\n",
    "print(\"- moving_variance: 滑动方差（推理时使用，不可训练）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 2. 优化方式：激活函数前使用 BN\n",
    "\n",
    "论文原作者建议在激活函数**之前**应用 BN，这样可以：\n",
    "- 归一化线性组合的输出\n",
    "- 避免 Dense 层的 bias 与 BN 的 beta 参数冗余"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建优化版本：BN 在激活函数之前\n",
    "model_bn_before = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # Dense 层不使用 bias（BN 的 beta 会替代 bias 的作用）\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),  # 激活函数单独作为一层\n",
    "    \n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    \n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bn_before.summary()\n",
    "\n",
    "# 对比参数量\n",
    "print(f\"\\n参数量对比:\")\n",
    "print(f\"BN 在激活后: {model_bn_after.count_params():,} 参数\")\n",
    "print(f\"BN 在激活前: {model_bn_before.count_params():,} 参数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## 3. 训练与推理的区别\n",
    "\n",
    "BN 在训练和推理阶段有不同的行为：\n",
    "\n",
    "| 阶段 | 均值/方差来源 | 说明 |\n",
    "|------|---------------|------|\n",
    "| 训练 | 当前 mini-batch | 实时计算，同时更新滑动统计量 |\n",
    "| 推理 | 滑动均值/方差 | 使用训练期间累积的统计量 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 BN 在训练和推理时的不同行为\n",
    "def demonstrate_bn_behavior():\n",
    "    \"\"\"\n",
    "    演示 BatchNormalization 在训练和推理阶段的不同行为\n",
    "    \"\"\"\n",
    "    # 创建简单的 BN 层\n",
    "    bn = keras.layers.BatchNormalization()\n",
    "    \n",
    "    # 模拟输入数据\n",
    "    x_train = tf.random.normal((32, 10))  # batch_size=32, features=10\n",
    "    x_test = tf.random.normal((8, 10))    # 测试数据\n",
    "    \n",
    "    # 训练模式\n",
    "    output_train = bn(x_train, training=True)\n",
    "    print(\"训练模式:\")\n",
    "    print(f\"  输入均值: {tf.reduce_mean(x_train).numpy():.4f}\")\n",
    "    print(f\"  输出均值: {tf.reduce_mean(output_train).numpy():.4f}\")\n",
    "    print(f\"  输入标准差: {tf.math.reduce_std(x_train).numpy():.4f}\")\n",
    "    print(f\"  输出标准差: {tf.math.reduce_std(output_train).numpy():.4f}\")\n",
    "    \n",
    "    # 推理模式\n",
    "    output_inference = bn(x_test, training=False)\n",
    "    print(\"\\n推理模式:\")\n",
    "    print(f\"  使用滑动均值: {bn.moving_mean.numpy()[:3]}...\")\n",
    "    print(f\"  使用滑动方差: {bn.moving_variance.numpy()[:3]}...\")\n",
    "\n",
    "demonstrate_bn_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## 4. 完整训练示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Fashion MNIST 数据集\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"训练集: {X_train.shape}\")\n",
    "print(f\"验证集: {X_valid.shape}\")\n",
    "print(f\"测试集: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_bn(use_bn=True):\n",
    "    \"\"\"\n",
    "    创建带或不带 BN 的模型用于对比\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_bn : bool\n",
    "        是否使用批量归一化\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        编译好的模型\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    for units in [256, 128, 64]:\n",
    "        model.add(keras.layers.Dense(units, kernel_initializer='he_normal', use_bias=not use_bn))\n",
    "        if use_bn:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Activation('elu'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 创建两个模型进行对比\n",
    "model_with_bn = create_model_with_bn(use_bn=True)\n",
    "model_without_bn = create_model_with_bn(use_bn=False)\n",
    "\n",
    "print(f\"带 BN 的模型参数量: {model_with_bn.count_params():,}\")\n",
    "print(f\"不带 BN 的模型参数量: {model_without_bn.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练对比（使用简化参数快速验证）\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"训练带 BN 的模型...\")\n",
    "history_with_bn = model_with_bn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n训练不带 BN 的模型...\")\n",
    "history_without_bn = model_without_bn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 准确率对比\n",
    "axes[0].plot(history_with_bn.history['accuracy'], 'b-', label='带 BN (训练)')\n",
    "axes[0].plot(history_with_bn.history['val_accuracy'], 'b--', label='带 BN (验证)')\n",
    "axes[0].plot(history_without_bn.history['accuracy'], 'r-', label='不带 BN (训练)')\n",
    "axes[0].plot(history_without_bn.history['val_accuracy'], 'r--', label='不带 BN (验证)')\n",
    "axes[0].set_title('准确率对比')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 损失对比\n",
    "axes[1].plot(history_with_bn.history['loss'], 'b-', label='带 BN (训练)')\n",
    "axes[1].plot(history_with_bn.history['val_loss'], 'b--', label='带 BN (验证)')\n",
    "axes[1].plot(history_without_bn.history['loss'], 'r-', label='不带 BN (训练)')\n",
    "axes[1].plot(history_without_bn.history['val_loss'], 'r--', label='不带 BN (验证)')\n",
    "axes[1].set_title('损失对比')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_normalization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 测试集评估\n",
    "print(\"\\n测试集评估:\")\n",
    "test_loss_bn, test_acc_bn = model_with_bn.evaluate(X_test, y_test, verbose=0)\n",
    "test_loss_no_bn, test_acc_no_bn = model_without_bn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"带 BN: 准确率 = {test_acc_bn:.4f}, 损失 = {test_loss_bn:.4f}\")\n",
    "print(f\"不带 BN: 准确率 = {test_acc_no_bn:.4f}, 损失 = {test_loss_no_bn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 5. BN 的超参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNormalization 的完整参数\n",
    "bn_layer_custom = keras.layers.BatchNormalization(\n",
    "    axis=-1,                 # 归一化的轴，-1 表示最后一个轴（特征轴）\n",
    "    momentum=0.99,           # 滑动平均的动量（默认 0.99）\n",
    "    epsilon=1e-3,            # 防止除零的小常数（默认 0.001）\n",
    "    center=True,             # 是否添加 beta（偏移量）\n",
    "    scale=True,              # 是否添加 gamma（缩放因子）\n",
    "    beta_initializer='zeros',\n",
    "    gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones'\n",
    ")\n",
    "\n",
    "print(\"BatchNormalization 超参数说明:\")\n",
    "print(\"=\"*50)\n",
    "print(\"momentum: 滑动平均动量，值越大历史信息保留越多\")\n",
    "print(\"  - 0.99: 适合大数据集（默认）\")\n",
    "print(\"  - 0.9: 适合小数据集\")\n",
    "print(\"epsilon: 数值稳定性常数，通常无需修改\")\n",
    "print(\"center/scale: 是否使用 beta/gamma 参数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## 6. 使用建议\n",
    "\n",
    "### 何时使用 BN\n",
    "\n",
    "| 场景 | 建议 |\n",
    "|------|------|\n",
    "| 深层网络 | 强烈推荐 |\n",
    "| 训练不稳定 | 推荐 |\n",
    "| 想用更大学习率 | 推荐 |\n",
    "| 小 batch size (<16) | 考虑使用 Layer Normalization |\n",
    "| RNN/LSTM | 使用 Layer Normalization |\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "1. **Batch Size 影响**: 小 batch 时 BN 效果不稳定，考虑使用 Layer Normalization\n",
    "2. **Dropout + BN**: 通常不建议同时使用，或将 Dropout 放在 BN 之后\n",
    "3. **迁移学习**: 微调时可能需要冻结 BN 层的统计量\n",
    "4. **推理模式**: 确保推理时 `training=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证代码正确性\n",
    "print(\"批量归一化模块测试完成\")\n",
    "print(\"\\n关键要点:\")\n",
    "print(\"1. BN 归一化每层输入，加速训练并稳定梯度\")\n",
    "print(\"2. 推荐在激活函数前使用 BN，并设置 use_bias=False\")\n",
    "print(\"3. 训练和推理阶段使用不同的统计量\")\n",
    "print(\"4. 小 batch size 时考虑使用 Layer Normalization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
