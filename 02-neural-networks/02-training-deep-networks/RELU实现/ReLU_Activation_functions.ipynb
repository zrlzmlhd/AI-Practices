{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 深度学习激活函数详解\n",
    "\n",
    "## 激活函数的作用\n",
    "\n",
    "激活函数为神经网络引入非线性特性。没有激活函数，无论网络多深，都等价于单层线性变换。\n",
    "\n",
    "## 激活函数演进历史\n",
    "\n",
    "```\n",
    "sigmoid/tanh → ReLU → Leaky ReLU → PReLU → ELU → SELU → Swish/GELU\n",
    "```\n",
    "\n",
    "## 主要问题与解决方案\n",
    "\n",
    "| 问题 | 影响 | 解决方案 |\n",
    "|------|------|----------|\n",
    "| 梯度饱和 | sigmoid/tanh 在极值区梯度趋近于0 | 使用 ReLU 系列 |\n",
    "| Dead ReLU | 负输入永久置零，神经元\"死亡\" | Leaky ReLU, PReLU, ELU |\n",
    "| 输出不对称 | ReLU 输出均值不为0，影响下一层 | ELU, SELU |\n",
    "| 计算效率 | 某些激活函数计算代价高 | ReLU 简单高效 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**公式**: f(x) = max(0, x)\n",
    "\n",
    "**优点**:\n",
    "- 计算简单高效\n",
    "- 缓解梯度消失问题\n",
    "- 稀疏激活（约50%神经元输出为0）\n",
    "\n",
    "**缺点**:\n",
    "- Dead ReLU 问题：负输入恒为0，梯度为0，神经元可能永久\"死亡\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 激活函数可视化\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# ReLU: max(0, x)\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 函数图像\n",
    "axes[0].plot(x, relu, 'b-', linewidth=2, label='ReLU')\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('ReLU: f(x) = max(0, x)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 导数图像\n",
    "relu_grad = np.where(x > 0, 1, 0)\n",
    "axes[1].plot(x, relu_grad, 'r-', linewidth=2, label=\"ReLU'\")\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"f'(x)\")\n",
    "axes[1].set_title('ReLU 导数')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## 2. Leaky ReLU\n",
    "\n",
    "**公式**: f(x) = x if x > 0 else αx (通常 α = 0.01 或 0.3)\n",
    "\n",
    "**优点**: 解决 Dead ReLU 问题，负区域有小梯度\n",
    "\n",
    "**缺点**: α 是超参数，需要调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 中使用 Leaky ReLU\n",
    "# 方式1: 作为独立层使用\n",
    "model_leaky = keras.Sequential([\n",
    "    layers.Dense(64, kernel_initializer='he_normal'),\n",
    "    layers.LeakyReLU(negative_slope=0.3),  # 负斜率为 0.3\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 可视化不同 alpha 值的 Leaky ReLU\n",
    "alphas = [0.01, 0.1, 0.3]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for alpha, color in zip(alphas, colors):\n",
    "    leaky_relu = np.where(x > 0, x, alpha * x)\n",
    "    plt.plot(x, leaky_relu, color=color, linewidth=2, label=f'α = {alpha}')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Leaky ReLU with different α values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Leaky ReLU 模型构建成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## 3. PReLU (Parametric ReLU)\n",
    "\n",
    "**公式**: f(x) = x if x > 0 else αx，其中 α 是可学习参数\n",
    "\n",
    "**优点**: α 通过反向传播自动学习，无需手动调优\n",
    "\n",
    "**缺点**: 增加模型参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 中使用 PReLU\n",
    "model_prelu = keras.Sequential([\n",
    "    layers.Dense(64, kernel_initializer='he_normal'),\n",
    "    layers.PReLU(),  # α 是可学习参数\n",
    "    layers.Dense(32, kernel_initializer='he_normal'),\n",
    "    layers.PReLU(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 构建模型以查看参数\n",
    "model_prelu.build(input_shape=(None, 784))\n",
    "model_prelu.summary()\n",
    "\n",
    "print(\"\\nPReLU 模型构建成功\")\n",
    "print(\"注意: PReLU 层有可学习参数 alpha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 4. ELU (Exponential Linear Unit)\n",
    "\n",
    "**公式**: f(x) = x if x > 0 else α(e^x - 1)\n",
    "\n",
    "**优点**:\n",
    "- 输出均值接近0（自归一化特性）\n",
    "- 负区域平滑过渡，梯度不会突变\n",
    "- 缓解 Dead ReLU 问题\n",
    "\n",
    "**缺点**: 计算代价比 ReLU 高（涉及指数运算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU 可视化\n",
    "alpha = 1.0\n",
    "elu = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# 对比 ReLU 和 ELU\n",
    "plt.plot(x, relu, 'b--', linewidth=2, label='ReLU', alpha=0.7)\n",
    "plt.plot(x, elu, 'r-', linewidth=2, label=f'ELU (α={alpha})')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('ELU vs ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2, 5)\n",
    "plt.show()\n",
    "\n",
    "# Keras 中使用 ELU\n",
    "model_elu = keras.Sequential([\n",
    "    layers.Dense(64, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(32, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"ELU 模型构建成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 5. SELU (Scaled ELU)\n",
    "\n",
    "**公式**: f(x) = λ × (x if x > 0 else α(e^x - 1))\n",
    "\n",
    "其中 λ ≈ 1.0507, α ≈ 1.6733 是精心选择的常数。\n",
    "\n",
    "**优点**: 自归一化特性 - 在特定条件下，网络输出自动保持均值为0、方差为1\n",
    "\n",
    "**使用条件**:\n",
    "- 必须使用 `lecun_normal` 初始化\n",
    "- 必须使用 `AlphaDropout`（而非普通 Dropout）\n",
    "- 网络必须是全连接的（Sequential Dense layers）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU 自归一化网络示例\n",
    "model_selu = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    # SELU 必须配合 lecun_normal 初始化\n",
    "    layers.Dense(256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    layers.AlphaDropout(0.1),  # SELU 专用 Dropout\n",
    "    layers.Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    layers.AlphaDropout(0.1),\n",
    "    layers.Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_selu.summary()\n",
    "print(\"\\nSELU 自归一化网络构建成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 6. 所有激活函数对比可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各激活函数\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': np.maximum(0, x),\n",
    "    'Leaky ReLU (α=0.1)': np.where(x > 0, x, 0.1 * x),\n",
    "    'ELU (α=1)': np.where(x > 0, x, 1.0 * (np.exp(x) - 1)),\n",
    "    'SELU': 1.0507 * np.where(x > 0, x, 1.6733 * (np.exp(x) - 1)),\n",
    "    'tanh': np.tanh(x),\n",
    "    'sigmoid': 1 / (1 + np.exp(-x))\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "for idx, (name, values) in enumerate(activations.items()):\n",
    "    axes[idx].plot(x, values, color=colors[idx], linewidth=2)\n",
    "    axes[idx].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    axes[idx].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    axes[idx].set_title(name, fontsize=12)\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('f(x)')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlim(-4, 4)\n",
    "    axes[idx].set_ylim(-2, 4)\n",
    "\n",
    "plt.suptitle('深度学习常用激活函数对比', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('activation_functions_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 7. 实际应用：构建完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_activation(activation_name, use_bn=False):\n",
    "    \"\"\"\n",
    "    构建使用指定激活函数的模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    activation_name : str\n",
    "        激活函数名称: 'relu', 'leaky_relu', 'prelu', 'elu', 'selu'\n",
    "    use_bn : bool\n",
    "        是否使用批量归一化\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        编译好的模型\n",
    "    \"\"\"\n",
    "    # 根据激活函数选择初始化方法\n",
    "    if activation_name == 'selu':\n",
    "        initializer = 'lecun_normal'\n",
    "    else:\n",
    "        initializer = 'he_normal'\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    \n",
    "    for units in [256, 128, 64]:\n",
    "        if activation_name == 'leaky_relu':\n",
    "            model.add(layers.Dense(units, kernel_initializer=initializer))\n",
    "            model.add(layers.LeakyReLU(negative_slope=0.2))\n",
    "        elif activation_name == 'prelu':\n",
    "            model.add(layers.Dense(units, kernel_initializer=initializer))\n",
    "            model.add(layers.PReLU())\n",
    "        else:\n",
    "            model.add(layers.Dense(units, activation=activation_name, \n",
    "                                  kernel_initializer=initializer))\n",
    "        \n",
    "        if use_bn and activation_name != 'selu':\n",
    "            model.add(layers.BatchNormalization())\n",
    "        elif activation_name == 'selu':\n",
    "            model.add(layers.AlphaDropout(0.1))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 测试各种激活函数的模型构建\n",
    "activation_list = ['relu', 'leaky_relu', 'prelu', 'elu', 'selu']\n",
    "\n",
    "for act in activation_list:\n",
    "    model = build_model_with_activation(act)\n",
    "    param_count = model.count_params()\n",
    "    print(f\"{act.upper():12s} - 参数量: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## 8. 激活函数选择指南\n",
    "\n",
    "### 推荐策略\n",
    "\n",
    "| 场景 | 推荐激活函数 | 配套设置 |\n",
    "|------|--------------|----------|\n",
    "| 默认选择 | ReLU | He 初始化 |\n",
    "| 担心 Dead ReLU | Leaky ReLU / ELU | He 初始化 |\n",
    "| 自归一化网络 | SELU | LeCun 初始化 + AlphaDropout |\n",
    "| 二分类输出层 | sigmoid | - |\n",
    "| 多分类输出层 | softmax | - |\n",
    "| RNN 隐藏层 | tanh | - |\n",
    "\n",
    "### 性能对比（一般规律）\n",
    "\n",
    "- **训练速度**: ReLU > Leaky ReLU > ELU > SELU\n",
    "- **模型性能**: 通常差异不大，ELU/SELU 在深层网络中略有优势\n",
    "- **稳定性**: SELU（满足条件时）> ELU > Leaky ReLU > ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证代码正确性\n",
    "print(\"激活函数模块测试完成\")\n",
    "print(\"\\n关键要点:\")\n",
    "print(\"1. ReLU 是默认首选，简单高效\")\n",
    "print(\"2. Leaky ReLU/PReLU/ELU 解决 Dead ReLU 问题\")\n",
    "print(\"3. SELU 实现自归一化，需要特定配置\")\n",
    "print(\"4. 输出层使用 sigmoid（二分类）或 softmax（多分类）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
