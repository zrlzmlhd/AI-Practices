{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e85819101ab7b8",
   "metadata": {},
   "source": [
    "# TensorFlow自定义模型与训练算法\n",
    "\n",
    "本notebook深入讲解TensorFlow/Keras的高级自定义功能，包括自定义损失函数、层、模型和训练循环。这些技术是从\"调包侠\"进阶到\"算法工程师\"的关键能力。\n",
    "\n",
    "## 学习目标\n",
    "1. 掌握自定义损失函数的实现方法\n",
    "2. 学会自定义层（Layer）的创建\n",
    "3. 理解自定义模型（Model）的构建\n",
    "4. 实现自定义训练循环\n",
    "5. 掌握模型的保存与加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "## 1. 环境设置与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libs",
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\n\n# 设置随机种子\nRANDOM_SEED = 42\ntf.random.set_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nprint(f\"TensorFlow版本: {tf.__version__}\")\nprint(f\"Keras版本: {keras.__version__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_prepare",
   "metadata": {},
   "outputs": [],
   "source": "# 生成模拟回归数据集（模拟加州房价特征）\n# 使用模拟数据避免网络依赖，同时保持数据结构一致\n\nn_samples = 10000\nn_features = 8  # 与加州房价数据集特征数一致\n\n# 生成特征矩阵（已标准化）\nX_full = np.random.randn(n_samples, n_features).astype(np.float32)\n\n# 生成目标值（模拟房价，范围约0-5）\n# 使用线性组合加噪声模拟真实关系\ntrue_weights = np.random.randn(n_features).astype(np.float32)\ny_full = X_full @ true_weights + np.random.randn(n_samples).astype(np.float32) * 0.5\ny_full = (y_full - y_full.min()) / (y_full.max() - y_full.min()) * 5  # 归一化到0-5\n\n# 划分数据集\ntrain_size = int(0.64 * n_samples)  # 6400\nvalid_size = int(0.16 * n_samples)  # 1600\ntest_size = n_samples - train_size - valid_size  # 2000\n\nX_train = X_full[:train_size]\ny_train = y_full[:train_size]\nX_valid = X_full[train_size:train_size + valid_size]\ny_valid = y_full[train_size:train_size + valid_size]\nX_test = X_full[train_size + valid_size:]\ny_test = y_full[train_size + valid_size:]\n\nprint(f\"训练集: {X_train.shape[0]}样本\")\nprint(f\"验证集: {X_valid.shape[0]}样本\")\nprint(f\"测试集: {X_test.shape[0]}样本\")\nprint(f\"特征数: {X_train.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "custom_loss_title",
   "metadata": {},
   "source": [
    "## 2. 自定义损失函数\n",
    "\n",
    "### 2.1 函数式损失函数\n",
    "\n",
    "最简单的自定义损失函数是一个接受`y_true`和`y_pred`两个参数的函数。\n",
    "\n",
    "**Huber损失函数**是一种结合MSE和MAE优点的鲁棒损失函数：\n",
    "- 当误差较小时（|error| < δ），使用MSE（对小误差敏感）\n",
    "- 当误差较大时（|error| ≥ δ），使用MAE（对异常值鲁棒）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f179d4dd000065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Huber损失函数实现\n",
    "    \n",
    "    参数:\n",
    "        y_true: 真实标签\n",
    "        y_pred: 预测值\n",
    "        \n",
    "    返回:\n",
    "        损失值张量\n",
    "        \n",
    "    数学公式:\n",
    "        L(y, f(x)) = \n",
    "            0.5 * (y - f(x))^2,           if |y - f(x)| < 1\n",
    "            |y - f(x)| - 0.5,             otherwise\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1.0\n",
    "    squared_loss = tf.square(error) / 2.0\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# 测试Huber损失函数\n",
    "y_true_test = tf.constant([1.0, 2.0, 3.0])\n",
    "y_pred_test = tf.constant([1.5, 2.5, 5.0])  # 误差分别为0.5, 0.5, 2.0\n",
    "loss = huber_fn(y_true_test, y_pred_test)\n",
    "print(f\"测试Huber损失: {loss.numpy()}\")\n",
    "print(f\"  误差0.5 -> 使用MSE: 0.5^2/2 = 0.125\")\n",
    "print(f\"  误差2.0 -> 使用MAE: |2.0| - 0.5 = 1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "use_custom_loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义损失函数训练模型\n",
    "\n",
    "# 构建简单的回归模型\n",
    "def build_regression_model(input_shape):\n",
    "    \"\"\"构建用于回归任务的MLP模型\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1)  # 回归任务不需要激活函数\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_regression_model(input_shape=(X_train.shape[1],))\n",
    "model.compile(loss=huber_fn, optimizer='adam')\n",
    "\n",
    "# 使用简单参数进行快速测试\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n测试集损失: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "param_loss_title",
   "metadata": {},
   "source": [
    "### 2.2 带参数的损失函数\n",
    "\n",
    "如果损失函数需要可配置的参数（如Huber的阈值），可以使用工厂函数或类来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5222240dab03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    \"\"\"\n",
    "    创建指定阈值的Huber损失函数\n",
    "    \n",
    "    参数:\n",
    "        threshold: 阈值δ，控制MSE和MAE的切换点\n",
    "        \n",
    "    返回:\n",
    "        配置好的Huber损失函数\n",
    "    \"\"\"\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2.0\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2.0\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "# 使用不同阈值测试\n",
    "huber_1 = create_huber(1.0)\n",
    "huber_2 = create_huber(2.0)\n",
    "\n",
    "error_values = tf.constant([0.5, 1.5, 3.0])\n",
    "y_true = tf.constant([0.0, 0.0, 0.0])\n",
    "y_pred = -error_values\n",
    "\n",
    "print(f\"误差值: {error_values.numpy()}\")\n",
    "print(f\"阈值1.0的损失: {huber_1(y_true, y_pred).numpy()}\")\n",
    "print(f\"阈值2.0的损失: {huber_2(y_true, y_pred).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_class_title",
   "metadata": {},
   "source": [
    "### 2.3 继承keras.losses.Loss实现损失类\n",
    "\n",
    "继承`keras.losses.Loss`类可以实现：\n",
    "- 参数随模型一起保存\n",
    "- 支持`get_config()`序列化\n",
    "- 更好的代码组织"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52923bad9367da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Huber损失函数类实现\n",
    "    \n",
    "    继承keras.losses.Loss可以使参数随模型保存\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化Huber损失\n",
    "        \n",
    "        参数:\n",
    "            threshold: 阈值δ\n",
    "            **kwargs: 传递给父类的其他参数\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"计算损失值\"\"\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2.0\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2.0\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"返回配置字典，用于序列化\"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# 测试损失类\n",
    "huber_loss = HuberLoss(threshold=1.5)\n",
    "print(f\"损失类配置: {huber_loss.get_config()}\")\n",
    "\n",
    "# 使用损失类编译模型\n",
    "model = build_regression_model(input_shape=(X_train.shape[1],))\n",
    "model.compile(loss=HuberLoss(threshold=2.0), optimizer='adam')\n",
    "print(\"模型编译成功，使用HuberLoss(threshold=2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_components_title",
   "metadata": {},
   "source": [
    "## 3. 自定义激活函数、初始化器和正则化器\n",
    "\n",
    "这些组件都可以通过简单的函数或类来自定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552d77367ae3c11",
   "metadata": {},
   "outputs": [],
   "source": "# 自定义激活函数 - Softplus的手动实现\ndef my_softplus(z):\n    \"\"\"Softplus激活: log(exp(z) + 1)，是ReLU的平滑近似\"\"\"\n    return tf.math.log(tf.exp(z) + 1.0)\n\n# 自定义初始化器 - Glorot初始化\ndef my_glorot_initializer(shape, dtype=tf.float32):\n    \"\"\"Glorot/Xavier均匀初始化\"\"\"\n    stddev = tf.sqrt(2.0 / (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n\n# 使用自定义组件创建层\n# 注意：Keras 3.x中，正则化器和约束必须是相应基类的子类实例\n# 使用内置组件演示功能\ncustom_layer = keras.layers.Dense(\n    10,\n    activation=my_softplus,\n    kernel_initializer=my_glorot_initializer,\n    kernel_regularizer=keras.regularizers.l1(0.001),\n    kernel_constraint=keras.constraints.NonNeg()  # 非负约束\n)\n\n# 测试自定义层\ntest_input = tf.random.normal([2, 5])\noutput = custom_layer(test_input)\nprint(f\"输入形状: {test_input.shape}\")\nprint(f\"输出形状: {output.shape}\")\nprint(f\"权重是否非负: {tf.reduce_all(custom_layer.kernel >= 0).numpy()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557ed60b9679c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可序列化的正则化器类\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    \"\"\"可序列化的L1正则化器\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=0.001):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(weights)) * self.factor\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "# 测试正则化器\n",
    "reg = MyL1Regularizer(factor=0.01)\n",
    "test_weights = tf.constant([[1.0, -2.0], [3.0, -4.0]])\n",
    "reg_loss = reg(test_weights)\n",
    "print(f\"正则化损失: {reg_loss.numpy():.4f}\")\n",
    "print(f\"配置: {reg.get_config()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_metric_title",
   "metadata": {},
   "source": [
    "## 4. 自定义评估指标\n",
    "\n",
    "### 4.1 流式指标（Streaming Metric）\n",
    "\n",
    "流式指标在整个训练过程中累积计算，而不是每个批次单独计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441145f7c7e35335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    流式Huber指标\n",
    "    \n",
    "    在整个epoch中累积计算平均Huber损失\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        # 创建状态变量用于累积\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"更新状态变量\"\"\"\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2.0\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2.0\n",
    "        loss = tf.where(is_small_error, squared_loss, linear_loss)\n",
    "        \n",
    "        self.total.assign_add(tf.reduce_sum(loss))\n",
    "        self.count.assign_add(tf.cast(tf.size(loss), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"返回当前指标值\"\"\"\n",
    "        return self.total / self.count\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"重置状态（每个epoch开始时调用）\"\"\"\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# 测试自定义指标\n",
    "metric = HuberMetric(threshold=1.0)\n",
    "\n",
    "# 模拟多个批次的更新\n",
    "metric.update_state(tf.constant([1.0, 2.0]), tf.constant([1.5, 2.2]))\n",
    "metric.update_state(tf.constant([3.0, 4.0]), tf.constant([3.1, 5.5]))\n",
    "\n",
    "print(f\"累积Huber指标: {metric.result().numpy():.4f}\")\n",
    "\n",
    "# 重置\n",
    "metric.reset_state()\n",
    "print(f\"重置后: {metric.result().numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_layer_title",
   "metadata": {},
   "source": [
    "## 5. 自定义层\n",
    "\n",
    "### 5.1 无权重的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca90c088934a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    高斯噪声层\n",
    "    \n",
    "    在训练时添加高斯噪声作为正则化手段，推理时不添加噪声\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(\n",
    "                shape=tf.shape(inputs),\n",
    "                mean=0.0,\n",
    "                stddev=self.stddev\n",
    "            )\n",
    "            return inputs + noise\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"stddev\": self.stddev}\n",
    "\n",
    "# 测试噪声层\n",
    "noise_layer = GaussianNoise(stddev=0.1)\n",
    "test_input = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "print(f\"原始输入: {test_input.numpy()}\")\n",
    "print(f\"训练模式输出: {noise_layer(test_input, training=True).numpy()}\")\n",
    "print(f\"推理模式输出: {noise_layer(test_input, training=False).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight_layer_title",
   "metadata": {},
   "source": [
    "### 5.2 带权重的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdef3485334063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    自定义全连接层\n",
    "    \n",
    "    手动实现Dense层以理解权重管理机制\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"延迟创建权重，直到知道输入形状\"\"\"\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=[input_shape[-1], self.units],\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=[self.units],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        z = tf.matmul(inputs, self.kernel) + self.bias\n",
    "        if self.activation is not None:\n",
    "            return self.activation(z)\n",
    "        return z\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"units\": self.units,\n",
    "            \"activation\": keras.activations.serialize(self.activation)\n",
    "        }\n",
    "\n",
    "# 测试自定义Dense层\n",
    "my_dense = MyDense(5, activation='relu')\n",
    "test_input = tf.random.normal([3, 4])\n",
    "output = my_dense(test_input)\n",
    "\n",
    "print(f\"输入形状: {test_input.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"权重形状: {my_dense.kernel.shape}\")\n",
    "print(f\"偏置形状: {my_dense.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_io_layer_title",
   "metadata": {},
   "source": [
    "### 5.3 多输入/多输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2cf86898feee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    多输出计算层\n",
    "    \n",
    "    接收两个输入，返回它们的和、积、商\n",
    "    \"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "        return [\n",
    "            x1 + x2,\n",
    "            x1 * x2,\n",
    "            x1 / (x2 + 1e-7)  # 添加小值避免除零\n",
    "        ]\n",
    "\n",
    "# 测试多输出层\n",
    "multi_layer = MultiOutputLayer()\n",
    "x1 = tf.constant([[1.0, 2.0]])\n",
    "x2 = tf.constant([[3.0, 4.0]])\n",
    "outputs = multi_layer([x1, x2])\n",
    "\n",
    "print(f\"输入x1: {x1.numpy()}\")\n",
    "print(f\"输入x2: {x2.numpy()}\")\n",
    "print(f\"和: {outputs[0].numpy()}\")\n",
    "print(f\"积: {outputs[1].numpy()}\")\n",
    "print(f\"商: {outputs[2].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_model_title",
   "metadata": {},
   "source": [
    "## 6. 自定义模型\n",
    "\n",
    "### 6.1 残差块和残差网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f52c5a5be9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    残差块\n",
    "    \n",
    "    实现跳跃连接：output = F(x) + x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.hidden = [\n",
    "            keras.layers.Dense(\n",
    "                n_neurons,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=\"he_normal\"\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.hidden:\n",
    "            z = layer(z)\n",
    "        return inputs + z  # 残差连接\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"n_neurons\": self.n_neurons\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d265101bd5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    \"\"\"\n",
    "    残差回归模型\n",
    "    \n",
    "    使用残差块构建的回归网络\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(\n",
    "            30, activation=\"elu\", kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self.hidden1(inputs)\n",
    "        z = self.block1(z)\n",
    "        z = self.block2(z)\n",
    "        return self.out(z)\n",
    "\n",
    "# 测试残差模型\n",
    "res_model = ResidualRegressor(output_dim=1)\n",
    "test_input = tf.random.normal([5, 8])\n",
    "output = res_model(test_input)\n",
    "\n",
    "print(f\"输入形状: {test_input.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "res_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aux_loss_title",
   "metadata": {},
   "source": [
    "### 6.2 带辅助损失的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77ea685a28bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    \"\"\"\n",
    "    带重建损失的回归模型\n",
    "    \n",
    "    除了主要的回归任务外，还添加输入重建作为辅助任务，\n",
    "    这有助于学习更好的特征表示\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            keras.layers.Dense(\n",
    "                30, activation=\"selu\", kernel_initializer=\"lecun_normal\"\n",
    "            )\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruct = None  # 延迟创建\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_inputs = input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.hidden:\n",
    "            z = layer(z)\n",
    "        \n",
    "        # 计算重建损失并添加到模型\n",
    "        reconstruction = self.reconstruct(z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)  # 添加辅助损失\n",
    "        \n",
    "        return self.out(z)\n",
    "\n",
    "# 测试带辅助损失的模型\n",
    "recon_model = ReconstructingRegressor()\n",
    "recon_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# 快速训练测试\n",
    "history = recon_model.fit(\n",
    "    X_train[:1000], y_train[:1000],\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"模型总损失包含主损失+辅助损失\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_training_title",
   "metadata": {},
   "source": [
    "## 7. 自定义训练循环\n",
    "\n",
    "当`model.fit()`无法满足需求时（如GAN训练、强化学习），需要自定义训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedb464da6c5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size):\n",
    "    \"\"\"随机采样一个批次\"\"\"\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    \"\"\"打印训练进度条\"\"\"\n",
    "    metrics_str = \" - \".join(\n",
    "        [f\"{m.name}: {m.result():.4f}\" for m in (metrics or [])]\n",
    "    )\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - loss: {loss:.4f} - {metrics_str}\", end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a3a4604b2b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型和优化器\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 超参数\n",
    "n_epochs = 3\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "# 优化器和损失函数\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# 指标\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "# 自定义训练循环\n",
    "print(\"开始自定义训练循环...\")\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # 获取批次数据\n",
    "        X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "        \n",
    "        # 前向传播和梯度计算\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            # 加入正则化损失\n",
    "            total_loss = main_loss + sum(model.losses)\n",
    "        \n",
    "        # 反向传播\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # 更新指标\n",
    "        mean_loss(total_loss)\n",
    "        mae_metric(y_batch, y_pred)\n",
    "        \n",
    "        # 每100步打印一次\n",
    "        if step % 100 == 0:\n",
    "            print_status_bar(step, n_steps, mean_loss.result(), [mae_metric])\n",
    "    \n",
    "    # epoch结束打印\n",
    "    print_status_bar(n_steps, n_steps, mean_loss.result(), [mae_metric])\n",
    "    \n",
    "    # 重置指标\n",
    "    mean_loss.reset_state()\n",
    "    mae_metric.reset_state()\n",
    "\n",
    "print(\"\\n训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_load_title",
   "metadata": {},
   "source": [
    "## 8. 保存和加载自定义组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_load_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 创建使用自定义组件的模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=(8,)),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=HuberLoss(threshold=1.5), optimizer='adam')\n",
    "\n",
    "# 训练一个epoch\n",
    "model.fit(X_train, y_train, epochs=1, verbose=0)\n",
    "\n",
    "# 保存模型\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    model_path = os.path.join(tmpdir, 'model_with_custom_loss.keras')\n",
    "    model.save(model_path)\n",
    "    print(f\"模型已保存到: {model_path}\")\n",
    "    \n",
    "    # 加载模型时需要提供自定义对象\n",
    "    loaded_model = keras.models.load_model(\n",
    "        model_path,\n",
    "        custom_objects={\"HuberLoss\": HuberLoss}\n",
    "    )\n",
    "    print(f\"模型加载成功!\")\n",
    "    \n",
    "    # 验证\n",
    "    original_pred = model.predict(X_test[:5], verbose=0)\n",
    "    loaded_pred = loaded_model.predict(X_test[:5], verbose=0)\n",
    "    print(f\"预测结果一致: {np.allclose(original_pred, loaded_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## 知识点总结\n",
    "\n",
    "### 自定义组件速查表\n",
    "\n",
    "| 组件 | 继承类 | 必须实现的方法 |\n",
    "|-----|-------|---------------|\n",
    "| 损失函数 | `keras.losses.Loss` | `call(y_true, y_pred)` |\n",
    "| 评估指标 | `keras.metrics.Metric` | `update_state()`, `result()`, `reset_state()` |\n",
    "| 层 | `keras.layers.Layer` | `build()`, `call()` |\n",
    "| 模型 | `keras.Model` | `call()` |\n",
    "| 正则化器 | `keras.regularizers.Regularizer` | `__call__(weights)` |\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **函数式组件**：简单场景使用普通函数即可\n",
    "2. **类式组件**：需要保存配置时继承相应基类并实现`get_config()`\n",
    "3. **延迟构建**：在`build()`方法中创建权重，支持动态输入形状\n",
    "4. **自定义训练循环**：使用`tf.GradientTape`进行梯度计算\n",
    "5. **保存/加载**：通过`custom_objects`参数传递自定义类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}