
# 无监督学习:
1. 聚类: 将相似的实例分类到集群,用于数据分析 客户细分, 推荐系统, 搜索引擎, 图像分割 降维等
2. 异常检测: 识别与大多数实例显著不同的实例,用于信用卡欺诈检测, 网络入侵检测, 设备故障检测等
3. 密度估算: 估计数据分布的概率密度函数,用于数据生成, 异常检测, 数据可视化等
## 9.1 聚类算法
1. 根据数据的特征将相似的实例分配给相似实例的集群或者组
### 9.1.1 K-Means算法
1. 该算法尝试找出每个集群的中心 点, 然后将每个实例分配给距离最近的中心点所在的集群
2. 选择超参数k作为集群的数量 每个类的标签是该实例被算法分配给该集群的索引
3. K-Means通常是最快速的聚类算法之一, 但是前提是数据是聚类的 否则复杂度会很高
4. 中心点初始化方法:如果已经知道中心点 就将其传递给KMeans类的init参数 同时将n_init设置为1
5. 如果不知道中心点 就是用算法保留最优解 n_init参数指定运行KMeans算法的次数, 然后选择最优解默认状态下是10
6. 寻找中心的算法使用模型的惯性(inertia_)属性来衡量是否最优 可以通过inertia_instance变量来查看model的惯性
### 9.1.2 Kmeans++
1. Kmeans++算法是Kmeans算法的变体, 它使用更智能的方法来选择初始中心点 从而加速收敛 避免收敛到次优解
2. Kmeans默认使用Kmeans++算法来初始化中心点 如果想要使用Kmeans算法, 就将algorithm参数设置为"random
### 9.1.3 加速的k-means算法和小批量k-means
1. 如果需要使用原始k-means算法 将algorithm参数设置为"full"
2. 小批量k-means算法速度更快 但是惯性通常会更高 并且随着k的增加 误差也会增加
### 9.1.4 寻找最优的k值
1. 在选择最佳k值 惯性并不是一个好的指标 因为随着k的增加 惯性会不断减小
2. 集群越多 每个实例越接近最接近的中心点 因此惯性会不断减小
3. 肘部法则: 将惯性绘制成k的函数, 然后选择肘部附近的k值
4. 轮廓系数:(b-a)/max(a,b) a是实例i到它所在集群的其他实例的平均距离 b是实例i到除本身外最近的集群的所有实例的平均距离
5. 画出轮廓系数随k变化的图像, 选择轮廓系数最高的k值
### 9.1.5 K-means的局限性
1. 要多次运行算法才能避免次优解 而且要指定集群值k
2. 在椭球形集群或者不同大小和密度的集群上表现不佳
3. 进行K-means聚类之前, 需要缩放数据 避免集群变形
### 9.1.6 DBSCAN算法
1. DBSCAN算法是一种基于密度的聚类算法, 它将密度相连的实例分配到同一个集群
2. 对于每个实例, 如果在指定半径eps内有足够多的邻居(min_samples), 则该实例被视为核心点
3. 核心实力附近的所有实例都属于同一个集群 这个领域可能包括其他核心实例
4. DBCAN算法不可以预测新实例属于那个集群 因为不同算法可以更好完成不同的任务 比如可以使用KNN算法来预测新实例的集群 将dbscan的标签作为KNN的训练集, 然后使用KNN来预测新实例的标签
5. 也可以选择在DBSCAN的异常值上面训练一个分类器 集体什么取决于具体任务
6. DBCAN具有鲁棒性 但是对于密度变化较大的集群表现不佳 但是eps比较大的话需要的内存可能比较多
### 9.1.7 其它聚类方法
1. 聚集聚类: 适用于小型训练集
2. BIRCH: 专为大型训练集准备 但是要求特征数量不要太大 一般小于20 他可以比批量处理kmeans更快 结果相似 和聚集聚类一样以树的原理构建
3. 均值漂移:在每个实例上居中放一个○ 对于每个元计算所有实例均值 然后移动○到均值位置 重复这个过程 直到移动距离小于阈值
4. 相似性传播: 投票机制 对相似的实例进行投票将其作为代表 一旦算法收敛每个代表及其投票者组成一个集群 适合于任何数量的集群 但是复杂度是O(n^2) 适合小型数据集
5. 谱聚类: 适用于高维数据集 当集群大小不同结构复杂时表现不好
## 9.2 高斯混合模型
1. 高斯混合模型(GMM)是K-means算法的扩展, 它可以处理非球形集群, 并且每个集群可以有不同的大小和密度 假定每个实例都是高斯分布分到的
2. GaussianMixture: 必须事先指定集群的数量k
3. 细节证明这里不展开 涉及太多数学推导 等数学功底打好之后看论文