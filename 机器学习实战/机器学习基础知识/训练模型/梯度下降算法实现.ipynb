{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T02:13:36.981958Z",
     "start_time": "2025-10-17T02:13:36.977959Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 梯度下降算法 (Gradient Descent)\n# ============================================================\n# \n# 梯度下降是一种优化算法，用于找到函数的最小值\n# 核心思想：沿着梯度（最陡峭方向）的反方向迭代更新参数\n# \n# 更新规则：θ = θ - η × ∇J(θ)\n# 其中：\n#   - η (eta): 学习率，控制每步移动的大小\n#   - ∇J(θ): 损失函数的梯度\n# \n# 对于线性回归，损失函数（均方误差）的梯度：\n# ∇J(θ) = (2/m) × Xᵀ(Xθ - y)\n# ============================================================\n\nimport numpy as np\n\n# 设置随机种子，确保结果可复现\nnp.random.seed(42)\n\n# ============================================================\n# 第一步：准备数据\n# ============================================================\n# 生成与之前相同的线性数据\n\nn_samples = 100  # 样本数量\n\n# 生成特征 X：在 [2, 3) 范围内\nX = 2 + np.random.rand(n_samples, 1)\n\n# 生成目标 y：y = 4 + 3x + 噪声\ny = 4 + 3 * X + np.random.rand(n_samples, 1)\n\n# 构建设计矩阵（添加偏置项）\nX_b = np.c_[np.ones((n_samples, 1)), X]\n\nprint(f\"数据形状: X_b = {X_b.shape}, y = {y.shape}\")\nprint(f\"真实参数: θ₀ ≈ 4.5, θ₁ = 3\")",
   "id": "b044699c6a914364",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T02:14:26.642234Z",
     "start_time": "2025-10-17T02:14:26.633973Z"
    }
   },
   "source": "# ============================================================\n# 第二步：批量梯度下降 (Batch Gradient Descent)\n# ============================================================\n# \n# 批量梯度下降在每次迭代中使用所有样本计算梯度\n# \n# 超参数设置：\n#   - 学习率 (learning rate): 太大会发散，太小收敛慢\n#   - 迭代次数: 足够多才能收敛\n\n# 超参数\neta = 0.1           # 学习率 (learning rate)\nn_iterations = 1000 # 最大迭代次数\nm = n_samples       # 样本数量\n\n# 随机初始化参数\ntheta = np.random.randn(2, 1)\nprint(f\"初始参数: θ₀ = {theta[0, 0]:.4f}, θ₁ = {theta[1, 0]:.4f}\")\n\n# 记录训练过程中的损失值\nloss_history = []\n\n# ============================================================\n# 梯度下降迭代\n# ============================================================\nfor iteration in range(n_iterations):\n    # 1. 计算预测值: ŷ = Xθ\n    y_pred = X_b.dot(theta)\n    \n    # 2. 计算误差: error = ŷ - y\n    error = y_pred - y\n    \n    # 3. 计算梯度: ∇J(θ) = (2/m) × Xᵀ × error\n    gradients = (2 / m) * X_b.T.dot(error)\n    \n    # 4. 更新参数: θ = θ - η × ∇J(θ)\n    theta = theta - eta * gradients\n    \n    # 5. 计算并记录损失值 (MSE)\n    loss = np.mean(error ** 2)\n    loss_history.append(loss)\n    \n    # 每200次迭代输出一次\n    if iteration % 200 == 0:\n        print(f\"迭代 {iteration:4d}: θ₀ = {theta[0,0]:.4f}, θ₁ = {theta[1,0]:.4f}, Loss = {loss:.6f}\")\n\nprint(f\"\\n最终结果 (迭代 {n_iterations} 次):\")\nprint(f\"  θ₀ = {theta[0, 0]:.4f}\")\nprint(f\"  θ₁ = {theta[1, 0]:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T02:14:26.843252Z",
     "start_time": "2025-10-17T02:14:26.840252Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第三步：可视化训练过程\n# ============================================================\n\nimport matplotlib.pyplot as plt\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 图1：损失函数变化曲线\naxes[0].plot(loss_history, 'b-', linewidth=1)\naxes[0].set_xlabel('迭代次数', fontsize=12)\naxes[0].set_ylabel('均方误差 (MSE)', fontsize=12)\naxes[0].set_title('训练损失曲线', fontsize=14)\naxes[0].set_yscale('log')  # 使用对数刻度更清晰\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=loss_history[-1], color='r', linestyle='--', alpha=0.5,\n                label=f'最终损失: {loss_history[-1]:.6f}')\naxes[0].legend()\n\n# 图2：拟合结果\naxes[1].scatter(X, y, alpha=0.6, label='训练数据', color='blue')\nX_plot = np.array([[2], [3]])\nX_plot_b = np.c_[np.ones((2, 1)), X_plot]\ny_plot = X_plot_b.dot(theta)\naxes[1].plot(X_plot, y_plot, 'r-', linewidth=2, \n             label=f'拟合直线: y = {theta[0,0]:.2f} + {theta[1,0]:.2f}x')\naxes[1].set_xlabel('X', fontsize=12)\naxes[1].set_ylabel('y', fontsize=12)\naxes[1].set_title('梯度下降拟合结果', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n最终参数:\")\nprint(f\"  θ₀ (截距): {theta[0, 0]:.4f}\")\nprint(f\"  θ₁ (斜率): {theta[1, 0]:.4f}\")",
   "id": "caffc6fd07f02e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================\n# 第四步：学习率对收敛的影响\n# ============================================================\n# \n# 学习率是梯度下降中最重要的超参数：\n#   - 太小：收敛速度慢，需要更多迭代\n#   - 太大：可能震荡甚至发散\n#   - 合适：快速稳定收敛\n\ndef gradient_descent(X_b, y, eta, n_iterations):\n    \"\"\"梯度下降算法\"\"\"\n    m = len(y)\n    theta = np.random.randn(2, 1)\n    np.random.seed(42)  # 确保初始值一致\n    theta = np.random.randn(2, 1)\n    loss_history = []\n    \n    for _ in range(n_iterations):\n        gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)\n        theta = theta - eta * gradients\n        loss = np.mean((X_b.dot(theta) - y) ** 2)\n        loss_history.append(loss)\n    \n    return theta, loss_history\n\n# 测试不同的学习率\nlearning_rates = [0.001, 0.01, 0.1, 0.5]\n\nplt.figure(figsize=(12, 4))\nfor i, eta in enumerate(learning_rates):\n    theta_result, history = gradient_descent(X_b, y, eta, 100)\n    plt.subplot(1, 4, i+1)\n    plt.plot(history, linewidth=2)\n    plt.xlabel('迭代次数')\n    plt.ylabel('MSE')\n    plt.title(f'η = {eta}')\n    plt.grid(True, alpha=0.3)\n    if eta >= 0.5:\n        plt.ylim(0, 20)  # 限制y轴范围\n\nplt.suptitle('不同学习率的收敛行为', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# ============================================================\n# 知识总结\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"梯度下降算法总结\")\nprint(\"=\" * 60)\nprint(\"\"\"\n1. 核心公式:\n   θ = θ - η × ∇J(θ)\n\n2. 三种变体:\n   - 批量梯度下降 (BGD): 使用全部样本，稳定但慢\n   - 随机梯度下降 (SGD): 每次一个样本，快但震荡\n   - 小批量梯度下降 (Mini-batch): 折中方案，最常用\n\n3. 学习率选择:\n   - 常见范围: 0.001 ~ 0.1\n   - 可使用学习率衰减策略\n   - 或自适应学习率 (Adam, RMSprop等)\n\n4. 收敛判断:\n   - 损失变化小于阈值\n   - 达到最大迭代次数\n   - 梯度范数足够小\n\"\"\")",
   "id": "c4f66f7ea903df8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}