{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T14:10:26.148560Z",
     "start_time": "2025-10-16T14:10:26.144554Z"
    }
   },
   "source": "# ============================================================\n# 线性回归 - 标准方程法 (Normal Equation)\n# ============================================================\n# \n# 线性回归是最基础的机器学习算法之一\n# 目标：找到最佳参数 θ，使预测值与真实值的误差最小\n# \n# 数学模型: y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ\n# 矩阵形式: ŷ = Xθ\n# \n# 标准方程（闭式解）: θ = (XᵀX)⁻¹Xᵀy\n# ============================================================\n\nimport numpy as np\n\n# 设置随机种子，确保结果可复现\nnp.random.seed(42)\n\n# ============================================================\n# 第一步：生成模拟数据\n# ============================================================\n# 真实关系：y = 4 + 3x + 噪声\n# θ₀ (截距) = 4, θ₁ (斜率) = 3\n\nn_samples = 100  # 样本数量\n\n# 生成特征 X：在 [2, 3) 范围内的均匀分布\nX = 2 + np.random.rand(n_samples, 1)\n\n# 生成目标 y：线性关系 + 随机噪声\ny = 4 + 3 * X + np.random.rand(n_samples, 1)\n\nprint(f\"样本数量: {n_samples}\")\nprint(f\"X 形状: {X.shape}\")\nprint(f\"y 形状: {y.shape}\")\n\n# ============================================================\n# 第二步：使用标准方程求解最佳参数\n# ============================================================\n# 公式: θ = (XᵀX)⁻¹Xᵀy\n# \n# 需要在 X 前面添加一列 1（偏置项），构成设计矩阵 X_b\n\nX_b = np.c_[np.ones((n_samples, 1)), X]  # 添加 x₀ = 1 列\nprint(f\"\\n设计矩阵 X_b 形状: {X_b.shape}\")\n\n# 计算最佳参数\n# np.linalg.inv: 计算矩阵的逆\n# .dot(): 矩阵乘法\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\nprint(f\"\\n最佳参数 θ:\")\nprint(f\"  θ₀ (截距): {theta_best[0, 0]:.4f}  (真实值约为 4.5)\")\nprint(f\"  θ₁ (斜率): {theta_best[1, 0]:.4f}  (真实值为 3)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:10:26.763336Z",
     "start_time": "2025-10-16T14:10:26.692322Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第三步：可视化数据和拟合结果\n# ============================================================\n\nimport matplotlib.pyplot as plt\n\n# 设置中文字体（如果需要）\nplt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 创建图形\nplt.figure(figsize=(10, 6))\n\n# 绘制原始数据点\nplt.scatter(X, y, alpha=0.6, label='训练数据', color='blue')\n\n# 绘制拟合直线\nX_plot = np.array([[2], [3]])  # 用于绘图的 x 范围\nX_plot_b = np.c_[np.ones((2, 1)), X_plot]\ny_plot = X_plot_b.dot(theta_best)\nplt.plot(X_plot, y_plot, 'r-', linewidth=2, label=f'拟合直线: y = {theta_best[0,0]:.2f} + {theta_best[1,0]:.2f}x')\n\nplt.xlabel('X (特征)', fontsize=12)\nplt.ylabel('y (目标)', fontsize=12)\nplt.title('线性回归 - 标准方程法', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
   "id": "c645013d9dcb0328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:10:27.324026Z",
     "start_time": "2025-10-16T14:10:27.320178Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第四步：使用训练好的模型进行预测\n# ============================================================\n# \n# 预测公式: ŷ = Xθ\n# 给定新的 x 值，计算对应的 y 预测值\n\n# 定义新的测试点\nX_new = np.array([[0], [2]])  # 两个测试点: x=0 和 x=2\n\n# 构建设计矩阵（添加偏置项）\nX_new_b = np.c_[np.ones((2, 1)), X_new]\n\n# 进行预测\ny_predict = X_new_b.dot(theta_best)\n\nprint(\"预测结果:\")\nprint(\"-\" * 40)\nfor i in range(len(X_new)):\n    print(f\"  x = {X_new[i, 0]:.1f}  -->  ŷ = {y_predict[i, 0]:.4f}\")\nprint(\"-\" * 40)\nprint(f\"\\n验证：当 x=0 时，ŷ ≈ θ₀ = {theta_best[0, 0]:.4f}\")",
   "id": "982b87eb8e0af807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:10:27.847114Z",
     "start_time": "2025-10-16T14:10:27.788068Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 可视化预测结果\n# ============================================================\n\nplt.figure(figsize=(10, 6))\n\n# 绘制原始数据点\nplt.plot(X, y, \"b.\", alpha=0.6, markersize=8, label='训练数据')\n\n# 绘制预测直线（连接两个预测点）\nplt.plot(X_new, y_predict, \"r-\", linewidth=2, label='预测直线')\n\n# 标记预测点\nplt.scatter(X_new, y_predict, color='red', s=100, zorder=5, \n            edgecolors='black', linewidths=1.5, label='预测点')\n\n# 添加预测点标注\nfor i in range(len(X_new)):\n    plt.annotate(f'({X_new[i,0]:.0f}, {y_predict[i,0]:.2f})', \n                 xy=(X_new[i,0], y_predict[i,0]),\n                 xytext=(10, 10), textcoords='offset points',\n                 fontsize=10, color='red')\n\nplt.xlabel('X', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.axis([0, 3, 0, 15])\nplt.title('线性回归预测可视化', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()",
   "id": "6dba9f02e6a8b85b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:10:28.547851Z",
     "start_time": "2025-10-16T14:10:28.391340Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第五步：使用 Scikit-learn 的线性回归\n# ============================================================\n# \n# Scikit-learn 提供了更简洁的 API 来实现线性回归\n# 内部使用的是更稳定的数值方法（SVD分解）\n\nfrom sklearn.linear_model import LinearRegression\n\n# 创建线性回归模型实例\nlin_reg = LinearRegression()\n\n# 训练模型（拟合数据）\n# 注意：sklearn 会自动处理偏置项，不需要手动添加\nlin_reg.fit(X, y)\n\n# 查看学习到的参数\nprint(\"Scikit-learn 线性回归结果:\")\nprint(\"-\" * 40)\nprint(f\"  截距 (intercept_): {lin_reg.intercept_[0]:.4f}\")\nprint(f\"  系数 (coef_):      {lin_reg.coef_[0, 0]:.4f}\")\nprint(\"-\" * 40)\n\n# 与手动计算结果对比\nprint(\"\\n与标准方程法结果对比:\")\nprint(f\"  标准方程 θ₀: {theta_best[0, 0]:.4f}\")\nprint(f\"  sklearn θ₀:  {lin_reg.intercept_[0]:.4f}\")\nprint(f\"  差异: {abs(theta_best[0, 0] - lin_reg.intercept_[0]):.10f}\")\n\n# 使用模型预测\ny_pred_sklearn = lin_reg.predict(X_new)\nprint(f\"\\nsklearn 预测结果:\")\nfor i in range(len(X_new)):\n    print(f\"  x = {X_new[i, 0]:.1f}  -->  ŷ = {y_pred_sklearn[i, 0]:.4f}\")",
   "id": "9dca7c6133172263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:11:47.603068Z",
     "start_time": "2025-10-16T14:11:47.599068Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第六步：理解底层实现 - SVD 分解\n# ============================================================\n# \n# sklearn 的 LinearRegression 使用 np.linalg.lstsq 函数\n# 该函数基于奇异值分解(SVD)，比标准方程更稳定\n# \n# 优点：\n# - 当 XᵀX 接近奇异矩阵时不会出错\n# - 数值稳定性更好\n# - 计算效率更高\n\n# 使用 lstsq 直接求解\ntheta_svd, residuals, rank, singular_values = np.linalg.lstsq(X_b, y, rcond=None)\n\nprint(\"SVD 方法求解结果:\")\nprint(\"-\" * 40)\nprint(f\"  θ₀ (截距): {theta_svd[0, 0]:.4f}\")\nprint(f\"  θ₁ (斜率): {theta_svd[1, 0]:.4f}\")\nprint(\"-\" * 40)\nprint(f\"\\n矩阵秩 (rank): {rank}\")\nprint(f\"奇异值: {singular_values}\")",
   "id": "99e7639e6209b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:19.768881Z",
     "start_time": "2025-10-16T14:14:19.762882Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第七步：使用伪逆矩阵计算\n# ============================================================\n# \n# Moore-Penrose 伪逆矩阵: X⁺ = (XᵀX)⁻¹Xᵀ\n# 当 XᵀX 不可逆时，伪逆仍然存在\n# \n# 标准方程可以改写为: θ = X⁺y\n\n# 计算伪逆\nX_b_pinv = np.linalg.pinv(X_b)\n\n# 使用伪逆计算参数\ntheta_pinv = X_b_pinv.dot(y)\n\nprint(\"伪逆矩阵方法求解结果:\")\nprint(\"-\" * 40)\nprint(f\"  θ₀ (截距): {theta_pinv[0, 0]:.4f}\")\nprint(f\"  θ₁ (斜率): {theta_pinv[1, 0]:.4f}\")\nprint(\"-\" * 40)\n\n# ============================================================\n# 总结：三种方法得到相同的结果\n# ============================================================\nprint(\"\\n三种方法对比:\")\nprint(\"-\" * 50)\nprint(f\"{'方法':<20} {'θ₀':>12} {'θ₁':>12}\")\nprint(\"-\" * 50)\nprint(f\"{'标准方程法':<20} {theta_best[0,0]:>12.4f} {theta_best[1,0]:>12.4f}\")\nprint(f\"{'SVD分解':<20} {theta_svd[0,0]:>12.4f} {theta_svd[1,0]:>12.4f}\")\nprint(f\"{'伪逆矩阵':<20} {theta_pinv[0,0]:>12.4f} {theta_pinv[1,0]:>12.4f}\")\nprint(\"-\" * 50)",
   "id": "bbade39c993456ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================\n# 知识总结\n# ============================================================\n# \n# 1. 线性回归的数学模型：ŷ = Xθ\n# \n# 2. 求解方法：\n#    - 标准方程法: θ = (XᵀX)⁻¹Xᵀy （直接闭式解）\n#    - SVD分解: 更稳定的数值方法\n#    - 伪逆矩阵: 适用于奇异矩阵情况\n# \n# 3. Scikit-learn 使用方法：\n#    model = LinearRegression()\n#    model.fit(X, y)\n#    predictions = model.predict(X_new)\n# \n# 4. 标准方程法的时间复杂度: O(n³)\n#    当特征数量很大时，应考虑使用梯度下降法\n# \n# ============================================================\n\nprint(\"线性回归学习完成！\")\nprint(\"\\n下一步学习建议：\")\nprint(\"  - 梯度下降算法（适用于大规模数据）\")\nprint(\"  - 多项式回归（处理非线性关系）\")\nprint(\"  - 正则化方法（Ridge、Lasso、Elastic Net）\")",
   "id": "ddd71dd799a399c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}