{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-18T02:36:36.692516Z",
     "start_time": "2025-10-18T02:36:36.357986Z"
    }
   },
   "source": "# ============================================================\n# 支持向量机 (Support Vector Machine, SVM)\n# ============================================================\n# \n# SVM 是一种强大的分类算法，核心思想是：\n# 找到一个超平面，使得两类样本之间的间隔(margin)最大化\n# \n# 关键概念：\n# 1. 超平面 (Hyperplane): 分隔不同类别的决策边界\n# 2. 支持向量 (Support Vectors): 距离超平面最近的样本点\n# 3. 间隔 (Margin): 超平面到最近样本点的距离\n# \n# 硬间隔 vs 软间隔：\n# - 硬间隔：要求完全分开，不允许任何错误\n# - 软间隔：允许一些违规，更鲁棒\n# \n# 正则化参数 C：\n# - C 大：严格分类，可能过拟合\n# - C 小：更大的间隔，允许更多违规\n# ============================================================\n\nfrom sklearn import datasets\nimport numpy as np\n\n# 设置随机种子，确保结果可复现\nnp.random.seed(42)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\n# ============================================================\n# 第一步：加载和准备数据\n# ============================================================\n\n# 加载鸢尾花数据集\niris = datasets.load_iris()\n\n# 只使用两个特征：花瓣长度和花瓣宽度\nX = iris['data'][:, (2, 3)]  # (petal length, petal width)\n\n# 二分类：是否为 Virginica 类\ny = (iris['target'] == 2).astype(np.float64)\n\nprint(f\"数据形状: X = {X.shape}\")\nprint(f\"类别分布: Virginica = {sum(y)}, 非Virginica = {len(y) - sum(y)}\")\n\n# ============================================================\n# 第二步：创建 SVM Pipeline\n# ============================================================\n# \n# 重要：SVM 对特征尺度敏感，必须先标准化！\n\nsvm_clf = Pipeline([\n    # 标准化：将特征缩放到均值=0，标准差=1\n    ('scaler', StandardScaler()),\n    \n    # 线性 SVM 分类器\n    # C=1: 正则化参数，控制间隔和违规之间的权衡\n    # loss='hinge': 使用标准的 hinge 损失函数\n    ('linear_svc', LinearSVC(C=1, loss='hinge', random_state=42))\n])\n\n# 训练模型\nsvm_clf.fit(X, y)\n\nprint(\"\\nPipeline 结构:\")\nfor name, step in svm_clf.steps:\n    print(f\"  {name}: {type(step).__name__}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:36:58.991259Z",
     "start_time": "2025-10-18T02:36:58.986261Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================\n# 第三步：使用模型进行预测\n# ============================================================\n\n# 测试样本：花瓣长度=5.5cm, 花瓣宽度=1.7cm\ntest_samples = [[5.5, 1.7], [2.0, 0.5], [4.0, 1.3]]\n\nprint(\"预测结果:\")\nprint(\"-\" * 50)\nprint(f\"{'花瓣长度':<10} {'花瓣宽度':<10} {'预测类别':<15}\")\nprint(\"-\" * 50)\n\nfor sample in test_samples:\n    pred = svm_clf.predict([sample])[0]\n    class_name = \"Virginica\" if pred == 1 else \"非Virginica\"\n    print(f\"{sample[0]:<10} {sample[1]:<10} {class_name:<15}\")\n\nprint(\"-\" * 50)",
   "id": "be517bb15d0b07b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================\n# 第四步：可视化决策边界\n# ============================================================\n\nimport matplotlib.pyplot as plt\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n\n# 创建网格\nx0_min, x0_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\nx1_min, x1_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n                        np.linspace(x1_min, x1_max, 200))\n\n# 预测网格点\nX_grid = np.c_[xx0.ravel(), xx1.ravel()]\ny_pred = svm_clf.predict(X_grid).reshape(xx0.shape)\n\n# 获取决策函数值（用于绘制间隔边界）\ndecision_values = svm_clf.decision_function(X_grid).reshape(xx0.shape)\n\nplt.figure(figsize=(10, 6))\n\n# 绘制决策边界和间隔\nplt.contourf(xx0, xx1, y_pred, alpha=0.3, cmap='RdYlGn')\nplt.contour(xx0, xx1, decision_values, levels=[-1, 0, 1], \n            colors=['blue', 'black', 'blue'], linestyles=['--', '-', '--'])\n\n# 绘制训练样本\nplt.scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', \n            edgecolors='black', s=50, label='非Virginica')\nplt.scatter(X[y==1, 0], X[y==1, 1], c='green', marker='s', \n            edgecolors='black', s=50, label='Virginica')\n\nplt.xlabel('花瓣长度 (cm)', fontsize=12)\nplt.ylabel('花瓣宽度 (cm)', fontsize=12)\nplt.title('SVM 线性分类器 - 决策边界与间隔', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# ============================================================\n# 知识总结\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"支持向量机 (SVM) 总结\")\nprint(\"=\" * 60)\nprint(\"\"\"\n1. 核心思想：最大间隔分类器\n   - 找到使间隔最大化的超平面\n   - 只有支持向量决定边界位置\n\n2. 关键参数：\n   - C: 正则化参数（C大=硬间隔，C小=软间隔）\n   - kernel: 核函数类型 ('linear', 'rbf', 'poly')\n   - gamma: RBF核的参数\n\n3. 使用建议：\n   - 必须标准化特征\n   - 对高维数据效果好\n   - 样本量大时训练较慢\n\n4. 常用类：\n   - LinearSVC: 线性SVM，训练快\n   - SVC: 支持核技巧，更灵活\n   - SVR: SVM回归\n\"\"\")",
   "id": "8210f952ed09bf14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}