# Decision Tree
## 6.1 训练和可视化决策树
1. 决策树不需要数据居中和特征缩放
2. 基尼纯度: 随机从数据中选出一个样本 能精准判断它属于哪一类的容易程度 纯度越高 模型越能判断选出来的样本是够属于同一类
3. Sklearn 里面的Decision Tree 使用的是CART算法 非叶结点只有两个两个子节点 即答案仅有是和否

## 6.4 CART训练算法
1. CART是一种贪婪算法 从最顶部开始以最优分裂为条件 每层重复这个操作进行递归 会产生一个不错的解 但是不保证最优解

## 6.5 时间复杂度
1. 遍历决策树的时间复杂度为O(log2n) n为样本数量
2. 训练算法比较每个节点所有样本特征 时间复杂度为O(n * mlog2m) n为样本数量 m为特征数量
3. 对于小数据集 Scikit-learn可以通过提前排序(presort= True)来加速训练 或者设置max-features

## 6.6 基尼不纯度或熵
1. 可以将超参数criterion设置为entropy 使用熵来作为不纯度的训练指标
2. 实际上两个指标生产的树区别不大 基尼不纯度训练速度更快
3. 基尼训练分裂出最常见的类别 熵生产更加平衡的树

## 6.7 正则化参数
1. max_depth: 树的最大深度 减少max_depth可以防止过拟合
2. min_samples_split: 节点分裂前节点所必要的最小样本数
3. min_samples_leaf: 叶子节点所必要的最小样本数
4. max_leaf_nodes: 树的最大叶子节点数
5. 或者不进行正则化 在model训练完成之后进行剪枝 如果一个节点的所有子节点全都为叶节点 那么该节点则是没必要的 除非它的表示很有意义
6. 通过标准统计测试 通过零假设来判断子节点是有必要的还是出于偶然而产生的

## 6.8 回归
1. 回归树使用均方误差(MSE)作为不纯度度量 而不是GINI或熵

## 6.9 不稳定性
1. 决策树对数据的不稳定只要来源于对数据的旋转 因为决策树更擅长处理可以正交可分的数据集
2. 对于不可分的可以使用主成分分析来解决 或者随机森林可以降低这种不稳定性