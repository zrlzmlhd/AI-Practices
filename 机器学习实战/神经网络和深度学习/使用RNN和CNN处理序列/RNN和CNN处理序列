# 第十五章：RNN和CNN处理序列

## 15.1 循环神经元和层

#### 核心理念

常规的密集神经网络（DNN）假设所有输入是相互独立的。它们没有“记忆”的概念。**循环神经网络（Recurrent Neural Networks, RNN）** 专为处理序列数据而设计，例如时间序列（股票价格）、文本（单词序列）或音频（声波样本）。

其核心思想是神经元不仅接收当前的输入 $x_t$，还接收**上一个时间步的隐藏状态（hidden state） $h_{t-1}$**。这个隐藏状态 $h_{t-1}$ 充当了网络对过去所有时间步的“记忆”。

在时间步 $t$，一个简单的循环神经元（SimpleRNN）的计算如下：

1.  **计算当前隐藏状态：** $h_t = \phi(W_x \cdot x_t + W_h \cdot h_{t-1} + b)$
2.  **计算当前输出：** $y_t = \phi_{out}(W_y \cdot h_t + b_y)$

<!-- end list -->

  * $x_t$ 是时间步 $t$ 的输入。
  * $h_{t-1}$ 是上一个时间步的隐藏状态。在第一个时间步 $t=0$ 时， $h_{-1}$ 通常初始化为全零向量。
  * $W_x$ 是输入权重矩阵。
  * $W_h$ 是隐藏状态（循环）权重矩阵。
  * $b$ 是偏置项。
  * $\phi$ 是激活函数，在SimpleRNN中通常是**双曲正切函数（tanh）**。

#### 设计目的

设计RNN的目的是让网络能够**捕捉序列中的时间依赖关系**。例如，在理解一个句子时，当前单词的含义严重依赖于它前面的单词。RNN通过传递隐藏状态 $h_t$ 来实现这种对上下文的记忆。

#### 应用场景

  * **自然语言处理（NLP）：** 文本分类、机器翻译、情感分析、命名实体识别。
  * **时间序列预测：** 股票价格、天气预报、电力消耗预测。
  * **语音识别** 和 **音乐生成**。

#### 注意事项与Keras实现

  * **权重共享：** 关键在于，在所有时间步中，$W_x$ 和 $W_h$ 是**相同**的。网络不是为每个时间步学习一套新权重，而是学习一种“状态转移规则”。
  * **Keras实现：** `keras.layers.SimpleRNN`
      * `units`：隐藏状态 $h_t$ 的维度（即神经元的数量）。
      * `activation`：隐藏状态的激活函数，默认为 `'tanh'`。
      * `return_sequences`：
          * `False` (默认)：只返回序列的**最后一个**时间步的隐藏状态 $h_{last}$。形状为 `[batch_size, units]`。
          * `True`：返回**每个**时间步的隐藏状态 $h_0, h_1, ..., h_{last}$。形状为 `[batch_size, timesteps, units]`。

<!-- end list -->

```python
import tensorflow as tf
from tensorflow import keras

# 假设输入序列长度为10，每个时间步有5个特征
input_shape = (10, 5) 

# 默认只返回最后一个输出
model = keras.Sequential([
    keras.layers.SimpleRNN(units=32, input_shape=input_shape)
])
# model.output_shape -> (None, 32)

# 返回所有时间步的输出（用于堆叠RNN层）
model_seq = keras.Sequential([
    keras.layers.SimpleRNN(units=32, input_shape=input_shape, return_sequences=True)
])
# model_seq.output_shape -> (None, 10, 32)
```

### 15.1.1 记忆单元

#### 核心理念

“记忆单元”（Memory Cell）在广义上是指RNN中负责**存储和传递状态**的整个计算单元。在SimpleRNN中，这个单元就是执行 $h_t = \tanh(W_x \cdot x_t + W_h \cdot h_{t-1} + b)$ 的那个神经元。

在更高级的RNN（如LSTM和GRU）中，“记忆单元”是一个更复杂的结构，它包含多个门（gate）来精细地控制信息流，从而实现长期记忆。

#### 设计目的

记忆单元的设计目的是为了**在时间步之间传递信息**。SimpleRNN的单元设计简单，但它的记忆是“短暂的”。你提到的“10个步长”是指SimpleRNN在实践中通常只能有效回顾大约10个时间步长的信息，超过这个长度，来自早先时间步的信息（梯度）就会在反向传播中“消失”。

#### 应用场景

记忆单元是所有RNN（SimpleRNN, LSTM, GRU）的基础构建块。

#### 注意事项

  * SimpleRNN的记忆单元存在**短期记忆问题**（见15.4.2）。
  * $h_t$ 不仅是 $x_t$ 和 $h_{t-1}$ 的函数，它也是网络在时间步 $t$ 的**输出**（或输出的基础）。

### 15.1.2 输入和输出序列

RNN的灵活性在于它可以处理不同组合的输入和输出序列。

#### 1\. 序列到向量（Sequence-to-Vector / Many-to-One）

  * **核心理念：** 输入一个序列（如单词序列），输出一个单独的向量（如类别标签）。
  * **设计目的：** 对整个序列进行总结或分类。
  * **应用场景：**
      * **情感分析：** 输入电影评论（序列），输出“正面”或“负面”（向量）。
      * **文本分类：** 输入新闻文章（序列），输出类别（如“体育”、“科技”）。
  * **实现：** 在Keras中，使用 `return_sequences=False` (默认) 的RNN层，最后接一个 `Dense` 层。

#### 2\. 向量到序列（Vector-to-Sequence / One-to-Many）

  * **核心理念：** 输入一个单独的向量（如一张图片或一个起始词），输出一个序列。
  * **设计目的：** 基于一个固定的上下文生成序列。
  * **应用场景：**
      * **图像描述：** 输入一张图片（通过CNN编码为向量），输出描述该图片的句子（序列）。
      * **音乐生成：** 输入一个流派或种子音符（向量），生成一段旋律（序列）。
  * **实现：** 通常在每个时间步将相同的输入向量 $x$ 反复喂给RNN单元。

#### 3\. 序列到序列（Sequence-to-Sequence / Many-to-Many）

  * **核心理念：** 输入一个序列，输出一个序列。这里有两种情况：
      * **同步Seq-to-Seq：** 输入和输出序列的长度相同，且一一对应。
      * **异步Seq-to-Seq：** 输入和输出序列的长度可以不同（见下文编码器-解码器）。
  * **设计目的：** 对序列的**每个时间步**进行预测或标注。
  * **应用场景（同步）：**
      * **时间序列预测：** 输入过去N天的股价，预测未来N天的股价。
      * **视频帧分类：** 对视频的每一帧进行分类。
  * **实现（同步）：** 在Keras中，使用 `return_sequences=True` 的RNN层，并在**每个时间步**应用一个 `Dense` 层（通常使用 `TimeDistributed` 包装器）。

#### 4\. 编码器-解码器（Encoder-Decoder / 异步Seq-to-Seq）

  * **核心理念：** 这是Seq-to-Seq的一种高级形式。
    1.  **编码器（Encoder）：** 一个RNN读取整个输入序列，并将其压缩成一个固定大小的**上下文向量（context vector） $c$**（通常是编码器RNN的最后一个隐藏状态）。
    2.  **解码器（Decoder）：** 另一个RNN以 $c$ 作为其初始隐藏状态，然后逐个生成输出序列中的元素。
  * **设计目的：** 处理**输入和输出序列长度不同**的任务。
  * **应用场景：**
      * **机器翻译：** 输入“Hello”（英语序列），输出“你好”（中文序列）。
      * **文本摘要：** 输入长篇文章（序列），输出简短摘要（序列）。

-----

## 15.2 训练RNN

#### 核心理念

RNN的训练方式被称为**时间反向传播（Backpropagation Through Time, BPTT）**。

1.  **展开（Unrolling）：** 首先，我们将RNN“展开”成一个深的、常规的前馈神经网络。如果你的序列有10个时间步，你就得到了一个10层的网络。
2.  **权重共享：** 这个展开后的10层网络有一个特殊之处：**每一层（每个时间步）都共享相同的权重** ($W_x, W_h$)。
3.  **常规反向传播：** 然后，我们在这个展开的网络上执行标准的反向传播算法。
4.  **聚合梯度：** 由于所有时间步共享权重，来自 $y_0, y_1, ..., y_T$ 的所有梯度都会被**相加**，然后用于更新 $W_x$ 和 $W_h$。

#### 设计目的

BPTT是一种将标准反向传播算法应用于RNN（具有循环边和共享权重）的策略。

#### 应用场景

这是训练所有类型RNN（SimpleRNN, LSTM, GRU）的标准方法。

#### 注意事项

  * **成本函数：**
      * 对于**序列到向量**（如情感分析），成本函数只计算**最后一个输出** $y_{last}$ 的损失。
      * 对于**序列到序列**（如时间序列预测），成本函数会计算**每个时间步**输出 $y_t$ 的损失，然后将它们**求和或求平均**。
  * **问题：** 由于展开的网络非常“深”（深度 = 序列长度），BPTT极易受到**梯度消失/爆炸**问题的影响（见15.4）。

-----

## 15.3 预测时间序列

#### 核心理念

时间序列数据是按时间顺序排列的一系列数据点。

  * **单变量（Univariate）：** 每个时间步只有一个值（如：每日气温）。
  * **多变量（Multivariate）：** 每个时间步有多个值（如：股票的“开盘价、最高价、最低价、收盘价”）。

RNN非常适合时间序列，因为它们的天性就是捕捉顺序和模式。

#### 15.3.1 基准指标

#### 核心理念

在构建复杂的RNN模型之前，你必须先建立一个**基准（Baseline）**。如果你的复杂模型连基准都打不赢，那它就是无用的。

  * **朴素预测（Naïve Forecasting）：** 最简单的基准。预测下一个值就是当前的值，即 $\hat{y}_{t+1} = y_t$。
  * **移动平均（Moving Average）：** 预测下一个值是过去 $N$ 个值的平均值。

#### 衡量指标

  * **均方根误差 (RMSE, Root Mean Square Error)：** $RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2}$。它对大的误差（离群值）惩罚更重。
  * **平均绝对误差 (MAE, Mean Absolute Error)：** $MAE = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|$。它对所有误差的权重相同，更易于解释。

#### 15.3.2 实现一个简单的RNN

#### 核心理念

使用一个简单的RNN（如 `keras.layers.SimpleRNN`）来预测时间序列。

#### 应用场景

适用于学习短期模式或作为更复杂模型（如LSTM/GRU）的性能基准。

#### 注意事项与Keras实现

  * **数据准备（窗口化）：** RNN不能直接吃下整个时间序列。你必须将其转换为\*\*（输入窗口，目标）\*\*对。
      * 例如，要用过去5个时间步预测下1个时间步，数据应处理为：
          * `X = [[t_0, t_1, t_2, t_3, t_4], [t_1, t_2, t_3, t_4, t_5], ...]`
          * `y = [t_5, t_6, ...]`
  * **输入形状：** Keras RNN要求输入形状为 `[batch_size, timesteps, features]`。
      * 对于单变量时间序列，`timesteps` 是你的窗口大小（例如5），`features` 是1。
      * `input_shape` 应为 `(5, 1)`。

<!-- end list -->

```python
# 窗口大小为5，特征为1（单变量）
window_size = 5
n_features = 1

model = keras.Sequential([
    # 输入形状：[时间步长, 特征维度]
    keras.layers.SimpleRNN(32, input_shape=[window_size, n_features]),
    keras.layers.Dense(1) # 输出层，预测1个值
])

# 编译时使用MAE或MSE作为损失函数
model.compile(loss="mse", optimizer="adam", metrics=["mae"])
```

  * **趋势和季节性：** RNN（以及ARIMA等）在处理**平稳（Stationary）序列时效果最好（即均值和方差不随时间变化）。如果你的数据有明显的上升趋势（Trend）或周期性季节性（Seasonality）**：
    1.  **移除它们：** 例如，通过“差分”（$y'_t = y_t - y_{t-1}$）来移除趋势，或通过减去去年的值（$y'_t = y_t - y_{t-365}$）来移除季节性。
    2.  **训练模型：** 在处理过的平稳数据 $y'$ 上训练RNN。
    3.  **预测：** 用模型预测 $\hat{y}'_{t+1}$。
    4.  **添加回去：** 将趋势和季节性加回去，得到最终预测值 $\hat{y}_{t+1}$。

#### 15.3.3 深度RNN

#### 核心理念

**堆叠（Stacking）** 多个RNN层。第一层RNN处理原始输入，第二层RNN处理第一层RNN的输出（隐藏状态序列），以此类推。

#### 设计目的

更深的网络可以学习更复杂的模式和层次化特征。浅层RNN可能学习短期模式，深层RNN可能学习基于这些模式的长期模式。

#### 应用场景

当单个RNN层不足以捕捉数据复杂性时（例如，复杂的语言或时间序列模式）。

#### 注意事项与Keras实现

  * **`return_sequences=True` 规范：**
      * 当你堆叠RNN层时，**除了最后一层之外**的所有RNN层都**必须**设置 `return_sequences=True`。
      * **为什么？** 因为下一层RNN期望的输入是一个序列（形状 `[batch_size, timesteps, features]`），而不是单个向量（形状 `[batch_size, features]`）。
  * **最后一层：**
      * 如果你的最终目标是**序列到向量**（如情感分析），最后一层RNN应为 `return_sequences=False`。
      * 如果你的最终目标是**序列到序列**（如时间序列标注），最后一层RNN应为 `return_sequences=True`。

<!-- end list -->

```python
model_deep = keras.Sequential([
    # 第1层：必须返回序列
    keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, n_features]),
    # 第2层：必须返回序列
    keras.layers.SimpleRNN(32, return_sequences=True),
    # 第3层（最后RNN层）：
    # 如果是Seq-to-Vec，设为False
    # keras.layers.SimpleRNN(32, return_sequences=False),
    # keras.layers.Dense(1)
    
    # 如果是Seq-to-Seq，设为True
    keras.layers.SimpleRNN(32, return_sequences=True),
    # 并使用TimeDistributed
    keras.layers.TimeDistributed(keras.layers.Dense(1)) 
])
```

#### 15.3.4 预测未来几个时间步长

你有三种主要策略来预测未来 $N$ 个时间步（例如 $N=10$）。

#### 1\. 迭代式预测（Iterative Forecasts）

  * **核心理念：** 你只训练一个模型来预测**未来1个时间步**。
  * **预测过程：**
    1.  用 $[t_0, ..., t_9]$ 预测 $\hat{t}_{10}$。
    2.  将 $\hat{t}_{10}$ 添加到输入窗口中，得到 $[t_1, ..., t_9, \hat{t}_{10}]$。
    3.  用这个新窗口预测 $\hat{t}_{11}$。
    4.  ...重复 $N$ 次。
  * **优点：** 简单，只需要一个模型。
  * **缺点：** **误差会累积**。如果 $\hat{t}_{10}$ 稍有不准，这个误差会传递下去，导致 $\hat{t}_{11}$ 更不准，以此类推。预测越远，越不靠谱。

#### 2\. 向量输出（Vector Output / Seq-to-Vec）

  * **核心理念：** 训练一个模型**一次性输出所有 $N$ 个值**。
  * **实现：** 你的RNN层（`return_sequences=False`）之后，接一个 `Dense` 层，其单元数（units）等于你要预测的步数 $N$。
  * **优点：** 速度快（一次前向传播）。
  * **缺点：** 试图用一个模型同时学习 $N$ 个不同的预测任务，可能不如专门的模型准确。

<!-- end list -->

```python
# 预测未来10个步长
N_AHEAD = 10
model_vec = keras.Sequential([
    keras.layers.SimpleRNN(32, input_shape=[window_size, n_features]),
    keras.layers.Dense(N_AHEAD) # 直接输出10个值
])
# 模型的输出Y应该是 [t_10, t_11, ..., t_19]
```

#### 3\. 序列到序列（Sequence-to-Sequence）

  * **核心理念：** 训练一个模型，在**输入的每个时间步**都进行预测。
  * **数据准备：**
      * 输入 $X$：$[t_0, t_1, ..., t_9]$
      * 目标 $Y$：$[t_1, t_2, ..., t_{10}]$  (注意：目标序列通常是输入序列**偏移一个时间步**)
  * **实现：**
    1.  所有RNN层都必须设置 `return_sequences=True`。
    2.  在最后，使用 `keras.layers.TimeDistributed(keras.layers.Dense(1))_` 层。`TimeDistributed` 包装器会将 `Dense(1)` 应用到序列中的**每一个时间步**。
  * **优点：**
    1.  **训练更稳定：** 损失（Loss）是在每个时间步计算的，而不仅仅是在最后。这意味着梯度在BPTT期间可以从**每个时间步**流回，极大地缓解了梯度消失问题。
    2.  **模型更鲁棒：** 强制模型在 $t_1$ 时预测 $t_2$，在 $t_2$ 时预测 $t_3$，...
  * **预测：** 这种模型本身只预测 $N$ 个步长（$N$=输入窗口大小）。你可以将其与**迭代式预测**（方法1）结合使用，以获得最佳效果。

<!-- end list -->

```python
model_seq_to_seq = keras.Sequential([
    keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[window_size, n_features]),
    keras.layers.TimeDistributed(keras.layers.Dense(1)) # 在每个时间步应用Dense
])
```

-----

## 15.4 处理长序列

#### 核心理念

BPTT在长序列（例如100个时间步）上存在两个严重问题：

1.  **梯度消失（Vanishing Gradients）：** 在反向传播时，梯度在通过每一层（时间步）时都会乘以循环权重 $W_h$。如果 $W_h$ 的值（或其雅可比矩阵的特征值）小于1，梯度会呈指数级缩小，直到变为0。这导致网络**无法学习长期依赖关系**（即 $t_0$ 处的输入无法影响 $t_{100}$ 处的输出）。
2.  **梯度爆炸（Exploding Gradients）：** 相反，如果 $W_h$ 的值大于1，梯度会呈指数级增长，直到变为 `Inf` 或 `NaN`，导致训练崩溃。

### 15.4.1 应对不稳定梯度的问题

#### 1\. 梯度裁剪（Gradient Clipping）

  * **核心理念：** 这是解决**梯度爆炸**的标准方法。它在反向传播期间监控梯度向量的大小（范数）。如果梯度向量超过了某个阈值（threshold），就将其**按比例缩小**。
  * **设计目的：** 它不能“修复”梯度爆炸，但它能**防止**梯度变得过大而导致训练崩溃。
  * **应用场景：** **几乎总是**与RNN一起使用，尤其是当你看到损失（Loss）突然变为 `NaN` 时。
  * **Keras实现：** 在定义优化器时设置 `clipvalue` (按值裁剪) 或 `clipnorm` (按范数裁剪)。

<!-- end list -->

```python
# 将梯度裁剪到-1.0和1.0之间
optimizer = keras.optimizers.Adam(learning_rate=0.001, clipvalue=1.0)
model.compile(loss="mse", optimizer=optimizer)
```

#### 2\. 批量归一化 (Batch Normalization, BN)

  * **核心理念：** BN通过在每个mini-batch中对输入进行归一化（减去均值，除以标准差）来稳定训练。
  * **应用场景：** 在CNN和DNN中效果极好。
  * **注意事项：**
      * BN在RNN中**效果不佳**。你**不能**在时间步之间（即在 $h_{t-1}$ 和 $h_t$ 之间）应用BN，因为它会破坏RNN的动态记忆。
      * 它**可以**在循环层*之间*使用（例如，在 `SimpleRNN` 层1的输出和 `SimpleRNN` 层2的输入之间），或者应用在输入 $x_t$ 上。但总体而言，它带来的好处有限。

#### 3\. 层归一化 (Layer Normalization, LN)

  * **核心理念：** LN是BN的替代方案，专为RNN设计。
  * **设计目的（与BN的区别）：**
      * **BN** 是“跨批次”归一化：它计算**一个批次中所有样本**在*某个特定特征*上的均值和方差。
      * **LN** 是“跨特征”归一化：它计算**单个样本**在*所有特征*上的均值和方差。
  * **为什么LN适用于RNN？** 因为LN的计算完全在单个样本内部完成，不依赖于批次中的其他样本。这使得它可以在**每个时间步**独立地对隐藏状态 $h_t$ 进行归一化，而不会干扰时间动态。
  * **应用场景：** 当你需要归一化RNN内部状态以加速收敛和稳定训练时，**优先使用LN而不是BN**。
  * **Keras实现：** `keras.layers.LayerNormalization`。通常将其添加在RNN层之后。

<!-- end list -->

```python
model_ln = keras.Sequential([
    keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, n_features]),
    # 在RNN层的输出上应用层归一化
    keras.layers.LayerNormalization(), 
    keras.layers.SimpleRNN(32, return_sequences=True),
    keras.layers.LayerNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(1))
])
```

### 15.4.2 解决短期记忆问题

**梯度消失**导致了RNN的**短期记忆问题**：来自遥远过去的信息无法传播到当前时间步。LSTM和GRU是专门为解决这个问题而设计的。

#### 1\. LSTM (长短期记忆网络)

  * **核心理念：** LSTM是一种高级的RNN单元。它引入了一个**单元状态（Cell State） $c_t$**，作为与隐藏状态 $h_t$ 并行的“记忆高速公路”。

  * **设计目的：** $c_t$ 的设计目的是让信息可以**几乎不变地**流过长序列。 $c_t$ 只会经历非常轻微的线性变换。

  * **详细架构：** LSTM使用三个“门”（Gate）来精确控制 $c_t$ 和 $h_t$ 的信息流。这些门都是由 `sigmoid` 激活的微型神经网络，它们的输出在0（关闭）到1（打开）之间。

    1.  **遗忘门 (Forget Gate, $f_t$)：**
          * **作用：** 决定从*上一个单元状态* $c_{t-1}$ 中**丢弃**多少信息。
          * **计算：** $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
    2.  **输入门 (Input Gate, $i_t$)：**
          * **作用：** 决定让多少*新信息* $\tilde{g}_t$ **进入**到当前单元状态 $c_t$ 中。
          * **计算：** $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
          * **新信息 (Candidate, $\tilde{g}_t$)：** $\tilde{g}_t = \tanh(W_g \cdot [h_{t-1}, x_t] + b_g)$
    3.  **单元状态更新 (Cell State Update)：**
          * **计算：** $c_t = (f_t \odot c_{t-1}) + (i_t \odot \tilde{g}_t)$
          * **解释：** (第一部分：忘记旧信息) + (第二部分：添加新信息)。
    4.  **输出门 (Output Gate, $o_t$)：**
          * **作用：** 决定从*当前单元状态* $c_t$ 中**输出**多少信息作为*当前隐藏状态* $h_t$。
          * **计算：** $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
    5.  **隐藏状态更新 (Hidden State Update)：**
          * **计算：** $h_t = o_t \odot \tanh(c_t)$

  * **应用场景：** 处理**非常长**的序列，当SimpleRNN因梯度消失而失败时，LSTM是首选。

  * **Keras实现：** `keras.layers.LSTM`。它的参数（`units`, `return_sequences`）与 `SimpleRNN` 完全相同。

<!-- end list -->

```python
model_lstm = keras.Sequential([
    # 用法与SimpleRNN完全一样
    keras.layers.LSTM(32, return_sequences=True, input_shape=[None, n_features]) 
])
```

#### 2\. 窥视孔连接 (Peephole Connections)

  * **核心理念：** 这是LSTM的一个变体。常规LSTM的门（$f_t, i_t, o_t$）只看 $x_t$ 和 $h_{t-1}$。
  * **设计目的：** 允许门控制器\*\*“窥视”\*\*单元状态 $c_{t-1}$（遗忘门和输入门）和 $c_t$（输出门）。
  * **应用场景：** 在某些特定任务（如需要精确计时的任务）上可能略有帮助，但在大多数任务上，标准LSTM效果同样好甚至更好。Keras的 `LSTM` 层默认**不**使用它。

#### 3\. GRU (门控循环单元)

  * **核心理念：** GRU是LSTM的简化版本，由Cho等人在2014年提出。

  * **设计目的：** 它将LSTM的“遗忘门”和“输入门”合并为单个\*\*“更新门” ($z_t$)\*\*。它还合并了单元状态 $c_t$ 和隐藏状态 $h_t$，只保留一个 $h_t$。

  * **详细架构：** GRU只有两个门。

    1.  **重置门 (Reset Gate, $r_t$)：**
          * **作用：** 决定在计算*新信息*时，要**忽略**多少*上一个隐藏状态* $h_{t-1}$。
          * **计算：** $r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$
    2.  **更新门 (Update Gate, $z_t$)：**
          * **作用：** 决定**保留**多少 $h_{t-1}$ (类似遗忘门)，以及**添加**多少*新信息* $\tilde{h}_t$ (类似输入门)。
          * **计算：** $z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$
    3.  **新信息 (Candidate, $\tilde{h}_t$)：**
          * **计算：** $\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$
          * **解释：** 如果重置门 $r_t$ 接近0，则完全忽略过去的 $h_{t-1}$。
    4.  **隐藏状态更新：**
          * **计算：** $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$
          * **解释：** (第一部分：保留旧状态) + (第二部分：添加新状态)。

  * **应用场景 (LSTM vs GRU)：**

      * GRU的参数更少，计算更快，训练更容易。
      * 在大多数任务上，GRU和LSTM的性能不相上下。
      * **经验法则：** **优先尝试GRU**。如果它效果不够好，或者你有超长序列和海量数据，再换用LSTM。

  * **Keras实现：** `keras.layers.GRU`。用法与 `LSTM` 相同。

<!-- end list -->

```python
model_gru = keras.Sequential([
    # 用法与SimpleRNN/LSTM完全一样
    keras.layers.GRU(32, return_sequences=True, input_shape=[None, n_features]) 
])
```

-----

### 15.4.3 使用一维卷积层处理序列

  * **核心理念：** 虽然RNN很强大，但它们是**顺序**的（必须先计算 $h_t$ 才能计算 $h_{t+1}$），这导致训练缓慢。
  * **设计目的：** 使用 `keras.layers.Conv1D` 来**预处理**序列。
  * **应用场景（1D CNN + RNN）：**
    1.  **缩短序列：** 你可以在RNN层之前放一个 `Conv1D` 层。
    2.  如果 `Conv1D` 使用 `strides=2`（步幅为2），它会将序列长度减半。例如，长度为100的序列输入`Conv1D(strides=2)`后，输出为长度50的序列。
    3.  将这个**更短的序列**喂给LSTM或GRU。
    4.  **好处：** RNN现在只需要处理一个更短的序列，极大地减轻了梯度消失问题，同时训练速度更快。`Conv1D` 层作为“局部模式检测器”，先将原始序列中局部的模式（如3-5个时间步内的）聚合起来。

<!-- end list -->

```python
model_cnn_rnn = keras.Sequential([
    # 输入序列长度100，特征5
    # Conv1D层先在局部（kernel_size=5）提取特征，并用步幅2将序列长度减半
    keras.layers.Conv1D(filters=32, kernel_size=5, strides=2, padding="valid", 
                           input_shape=[100, n_features]),
    keras.layers.GRU(32, return_sequences=True),
    # ...
])
# Conv1D的输出形状约为 [batch, 50, 32]，这个短序列再交给GRU处理
```

-----

### 15.4.4 WaveNet

  * **核心理念：** WaveNet（来自DeepMind）是一种完全**基于卷积**（CNN）的序列处理架构，但它能达到甚至超越RNN的性能，尤其是在音频生成任务上。

  * **设计目的：** 它结合了两种技术，以实现高效的、具有长期记忆的序列处理：

    1.  **因果卷积 (Causal Convolutions)：**

          * **问题：** 常规的CNN在做卷积时会“看到未来”。例如，一个核大小为3的卷积在 $t$ 时刻会看到 $x_{t-1}, x_t, x_{t+1}$。在预测时间序列时，这是作弊。
          * **解决：** 因果卷积通过在输入序列左侧填充（padding）来确保在 $t$ 时刻的输出**只依赖于 $x_t, x_{t-1}, x_{t-2}, ...$**，绝不会看到 $x_{t+1}$。
          * **Keras实现：** `keras.layers.Conv1D(..., padding="causal")`

    2.  **空洞卷积 (Dilated Convolutions / Atrous Convolutions)：**

          * **问题：** 因果卷积的“感受野”（receptive field）随层数线性增长。要看到100步之前的信息，你需要一个100层的网络或一个大小为100的核，这不现实。
          * **解决：** WaveNet堆叠 `Conv1D` 层，并使\*\*空洞率（dilation rate）\*\*按2的幂指数增长。
              * 第1层：`dilation_rate=1` (看 $x_t, x_{t-1}$)
              * 第2层：`dilation_rate=2` (看 $x_t, x_{t-2}$)
              * 第3层：`dilation_rate=4` (看 $x_t, x_{t-4}$)
              * 第4层：`dilation_rate=8` (看 $x_t, x_{t-8}$)
              * ...
          * **好处：** 感受野**呈指数级增长**。只需要 $\log_2(N)$ 层就可以看到 $N$ 步之前的信息。例如，10层网络就可以拥有 $2^{10} \approx 1024$ 的感受野。

  * **应用场景：**

      * **音频生成：** 原始WaveNet的初衷，效果惊人。
      * **时间序列预测：** 作为RNN（LSTM/GRU）的一种**高性能替代方案**。它训练速度更快（因为CNN是可并行的，不像RNN必须顺序计算），并且能捕捉极长的依赖关系。

  * **Keras实现（WaveNet架构）：**

<!-- end list -->

```python
model_wavenet = keras.Sequential()
# 输入形状 [None, n_features] (None代表任意长度)
model_wavenet.add(keras.layers.InputLayer(input_shape=[None, n_features]))

# 堆叠多层，dilation_rate呈指数增长
for rate in (1, 2, 4, 8) * 2: # 堆叠两组 (1,2,4,8)
    model_wavenet.add(
        keras.layers.Conv1D(
            filters=32,
            kernel_size=2, # 核大小通常为2
            padding="causal", # 关键：使用因果填充
            dilation_rate=rate # 关键：使用空洞率
        )
    )

# 最后的输出层
model_wavenet.add(keras.layers.Conv1D(filters=1, kernel_size=1))
```