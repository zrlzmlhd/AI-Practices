# 第11章:训练深度神经网络:
1. 梯度消失,梯度下降,梯度爆炸
2. 对于巨大的网络没有足够的训练集或者做label的代价太高
3. 训练缓慢
4. 在百万个参数下model可能会有验证过拟合训练集的情况 尤其是在没有充足训练实例或者噪声太大的情况

## 11.1 梯度消失和梯度爆炸的问题
1. 梯度消失:在反向传播过程中,梯度越来越小,低层的权重几乎不再更新
2. 梯度爆炸:在反向传播过程中,梯度越来越大,导致权重值变得非常大
3. cost function的平均值如果在0处 训练表现会更好

## 权重初始化技术:
### 11.1.1 Glorot and He Initialization
1. 每一层在输入输出的数据应该含有相同的方差 在反向传播时输入输出的方差也应该一样 但是这要求层含有相同数量的输入和神经元
2. 所以最后的方案是:fan_avg = (fan_in + fan_out) / 2
3. Xavier,Glorot初始化:1.正态分布:均值为0,方差为1/fan_avg,或者在-r到r的均匀分布 r = sqrt(3 / fan_avg)
4. 使用Glorot初始化可以大幅加快深度学习的速度

#### 11.1.1.1每种激活函数的初始化参数
1. Glorot: 激活函数有None,logistic,tanh,softmax 方差为1/fan_avg
2. He: 激活函数为ReLU及其变体, 方差为2/fan_in
3. Lecun: 激活函数为SELU, 方差为1/fan_in


### 11.1.2 非饱和激活函数(RELU)
1. RELU对正值有不饱和 而且计算速度很快
#### 11.1.2.1 RELU
1. RELU对正值有不饱和度 而且训练速度很快
2. RELU缺点是会导致一些神经元的死亡 只输出0值 在某些情况可能一半的神经元都会死去 特别是使用较大的学习率
3. 当神经元的权重进行调整时,输入的加权和对于训练集中的所有实例均为负数,神经元会死亡 只会继续输出0,梯度下降不会再影响它 因为RELU函数的输入为负数时其梯度为0
#### 11.1.2.2 Leaky RELU
1. LEAKEY RELU:max(az,z) 超参数a定义了泄露的程度 他是z<0时的函数的斜率 通常设置为0.01
2. 这个小的斜率确保了神经元不会死亡 而是陷入昏迷 有机会醒来 一般来讲效果要好于严格的RELU函数
3. 甚至设置a=0.2的大泄露比0.01的效果还要好
#### 11.1.2.3 随机Leaky RELU(RReLU)
1. RReLU:在训练时随机选择a的值 在测试时使用平均的a值
2. Leaky ReLU和ReLU相似 但是在负值有一个小的斜率
#### 11.1.2.4 Parametric RELU(PReLU) 参数化ReLU
1. PReLU的a可以在训练中学习 不是超参数 但是可以通过梯度下降来进行修正
2. 在大型图像数据上PReLU效果明显优于ReLU 但是在小数据集上面存在过拟合的风险
#### 11.1.2.5 Exponential Linear Unit(ELU) 指数线性单元
1. 在作者的实验里面胜过了所有的ReLU及其变体 训练时间更短 在测试集上面表现更好
2. 当z<0时 取负值 这让单元的平均输出为0 有助于缓解梯度消失 同时超参数a定义一个值 该值为z为较大负数的时候ELU逼近的值 通产设为1
3. 对于z<0 它具有非零梯度 避免了神经元死亡问题
4. 如果a = 1, 则函数在所有位置都是平滑的 加速了梯度下降 因为他在z=0两处弹跳不大
5. 缺点是:比ReLU的所有系列都要慢 因为使用了指数函数 但是在训练过程更快的收敛速度弥补了这种缓慢地计算 但是测试的时候ELU比ReLU慢
#### 11.1.2.6 Scaled ELU(SELU) 缩放指数线性单元
1. 如果训练层只有密集层 并且所有层都是用SELU激活函数 则网络在训练过程是自归一化的 每层的输出都倾向于保留平均值0和标准差1 解决了梯度消失和爆炸的问题
2. 产生自归一化条件:从输入特征必须是标准化的 平均值为0 方差为1 每个隐藏层权重必须使用LeCun正态初始化 网络架构必须是顺序的 无法用于循环网络还有wideand deep网络

### 如何对隐藏层使用激活函数:
1. 一般来讲 SELU > ELU > LeakyReLU及其变体 >ReLU > Tanh > logistic
2. 如果网络架构不能自归一化 则ELU可能要优于SELU 如果关心运行延迟 则使用Leakey Relu
3. 如果时间足够 使用交叉验证来评估其他激活函数 例如过拟合则为RRelu 如果数据集很大则PRELU 如果速度首位则ReLU 因为优化很好

### 11.1.3 批量归一化:
1. 在每个隐藏层的激活函数之前或者之后添加一个操作 将每个输入零中心化并归一化 然后每层使用两个新的参数向量缩放和偏移其结果
2. 该操作可以使模型学习各层输入的最佳缩放和均值 如果将缩放层添加为神经网络第一层 则无需归一化数据集 BN层会完成这个操作
3. 在每一个批量归一化层训练了四个参数:常规反向传播学习输出缩放向量,输出偏移向量,使用移动指数平均计算的最终输入均值向量和最终输入标准差向量 后者仅在训练后使用
4. 批量归一化的作用像正则化一样减少了对其他正则化技术的依赖 即使使用饱和激活函数 比较大一点的学习率 效果也会很好
5. 但是归一化会增加model复杂度 我们可以将隐藏层的权重修改 让隐藏层直接输出需要通过BN层输出的东西 再次简化复杂度
6. BN作者推荐在激活函数之前添加BN层 但是有争议 可以对比一下两种方案给出最佳
7. 超参数1:momentum:用于计算移动均值和标准差 良好的动量值应该接近1 超参数2:axis默认为-1表示哪个轴应该被归一化

### 11.1.4 梯度裁剪:
1. 梯度裁剪:在反向传播过程中,如果梯度的L2范数大于某个阈值,则将梯度缩小到阈值 最常用于循环神经网络因为RNN难以使用BN
2. 在Keras中实现裁剪就是在创建优化器时设置clipvalue和clipnorm参数 
3. 该优化器将梯度向量每个分量都裁剪到-1到1之间 但是应注意裁剪阈值可能改变梯度方向 例如[0.9,100]按照值裁剪后为[0.9,1] 改变了向量方向
4. 所以一般使用clipnorm=1.0来裁剪梯度向量使其L2范数等于1而不是clipvalue



## 11.2 重用预训练层
1. 迁移技术:避免从头开始训练