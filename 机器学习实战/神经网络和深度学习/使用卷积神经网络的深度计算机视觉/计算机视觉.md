# 14章:计算机视觉:
1. 引入了卷积层和池化层 因为具有全连接层的神经网络对于大量参数表现不佳
## 14.2 卷积层
1. CNN每一层都以2D表示 卷积层的每个神经元只连接到位于上一层的一个小矩阵里面的神经元 
2. 这种架构有助于提取前一个隐藏层的低阶特征然后在下一层组成新的高阶特征
3. 进行卷积的时候要在周围进行零填充 或者通过隔出接收野的方式 两个接收野之间的距离称为步幅
### 14.2.1 滤波器(卷积核):
1. 滤波器是卷积层中神经元的权重图 使用这些权重的神经元将忽略其接收野的除了非零部分的其余权重 因为所有输入都要乘以0
2. 使用相同滤波器的充满神经元的层会输出一个特征图 该图显示图像中最激活滤波器的区域
3. 不需要手动定义滤波器 卷积层将会在训练过程将自动学习对任务最有用的滤波器 而上面的层将他们组合成更加复杂的模式
4. 可以手动定义滤波器的个数 每个滤波器输出一个特征图

### 14.2.2 堆叠多个特征图:
1. 给定的特征图中的所有神经元共享相同的参数(权重和偏置) 这意味着在输入图像中相同位置的相同特征由所有神经元检测
2. 不同的特征图使用不同的参数 
3. 卷积层可以将多个可训练的滤波器应用于输入 这样model可以检测出输入中的任何位置的多个特征
4. CNN一旦学会了在一个位置的识别模式 那么可以在任何位置识别模式
5. 输入图像由多个子层组成 每个颜色通道一个子层 通常为3个红色绿色蓝色 灰色图像只有一个通道 卫星图像除了红绿蓝还有格外光频率

### 14.2.3 TensorFlow实现:
1. 在tensorflow里面每个输入图像通常为[height,width,channels]的3d张量 小批量的表示为[batch,height,width,channels]
2. 卷积层的权重表示为[filter_height,filter_width,input_channels,output_channels]的4d张量 偏置项为[fn]的一维张量
3. padding的作用是在点积发现像素点不够的时候 可以使用0填充 这样可以保留更多原始信息

### 14.2.4 内存需求:
1. 在训练期间CNN对内存的要求很大 因为反向传播要用到在正向传播中计算出的所有中间值

## 14.3 池化层:
1. 池化层目标是对输入图像进行下采样 即缩小 以减少计算量 比如一个2 * 2的池化层作用到1, 5, 3, 2上面 最后只有5传播到下一层 如果是最大池化层的话
2. 除了减少计算量 内存使用量 最大池化层还为小变换引入了一定程度的不变形 变换不变形指的是如果图像的某个部分发生轻微的变换不影响到边界 比如平移 池化层仍然会输出相同的值
3. 最大池化的缺点:比如2*2的最大池化就丢弃了75%的输入值 对于某些应用 不变形是不可取的比如进行语义分割的时候如果输入平移了那么输出也会平移
4. 平均池化层:平均池化层和最大池化层类似 但是不是取最大值而是取平均值 但是基本上最大池化使用较为平凡 因为这会保留更多信息 但是最大池化保存的是最强的特征 而平均池化保存的是特征的平均值
5. 最大池化和平均池化可以沿深度维度执行 这使CNN可以学习特征不变形 即学习多个滤波器 每个滤波器检测相同图像的各种旋转 而且深度最大池化会确保输出是相同的二不考虑旋转
6. keras不包括深度最大池化层 可以使用tensorflow.nn.max_pool函数 并将内核大小和步幅指定为4元组 深度最大池化目前只能在CPU上面运行
7. 全局平均池化层:计算整个特征图的均值, 使用和输入有相同空间维度的池化内核的平均池化层 每个特征图和每个实例只输出一个单值 对数据有破坏 但是适合作为输出层

### TensorFlow实现最大池化层:
1. tf.nn.max_pool(value,ksize,strides,padding,name=None) 深度最大池化目前只能在CPU上面运行

## 14.4 CNN架构:
1. CNN通常由多个卷积层和池化层组成 每个卷积层后面跟着一个或多个池化层 通常卷积层后面会有一个ReLU层
2. 一个常见错误是使用太大的卷积内核 比如使用5*5的卷积内核 在卷积层中通常使用2层3*3的卷积内核效果往往会更好

### 14.4.1 LeNet-5:
1. LeNet-5是第一个卷积神经网络 由Yann LeCun在1998年提出 广泛用于数字手写识别
2. LeNet-5的详细结构和参数 超参数如下:

### 14.4.2 AlexNet:
1. AlexNet在2012年ImageNet竞赛中首次使用CNN获得冠军 和LeNet-5类似 AlexNet也由卷积层和池化层组成 但是更大更深 第一个将卷积层直接堆叠在一起的方法 而不是将池化层堆积在每个卷积层上面
2. AlexNet在C1和C3的ReLU层之后马上进行了归一化操作 成为局部相应归一化,最强激活的神经元会抑制处于相邻特征图的相同地方的其他神经元 这样神经元学到的特征会更加广泛
3. AlexNet架构,参数 超参数详细如下:

### 数据增强:
1. 生成的样本基于原样本的变换 增加数据集的大小 减少过拟合 成为了一种正则化技术 理想状态下应该不能被人为识别 简单的添加白噪声无济于事
2. 比如变换图像大小 方向 水平旋转

### 14.4.3 GoogLeNet:
1. 该网络比以前的网络更深 并且使用了Inception模块 称为盗梦空间模块
2. incepetion模块由4个并行的卷积层和池化层组成 每个卷积层和池化层都有不同的内核大小 每个卷积层和池化层输出一个特征图
3. inception模块带有的1 * 1卷积层目的:1.沿深度维度识别特征 2.充当瓶颈层 输出比输入更少的特征图 降低维度3.每对卷积层就像一个更加强大的卷积层 可以识别更加复杂的模式
4. 每个卷积层的卷积核数是一个超参数 每个inception模块有6超参数
5. googlenet详细架构: 包括9个inception模块 所有卷积层都使用ReLU激活函数
6. 前两层将图像高度宽度除以4 面积除以16 降低计算量 使用较大的内核 保留更多信息 然后局部响应归一化层 然后两个卷积层 第一个卷积层像瓶颈层 然后局部响应归一化
7. 然后最大池化层将图像的高度宽度缩小一般 整个面积减少四分之一 然后加快训练速度
8. 然后重叠9个inception模块来减少维度 加快网络训练速度
9. 然后进行全局平均池化 输出每个特征图的均值 丢弃剩余的信息 最后进行dropout然后连接到全连接层 以softmax函数作为激活函数
10. 原始的Googlenet还有两个辅助分类器 插入到第三个和第六个inception模块的底部 它们都由一个平均池化层,一个卷积层,两个全连接层, 一个softmax激活 
11. 在训练期间他们的损失被添加到总损失中 为了解决梯度消失的问题 但是实际上没有什么用

### 14.4.4 VGGNet:
1. VGGNet由牛津大学视觉几何组开发 在ILSVRC 2014中获得了第二名
2. 详细架构为:

### 14.4.5 ResNet(残差网络):

