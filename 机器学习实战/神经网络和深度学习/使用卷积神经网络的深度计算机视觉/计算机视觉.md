# 14章:计算机视觉:
1. 引入了卷积层和池化层 因为具有全连接层的神经网络对于大量参数表现不佳
## 14.2 卷积层
1. CNN每一层都以2D表示 卷积层的每个神经元只连接到位于上一层的一个小矩阵里面的神经元 
2. 这种架构有助于提取前一个隐藏层的低阶特征然后在下一层组成新的高阶特征
3. 进行卷积的时候要在周围进行零填充 或者通过隔出接收野的方式 两个接收野之间的距离称为步幅
### 14.2.1 滤波器(卷积核):
1. 滤波器是卷积层中神经元的权重图 使用这些权重的神经元将忽略其接收野的除了非零部分的其余权重 因为所有输入都要乘以0
2. 使用相同滤波器的充满神经元的层会输出一个特征图 该图显示图像中最激活滤波器的区域
3. 不需要手动定义滤波器 卷积层将会在训练过程将自动学习对任务最有用的滤波器 而上面的层将他们组合成更加复杂的模式
4. 可以手动定义滤波器的个数 每个滤波器输出一个特征图

### 14.2.2 堆叠多个特征图:
1. 给定的特征图中的所有神经元共享相同的参数(权重和偏置) 这意味着在输入图像中相同位置的相同特征由所有神经元检测
2. 不同的特征图使用不同的参数 
3. 卷积层可以将多个可训练的滤波器应用于输入 这样model可以检测出输入中的任何位置的多个特征
4. CNN一旦学会了在一个位置的识别模式 那么可以在任何位置识别模式
5. 输入图像由多个子层组成 每个颜色通道一个子层 通常为3个红色绿色蓝色 灰色图像只有一个通道 卫星图像除了红绿蓝还有格外光频率

### 14.2.3 TensorFlow实现:
1. 在tensorflow里面每个输入图像通常为[height,width,channels]的3d张量 小批量的表示为[batch,height,width,channels]
2. 卷积层的权重表示为[filter_height,filter_width,input_channels,output_channels]的4d张量 偏置项为[fn]的一维张量
3. padding的作用是在点积发现像素点不够的时候 可以使用0填充 这样可以保留更多原始信息

### 14.2.4 内存需求:
1. 在训练期间CNN对内存的要求很大 因为反向传播要用到在正向传播中计算出的所有中间值

## 14.3 池化层:
1. 池化层目标是对输入图像进行下采样 即缩小 以减少计算量 比如一个2 * 2的池化层作用到1, 5, 3, 2上面 最后只有5传播到下一层 如果是最大池化层的话
2. 除了减少计算量 内存使用量 最大池化层还为小变换引入了一定程度的不变形 变换不变形指的是如果图像的某个部分发生轻微的变换不影响到边界 比如平移 池化层仍然会输出相同的值
3. 最大池化的缺点:比如2*2的最大池化就丢弃了75%的输入值 对于某些应用 不变形是不可取的比如进行语义分割的时候如果输入平移了那么输出也会平移
4. 平均池化层:平均池化层和最大池化层类似 但是不是取最大值而是取平均值 但是基本上最大池化使用较为平凡 因为这会保留更多信息 但是最大池化保存的是最强的特征 而平均池化保存的是特征的平均值
5. 最大池化和平均池化可以沿深度维度执行 这使CNN可以学习特征不变形 即学习多个滤波器 每个滤波器检测相同图像的各种旋转 而且深度最大池化会确保输出是相同的二不考虑旋转
6. keras不包括深度最大池化层 可以使用tensorflow.nn.max_pool函数 并将内核大小和步幅指定为4元组 深度最大池化目前只能在CPU上面运行
7. 全局平均池化层:计算整个特征图的均值, 使用和输入有相同空间维度的池化内核的平均池化层 每个特征图和每个实例只输出一个单值 对数据有破坏 但是适合作为输出层

### TensorFlow实现最大池化层:
1. tf.nn.max_pool(value,ksize,strides,padding,name=None) 深度最大池化目前只能在CPU上面运行

## 14.4 CNN架构:
1. CNN通常由多个卷积层和池化层组成 每个卷积层后面跟着一个或多个池化层 通常卷积层后面会有一个ReLU层
2. 一个常见错误是使用太大的卷积内核 比如使用5*5的卷积内核 在卷积层中通常使用2层3*3的卷积内核效果往往会更好

### 14.4.1 LeNet-5:
1. LeNet-5是第一个卷积神经网络 由Yann LeCun在1998年提出 广泛用于数字手写识别
2. LeNet-5的详细结构和参数 超参数如下:

### 14.4.2 AlexNet:
1. AlexNet在2012年ImageNet竞赛中首次使用CNN获得冠军 和LeNet-5类似 AlexNet也由卷积层和池化层组成 但是更大更深 第一个将卷积层直接堆叠在一起的方法 而不是将池化层堆积在每个卷积层上面
2. AlexNet在C1和C3的ReLU层之后马上进行了归一化操作 成为局部相应归一化,最强激活的神经元会抑制处于相邻特征图的相同地方的其他神经元 这样神经元学到的特征会更加广泛
3. AlexNet架构,参数 超参数详细如下:

### 数据增强:
1. 生成的样本基于原样本的变换 增加数据集的大小 减少过拟合 成为了一种正则化技术 理想状态下应该不能被人为识别 简单的添加白噪声无济于事
2. 比如变换图像大小 方向 水平旋转

### 14.4.3 GoogLeNet:
1. 该网络比以前的网络更深 并且使用了Inception模块 称为盗梦空间模块
2. incepetion模块由4个并行的卷积层和池化层组成 每个卷积层和池化层都有不同的内核大小 每个卷积层和池化层输出一个特征图
3. inception模块带有的1 * 1卷积层目的:1.沿深度维度识别特征 2.充当瓶颈层 输出比输入更少的特征图 降低维度3.每对卷积层就像一个更加强大的卷积层 可以识别更加复杂的模式
4. 每个卷积层的卷积核数是一个超参数 每个inception模块有6超参数
5. googlenet详细架构: 包括9个inception模块 所有卷积层都使用ReLU激活函数
6. 前两层将图像高度宽度除以4 面积除以16 降低计算量 使用较大的内核 保留更多信息 然后局部响应归一化层 然后两个卷积层 第一个卷积层像瓶颈层 然后局部响应归一化
7. 然后最大池化层将图像的高度宽度缩小一般 整个面积减少四分之一 然后加快训练速度
8. 然后重叠9个inception模块来减少维度 加快网络训练速度
9. 然后进行全局平均池化 输出每个特征图的均值 丢弃剩余的信息 最后进行dropout然后连接到全连接层 以softmax函数作为激活函数
10. 原始的Googlenet还有两个辅助分类器 插入到第三个和第六个inception模块的底部 它们都由一个平均池化层,一个卷积层,两个全连接层, 一个softmax激活 
11. 在训练期间他们的损失被添加到总损失中 为了解决梯度消失的问题 但是实际上没有什么用

### 14.4.4 VGGNet:
1. VGGNet由牛津大学视觉几何组开发 在ILSVRC 2014中获得了第二名
2. 详细架构为:

### 14.4.5 ResNet(残差网络):
1. 它证明了一个趋势:当模型变得越来越深 参数越来越少 能够训练这种深层网络的关键是使用跳过连接 也成为快捷链接
2. 将输入添加到网络的输出 进行残差学习 借助跳过连接 信号可以轻松地在整个神经网络中传播 深度残差网络可以看做残差单元的堆栈 每个残差单元都是具有跳过连接的小型神经网络
3. Google的Inception-v4结合了GoogleNet和ResNet的架构
4. Resnet具体架构如下:

### 14.4.6 Xception:
1. Xception是Inception模块的变体 它使用深度可分离卷积代替Inception模块中的卷积层
2. 可分离卷积层由两部分组成:1.第一部分为每个输入特征图应用一个空间滤波器, 2.第二部分专门寻找跨通道模式 它是具有1*1 滤波器的常规卷积层
3. 由于可分离卷积层每个输入通道只有一个空间滤波器 所以应该避免在通道数量太少的层 比如输入层后面使用
4. 与常规卷积层相比 分离卷积层参数更少 占用内存更少 计算更少 表现更好 除了在通道数较少的层之后以外应该默认使用可分离卷积
5. Xception架构如下:

### 14.4.7 SENet:
1. SENet的全称是Squeeze-and-Excitation Network 它是2017年提出的一种新的CNN架构 它引入了一种新的模块称为SE模块
2. inception和Resnet的扩展版本分别称为SE-inception和SE-resnet 而SENEt在原始架构中的每个单元添加了一个称为SE的小型神经网络
3. SE模块由全局平均池化层+RELU+sigmoid函数组成的密集输出层 专注于分析深度维度 了解哪一些特征通常最活跃 并重新校准特征图
4. 比如SE如果了解到眼睛 嘴巴 鼻子应该同时出现 而现在眼睛嘴巴权重很活跃 而鼻子不活跃 那么他就会增强鼻子特征图 也就是说它的作用是校准特征图
5. SE模块的架构和实现原理如下:

## 14.5 使用Keras实现ResNet-34 CNN:

## 14.6 使用Keras的预训练模型:
1. Keras的内置包keras.applications可以创建很多训练好的模型

## 14.7 迁移学习的预训练模型:
1. 想构建图像分类器但是没有足够数据就可以重用预训练模型较低层
2. 在进行迁移学习的时候在开始训练时冻结预训练层的权重通常是一个好主意
3. 在对模型进行几个批次的训练以后可以解冻所有层 或者只解冻顶层 并继续训练

## 14.8 分类和定位：
1. 定位图片中的物体可以看作是回归任务：预测物体周围的边界框
2. 但是这样意味着要对数据进行label标记 这很昂贵 
3. 对生成的数据边界框应该进行归一化 以便坐标 高度宽度都在0-1范围之内
4. 成本函数通常使用MSE 但是评估模型这并不是一个较好的指标 评估模型通常使用交并比 预测边界框和目标边界框之间的重叠面积除以他们的联合面积

## 14.9 物体检测：
1. 物体检测是指对多个物体进行分类和定位的任务
2. 滑动窗口方法：采用训练的CNN来对单个物体进行定位和分类 但是滑动窗口多次检测同一物体 为了消除不必要的边界框 要使用非极大值抑制法
3. 非极大值抑制：在CNN里面添加一个置信度分数 对边界框进行排序，然后选择得分最高的边界框，然后移除与该边界框的IoU大于某个阈值的边界框
4. 滑动窗口方法缺点：计算量大 速度慢
### 14.9.1 全卷积网络：
1. 全卷积网络：将CNN的最后一个全连接层替换为卷积层，然后使用全局平均池化层，这样就可以将任何大小的输入图像转换为固定大小的输出特征图
2. 要将全连接层替换成卷积层， 卷积层中的滤波器数量必须等于密集层中的单元数量 滤波器大小必须等于输入特征图的大小 必须使用valid填充 步幅可以设置为1或者更大
3. 全连接的网络架构和实现原理如下：

### 14.9.2 YOLO算法：
1. YOLO算法：将输入图像划分为S*S的网格，每个网格预测B个边界框和这些边界框的置信度分数，置信度分数表示边界框中包含物体的概率，以及边界框的修正值
2. 它为每个网格单元输出5个边界框 YOLOv3不再预测边界框的中心的绝对坐标 而是预测边界框相对于网格单元的偏移量
3. 在物体检测任务里面常见的测试指标是均值平均精度（mAP）
4. mAP的实现原理是：
5. 除了YOLO以外的其他流行架构：
6. YOLOv3的架构和实现原理如下：

## 14.10： 语义分割：
1. 语义分割：将图像划分为多个区域，每个区域包含相同类型的物体 每个元素根据所属的物体的类别进行分类
2. 率先提出的方法是使用预先训练的CNN转化为FCN 但是CNN对输出图像应用的总步幅为32 这导致图像太过粗糙 所以添加了一个单独的上采样层来将分辨率*32
3. 解决上采样的方法：双线性插值，转置的卷积层 
4. 转置的卷积层：首先插入空的行和列来拉伸图像 然后执行常规的卷积，可以对转置进行初始化来进行更接近于线性插值的操作
5. 可以使用tf.keras.Conv2DTranspose来实现转置的卷积层
6. 作者还进行了添加来自较低层的跳过连接 将输出图像上采样2倍 然后对结果进行16倍采样得到32倍
7. 或者放大2倍，添加较低层，放大2倍添加更底层的输出，然后放大8倍 甚至扩大到超出原始图像的大小 称为超分辨率技术
8. Github上面提供了很多Tensorflow语义分割实现 对每一个物体进行分割而不是将同一类归为一个大类