# 机器学习实战第四章笔记
## 4.1 线性回归
### 可以直接通过标准方程计算线性回归方程的最佳Theta值
### LinearRegression 是基于np.linalg.lstsq函数实现的 原理是最小二乘法
### np.linalg.lstsq(X_b, y, rcond=None) 返回值是theta_best_svd, residuals, rank, s
### 也可以通过奇异值分解找得到伪逆

## 复杂度
### 通过求x的转置乘以x这种方法求逆的计算复杂度是n^2.4到n^3 也就是说当特征数量翻倍 训练时间将提升到5到8倍
### sklearn的LinearRegression复杂度是n**2 当特征数量翻倍 训练时间将提升到4倍

## 4.2 梯度下降
### 学习步长与成本函数的斜率成正比 当参数接近于最小值的时候 补偿逐渐变小
### 应用梯度下降的时候要保证所有特征值的大小比例要相同 比如使用StandardScaler标准化特征值

### 随机梯度下降不会停止 但最后的参数是比较好的 而不是最优的
### 随机下降的实例随机选取下某一个实例可能被多次选择 可以对数据集进行混洗 然后逐个实例遍历 然后再次混洗 但是这种方法收敛速度比较慢

## 4.3 多项式回归
### 多项式回归能找到数据特征之间的关系 还可以将特征的所有组合添加到给定的多项式阶数例a,b两个特征 degree=3 会出现 a**3, a**2*b, a*b**2, b**3
### degree=d 表示一共有(n+d)!/(n!d!)个特征 小心特征组合的数量爆炸

## 4.4 学习曲线
### 学习曲线帮助判断是过拟合还是欠拟合 同样的作用还有交叉验证的指标
### 增加模型复杂度会显著提升模型的方差 降低模型复杂度提升模型偏差 降低方差

## 4.5 正则化线性模型
### 4.5.1 岭回归(Tikhonov 正则化):
1. 岭回归成本函数:J(θ) = MSE(theta) + a * 1/2*(theta_1**2 + ... + theta_n**2) 
2. a控制正则化的程度 0表示线性回归 a越大 所有权重都趋近于0 结果是一条经过数据均值的平线 a越大模型越平坦
3. 偏置项θ0并不进行正则化 
4. 在执行林回归之前要进行数据缩放 大多数正则化模型都需要缩放数据
5. 岭回归的闭式解方程是:θ=( X^T * X + a * I )^-1 * X^T * y

### 4.5.2 Lasso回归(最小绝对收缩 选择算子回归):
1. 添加的是权重向量的l1范数 而不是l2范数的一半
2. Lasso回归倾向于消除最不重要特征的权重 自动执行特征选择输出一个稀疏矩阵
3. 当特征数量大于训练实例数量的时候Lasso表现可能不稳定 或者有几个特征高强度相关的时候

### 4.5.3 弹性网络(Elastic Net):
1. 弹性网络是Lasso回归和岭回归的混合体
2. 弹性网络成本函数:J(θ) = MSE(theta) + r * a * 1/2 * (l1_ratio * theta_1**2 + (1 - l1_ratio) * theta_2**2) + (1 -r)a * 1/2 * (theta_1**2 + ... + theta_n**2)
3. 当r等于0就是岭回归 当r等于1就是Lasso回归
4. 当你觉得有用的特征数量只有几个的话 可以使用Lasso或者Elastic Net

### 4.5.4 提前停止:
1. 提前停止是一种正则化方法当模型在验证集上的表现不再提高的时候停止训练以便防止model过拟合