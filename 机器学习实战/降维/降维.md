# 降维
1. 降维的主要作用是加速训练 或者在某些情况下去掉不必要的噪声
2. 主流方法有:PCA, Kernel PCA, LLE
3. 降维有助于将数据可视化

## 8.1 降维的诅咒
1. 高维的数据表现将会于低纬度的数据表现差异很大 高维数据有很大可能下是非常稀疏的
2. 数据的维度越高 过拟合的风险就越大
3. 理论上来讲 通过增加训练集可以解决维数特征过大导致的稀疏问题 但是在现实中需要增加的训练实例数量是指数性上升的

## 8.2 降维的主要方法
### 8.2.1 投影
1. 将高维数据投影到低维 比如一个三维空间中的点 如果他们的分布离某一个平面特别近的话 可以将这个三维平面的点投影到这个二维平面
2. 缺点是子空间可能发生扭动或者扭曲 转动 不同维度的特征可能会重叠在一起
### 8.2.2 流形学习
1. 流形学习是一种试图保持高维数据内在结构的方法
2. 但是流行学习处理后的数据集虽然维数降低了 但是训练集的结构不一定变得更加可分 即这取决于数据集

## 8.3 PCA
1. 识别最靠近数据的超平面 将数据投影在上面
### 8.3.1 保留差异性
1. 保留差异性最大的轴 因为他可能比其他两种投影丢失的信息比较少
### 8.3.2 主要成分
1. 主成分分析可以在训练集中识别出哪条轴对差异性的贡献度最高
2. 第i个轴成为数据的第i个主要成分(principal component)
3. 如何找到主成分:奇异值分解(SVD)矩阵分解技术 将训练集矩阵分解为三个矩阵的乘法UEVT V包含了所有主要成分的单位向量
4. PCA假定数据是以原点为中心 Scikit-learn的PCA类默认会将数据平移到原点
### 8.3.3 向下投影到d维度
1. 将数据集投影到前D个主要成分定义的超平面上 从而将数据集的维度降低为d维 选择这个超平面可以尽最大可能保留数据结构差异性
2. 投影方法: 计算数据集矩阵X和包含所有主要成分的单位向量矩阵V的前d列的矩阵的乘积

### 8.3.5 可解释方差比
1. 可以使用explained_variance_ratio_属性查看每个主成分解释了多少差异性 即每个主成分上面包含了数据集方差的百分之几

### 8.3.6 选择正确的维度
1. 与其选择要减少到的维度 不如选择相加足够大的方差部分(比如90%)的维度
2. 使用PCA类的n_components属性来指定要保留的主成分比例

### 8.3.7 使用PCA进行降维
1. 降维可以有效的降低数据的维度 进而降低数据集占用的空间
2. 应用PCA投影的逆变换可以将数据解压缩到784维 但是由于压缩会丢失一些信息 重构的数据无法和原始数据一一对应 5%的方差会被丢弃

### 8.3.8 随机PCA
1. 将超参数svd_solver设置为randomized 则可以使用RandomizedPCA
2. 这种算法可以快速找到前d个主成分的近似值 当d远小于n时 效果非常好 n 通常可以表示原始数据的特征维度数 d是PCA 降维后选择保留的主成分数量
3. 默认参数是auto 当m或者n大于500并且d小于m或n的80%自动启用 如果强制要求使用完整的svd算法 则需要将参数设置为full
### 8.3.9 增量PCA
1. 当数据集太大以至于无法一次性载入内存时 可以使用IncrementalPCA类 将数据集划分成多个小批量 一次讲一个小批量训练 在线训练很有用

## 8.4 内核PCA
1. 核PCA是一种使用核技巧将数据映射到高维空间 然后在这个空间中使用支持向量机来分类
2. 核PCA和标准PCA的主要区别在于 核PCA不限制映射到高维空间的方法 核PCA可以映射到任意维度的空间
3. KernelPCA是一种无监督学习 没有什么比较好的性能指标 所以一般使用网格搜索来选择最佳内核以及超参数
4. 选择产生最低重构误差的内核和超参数
5. 对一个给定的实例在缩小的空间中翻转线性PCA 重构点将位于特征空间而不是原始空间 由于特征空间是无线维 所以无法计算真实的重构误差
6. 可以找到一个点映射到重建点附近进而测量其与原始实例的平方距离
7. 执行重构:将投影作为训练集 原始数据作为目标值

## 8.5 LLE(局部线性嵌入)
1. LLE是一种非常强大的非线性降维技术 它可以保持高维数据集的局部结构
2. 首先测量训练实例如何与最近的邻居相关 然后寻找可以最好的保留这些局部关系的训练集的低维表示形式
3. 核心思想：LLE是一种流形学习算法，其核心假设是高维数据分布在一个低维的非线性流形上。它的目标是在低维空间中“摊平”这个流形。
4. 关键假设：它假设每个数据点可以由其局部邻域内的点线性重构出来。
5. 邻域保持：LLE的核心在于保持局部邻域的线性关系。它认为在高维空间中成立的“重构权重”（即“拼凑配方”），在降维到低维空间后应该保持不变。
6. 算法步骤（三步）：1. 找邻居：为每个数据点 $x_i$ 找到其 $K$ 个最近邻。2. 算权重 (W)：固定 $x_i$ 和邻居，计算出用于重构 $x_i$ 的最佳线性权重 $W_{ij}$（即 $\min \sum_i \| x_i - \sum_j W_{ij} x_j \|^2$），并要求权重和为1（$\sum_j W_{ij} = 1$）。3. 算低维坐标 (Y)：固定权重 $W$，寻找低维坐标 $y_i$，使得在低维空间中，这种重构关系误差最小（即 $\min \sum_i \| y_i - \sum_j W_{ij} y_j \|^2$）。
7. 权重特性：权重 $W$ 具有平移、旋转和缩放不变性。这意味着它只编码了局部的几何结构，而不依赖于坐标系的绝对位置。适用场景：特别擅长处理“卷曲”但本身不“闭环”的数据结构（如瑞士卷）。
8. 主要参数：邻居数量 $K$。 $K$ 的选择对结果影响很大：太小可能导致流形断裂，太大则会“过度平滑”，失去局部特性（趋近于PCA）。

## 8.6 其他降维技术
1. 随机投影
2. 多维缩放
3. ISOmap
4. t分布随机近邻嵌入
5. 线性判别分析