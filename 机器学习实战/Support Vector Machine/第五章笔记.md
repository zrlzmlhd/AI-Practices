# 支持向量机:
1. 在类之间拟合可能的最宽的街道 也叫大间隔分类 在街道外添加的数据不会对决策边界产生影响
2. SVM对数据缩放十分敏感 缩放后的数据更能体现出决策边界
## 5.1 线性SVM分类
### 软间隔分类
1. 软间隔分类允许一些点出现在间隔之外 而硬间隔分类则要求数据是高度可分的 否则很容易被离群值带偏划分方向
2. 使用sklearn创建SVM分类器时 可以通过参数C来控制对误分类的惩罚程度 C越小 对误分类的惩罚程度越小 一般来讲C越大在训练数据上比较好 但是泛化能力较差
3. 使用常规的随机梯度下降来训练SVM分类器收敛速度不如LinearSVC类 但是处理在线分类任务或者不适合内存的庞大数据集很有作用
4. LinearSVC类会对偏置项进行正则化 所以要先减去平均值 让训练数据居中 使用StandardScaler则会自动进行这一步
5. 确保loss函数使用的是hinge 
6. 为了更好的性能 将dual设置为False 除非特征数量比训练样本还多

## 5.2 非线性SVM分类
1. 主要是为了处理不可分数据集 添加一些特征之后数据会变得可分
### 5.2.1 多项式内核
1. 添加多项式特征的问题:太少处理不了负责数据集 阶数太高导致创造太多特征模型变得太慢
2. 欠拟合提高kernel的阶数 过拟合就降低kernel的阶数
### 5.2.2 相似特征
1. 选择一个数据地标 基于选择的地标由高斯RBF函数重新计算特征值 以达到让不可分数据变得线性可分
2. 选择地标的方式: 在数据集每一个样本位置创建一个地标 这会创建出很多维度 同时增加了线性可分的机会 
3. 缺点: 一个又m个样本点的数据集会转化成一个m个实例m个特征的数据集(假设抛弃了原始特征) 导致特征数量巨大
### 5.2.3 高斯RBF内核
1. gamma(y)增加会使钟型曲线变得更窄 每个实例的影响范围变小 决策边界变得不规则 围绕单个边界绕圈 过拟合就降低gamma 欠拟合就提高
2. 特定数据集或者任务使用特定的kernel 函数 比如针对文本文档和DNA序列使用字符串核
### 核函数的选择
1. 先从线性核函数开始尝试(LinearSVC速度 >> SVC(kernel= 'Linear)) 特别是训练数据比较庞大
2. 数据集不算庞大可以试一下高斯RBF核 或者网格搜索
### 5.2.4 计算复杂度
1. LinearSVC基于liblinear实现 不支持核技巧 该算法与训练实例数量和特征数量几乎线性相关 时间复杂度O(m * n) 由容差超参数控制精度 一般默认够用 sklearn里面是tol控制
2. SVC基于libsvm实现 支持核技巧 时间复杂度O(m**3 * n) 不适用于训练实例数量巨大的任务 适合复杂但是中小型数据集 model

## 5.3 SVM回归
1. LinearSVR和SVR区别和分类的区别复杂度一样

## 5.4 工作原理(没看完 涉及到优化理论 线性规划 高等代数的东西)
### 5.4.1 决策函数和预测
1. 线性SVM分类器简单计算w.T * x + b 然后判断符号 结果为正就是1, 为负就是-1
2. 一般来讲当存在n个特征的时候 决策函数为n维超平面 决策边界为n-1维超平面
3. 决策函数的斜率等于权重向量的范数