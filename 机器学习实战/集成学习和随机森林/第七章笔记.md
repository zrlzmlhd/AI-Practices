# 集成学习和随机森林
## 7.1 投票分类器
1. 数量足够多 弱分类器在进行组合的时候会成为强分类器
2. 大数定理决定了即使每个独立分类器的概率表现只有51%的准确率 ,只要数量足够多,最终结果的准确率也可以达到75%
3. 如果每个Model可以给出predict_proba()方法,采用每个分类器的概率平均值 选出最高model 这种方法优于硬分类

## 7.2 bagging and pasting
1. bagging : 有放回的抽样 在不同的训练集随机子集上面进行训练 
2. pasting : 无放回的抽样 在不同的训练集随机自己上面进行训练
### 7.2.1 Scikit-learn中的bagging and pasting
1. BaggingClassifier : bootstrap=True为bagging bootstrap=False为pasting
2. 如果基本分类器可以predict_proba()方法,则BaggingClassifier会自动使用投票分类器,否则使用硬分类器
3. 通常来讲Bagging效果比Pasting要好一点 方差更小
### 7.2.2 Out-of-Bag Evaluation(包外评估)
1. BaggingClassifier中,bootstrap=True时,每个分类器都会使用大约63.2%的数据进行训练,剩余的36.8%的数据用于评估分类器的表现

## 7.3 随机补丁和随机子空间
### 7.3.1 对数据实例和特征都进行抽样
1. BaggingClassifier支持对特征进行采样 max_features参数可以指定采样特征的数量,如果max_features=1,则每个分类器都是随机选择一个特征进行训练
2. bootstrap=True时,每个分类器都会使用大约63.2%的特征进行训练,剩余的36.8%的特征用于评估分类器的表现
3. 这种采样方法对于高维数据 比如图像很有效
4. 样本和特征均充足，或需要更强的随机性来提升集成多样性时
### 7.3.2 随机子空间法
1. 保留所有训练实例:boostrap= False and max_samples= 1.0
2. 对特征进行抽样: boostrap_features = True and max_features= 1.0
3. 特征维度高但样本量有限，或希望保留更多样本信息时

## 7.4 随机森林
1. 如果不想对决策树之外的东西进行装袋 则可以直接使用RandomForestClassifier 否则使用BaggingClassifier(DecisionTreeClassifier())
### 7.4.1 极端随机树
1. 随机森林的单棵树在生长的时候每一个节点在分裂的时候仅考虑到了一个随机子集所包含的特征 如果对每个特征使用随机阈值 而不是搜索得到的最佳阈值 则称为极端随机树
2. Extremely Randomized Trees(极端随机树)实际上是提高了偏差 降低了方差
3. 使用sklearn.ExtraTreesClassifier() 可以使用极端随机树
4. 极端树的训练时间比常规树快得多 因为每个节点分裂的时候不需要搜索最佳阈值
5. 实际上极端树和普通树没有谁好谁坏的严格之分 无法预先知道 只能通过两个都尝试一遍然后进行网格搜索
### 7.4.2 特征重要性
1. 可以使用feature_importances_属性查看特征的重要性 快速了解一个数据集里面哪些数据特征是十分重要的
2. scikit-learn通过查看使用该特征的树节点平均减少不纯度的程度来衡量特征的重要性 也就是说那个特征用来分类产生的gini指数下降最多 则该特征的重要性越高 或者说信息增益

## 7.5 提升法
### 7.5.1 AdaBoost
1. 首先训练一个基础分类器 然后计算每个训练实例的权重 然后训练第二个分类器,第二个分类器会根据第一个分类器的表现来调整每个训练实例的权重
2. 一旦全部预测器训练完成 集成整体做出预测就和bagging和pasting一样了 除非预测器有不同的权重 因为他们的总的准确率是基于加权后的数据集
3. 缺陷是model无法并行训练
4. scikit-learn使用的是Adaboost的一个多分类版本:SAMME 当只有两类的时候 等同于AdaBoost
5. 如果预测器可以估算类概率 即有predict_proba()方法 则使用SAMME.R 通常表现会更好
### 7.5.2 Gradient Boosting(梯度提升)
1. 让新分类器基于上一个分类器的残差进行拟合
2. learning_rate表示对每棵树的贡献进行缩放 设置为低值则需要更多数据来拟合训练集 但是预测的泛化效果通常比较好
3. 可以使用staged_predict()提前停止训练来找到最佳树的数量
4. 也可以使用warm_start参数来使用之前训练好的分类器作为初始分类器

### 7.5.3 Random Gradients Boosting
1. 利用GradientsBoostingRegression 的subsample来指定训练每颗树的实例的比例 若等于0.2 则使用0.2的随机选择的实例进行训练
2. 实际上这也是用更高的偏差换取了更低的方差 同时加速了训练过程

### 7.5.4 XGBoost
1. XGBoost是Gradient Boosting的一个实现,但是它对梯度提升算法进行了许多改进
2. 对很多MachineLearning竞赛来说比较好用

## 7.6 堆叠法
1. 堆叠法将多个分类器的预测结果作为新的特征输入到另一个分类器中
2. 使用Deslib开源库实现Model的堆叠