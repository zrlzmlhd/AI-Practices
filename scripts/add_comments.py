#!/usr/bin/env python3
"""
Notebookæ³¨é‡Šå¢å¼ºè„šæœ¬
ä¸ºnotebooksæ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œæ–‡æ¡£
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple

# ============================================================
# æ³¨é‡Šæ¨¡æ¿åº“
# ============================================================

COMMENT_TEMPLATES = {
    # å¯¼å…¥ç›¸å…³
    'import numpy': '# NumPy: ç”¨äºæ•°å€¼è®¡ç®—å’Œæ•°ç»„æ“ä½œ',
    'import pandas': '# Pandas: ç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æ',
    'import matplotlib': '# Matplotlib: ç”¨äºæ•°æ®å¯è§†åŒ–',
    'import seaborn': '# Seaborn: åŸºäºMatplotlibçš„é«˜çº§å¯è§†åŒ–åº“',
    'import tensorflow': '# TensorFlow: æ·±åº¦å­¦ä¹ æ¡†æ¶',
    'import keras': '# Keras: é«˜çº§ç¥ç»ç½‘ç»œAPI',
    'from sklearn': '# Scikit-learn: æœºå™¨å­¦ä¹ åº“',
    'import torch': '# PyTorch: æ·±åº¦å­¦ä¹ æ¡†æ¶',

    # æ•°æ®å¤„ç†
    'train_test_split': '# å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†',
    'StandardScaler': '# æ ‡å‡†åŒ–å¤„ç†ï¼šå°†ç‰¹å¾ç¼©æ”¾åˆ°å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1',
    'MinMaxScaler': '# å½’ä¸€åŒ–å¤„ç†ï¼šå°†ç‰¹å¾ç¼©æ”¾åˆ°[0, 1]èŒƒå›´',
    'LabelEncoder': '# æ ‡ç­¾ç¼–ç ï¼šå°†åˆ†ç±»æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼',
    'OneHotEncoder': '# ç‹¬çƒ­ç¼–ç ï¼šå°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡',

    # æ¨¡å‹ç›¸å…³
    'LinearRegression': '# çº¿æ€§å›å½’æ¨¡å‹',
    'LogisticRegression': '# é€»è¾‘å›å½’æ¨¡å‹ï¼ˆç”¨äºåˆ†ç±»ï¼‰',
    'DecisionTreeClassifier': '# å†³ç­–æ ‘åˆ†ç±»å™¨',
    'RandomForestClassifier': '# éšæœºæ£®æ—åˆ†ç±»å™¨',
    'SVC': '# æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨',
    'KMeans': '# K-Meansèšç±»ç®—æ³•',
    'PCA': '# ä¸»æˆåˆ†åˆ†æï¼ˆé™ç»´ï¼‰',

    # æ·±åº¦å­¦ä¹ 
    'Sequential': '# é¡ºåºæ¨¡å‹ï¼šå±‚çš„çº¿æ€§å †å ',
    'Dense': '# å…¨è¿æ¥å±‚',
    'Conv2D': '# äºŒç»´å·ç§¯å±‚',
    'MaxPooling2D': '# äºŒç»´æœ€å¤§æ± åŒ–å±‚',
    'Dropout': '# Dropoutå±‚ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ',
    'BatchNormalization': '# æ‰¹æ ‡å‡†åŒ–å±‚ï¼šåŠ é€Ÿè®­ç»ƒï¼Œç¨³å®šæ¢¯åº¦',
    'LSTM': '# é•¿çŸ­æœŸè®°å¿†ç½‘ç»œå±‚',
    'GRU': '# é—¨æ§å¾ªç¯å•å…ƒå±‚',
    'Embedding': '# åµŒå…¥å±‚ï¼šå°†æ•´æ•°ç´¢å¼•è½¬æ¢ä¸ºç¨ å¯†å‘é‡',

    # è®­ç»ƒç›¸å…³
    '.fit(': '# è®­ç»ƒæ¨¡å‹',
    '.predict(': '# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹',
    '.evaluate(': '# è¯„ä¼°æ¨¡å‹æ€§èƒ½',
    '.compile(': '# ç¼–è¯‘æ¨¡å‹ï¼šé…ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè¯„ä¼°æŒ‡æ ‡',

    # è¯„ä¼°æŒ‡æ ‡
    'accuracy_score': '# è®¡ç®—å‡†ç¡®ç‡',
    'precision_score': '# è®¡ç®—ç²¾ç¡®ç‡',
    'recall_score': '# è®¡ç®—å¬å›ç‡',
    'f1_score': '# è®¡ç®—F1åˆ†æ•°',
    'confusion_matrix': '# æ··æ·†çŸ©é˜µ',
    'classification_report': '# åˆ†ç±»æŠ¥å‘Š',
    'mean_squared_error': '# å‡æ–¹è¯¯å·®',
    'r2_score': '# RÂ²å†³å®šç³»æ•°',
}

# ä»£ç å—è¯´æ˜æ¨¡æ¿
CODE_BLOCK_EXPLANATIONS = {
    'np.random.seed': '''
# ============================================================
# è®¾ç½®éšæœºç§å­
# ä½œç”¨ï¼šç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶äº§ç”Ÿç›¸åŒçš„éšæœºæ•°ï¼Œä¿è¯ç»“æœå¯é‡å¤
# ============================================================''',

    'plt.figure': '''
# ============================================================
# åˆ›å»ºå›¾å½¢
# ============================================================''',

    'model.fit': '''
# ============================================================
# æ¨¡å‹è®­ç»ƒ
# å°†è®­ç»ƒæ•°æ®è¾“å…¥æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•è°ƒæ•´æ¨¡å‹å‚æ•°
# ============================================================''',

    'model.compile': '''
# ============================================================
# ç¼–è¯‘æ¨¡å‹
# é…ç½®è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡
# ============================================================''',
}


def load_notebook(path: Path) -> dict:
    """åŠ è½½notebookæ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_notebook(path: Path, notebook: dict) -> None:
    """ä¿å­˜notebookæ–‡ä»¶"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=1)


def add_inline_comments(source: str) -> str:
    """ä¸ºä»£ç æ·»åŠ è¡Œå†…æ³¨é‡Š"""
    lines = source.split('\n')
    new_lines = []

    # è®°å½•å·²æ·»åŠ çš„æ³¨é‡Šï¼Œé¿å…é‡å¤
    added_comments = set()

    for i, line in enumerate(lines):
        stripped = line.strip()

        # è·³è¿‡ç©ºè¡Œå’Œå·²æœ‰æ³¨é‡Šçš„è¡Œ
        if not stripped or stripped.startswith('#'):
            new_lines.append(line)
            continue

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ³¨é‡Š
        comment_added = False
        for pattern, comment in COMMENT_TEMPLATES.items():
            if pattern in line and '#' not in line:
                # æ£€æŸ¥å‰ä¸€è¡Œæ˜¯å¦å·²æœ‰ç›¸åŒæ³¨é‡Š
                prev_line = new_lines[-1].strip() if new_lines else ''
                if prev_line == comment.strip():
                    # å·²æœ‰æ³¨é‡Šï¼Œè·³è¿‡
                    new_lines.append(line)
                    comment_added = True
                    break

                # æ£€æŸ¥æ˜¯å¦å·²åœ¨æœ¬cellæ·»åŠ è¿‡æ­¤æ³¨é‡Š
                comment_key = f"{comment}:{i}"
                if comment_key not in added_comments:
                    new_lines.append(f"{comment}")
                    added_comments.add(comment_key)

                new_lines.append(line)
                comment_added = True
                break

        if not comment_added:
            new_lines.append(line)

    return '\n'.join(new_lines)


def add_block_comments(source: str) -> str:
    """ä¸ºä»£ç å—æ·»åŠ åˆ†éš”æ³¨é‡Š"""
    for pattern, block_comment in CODE_BLOCK_EXPLANATIONS.items():
        if pattern in source and block_comment.strip() not in source:
            # åœ¨æ¨¡å¼ä¹‹å‰æ·»åŠ å—æ³¨é‡Š
            source = source.replace(pattern, f"{block_comment}\n{pattern}")

    return source


def enhance_notebook_comments(notebook: dict) -> Tuple[dict, int]:
    """å¢å¼ºnotebookä¸­çš„æ³¨é‡Š"""
    comments_added = 0

    for cell in notebook.get('cells', []):
        if cell.get('cell_type') == 'code':
            source_list = cell.get('source', [])
            if isinstance(source_list, list):
                source = ''.join(source_list)
            else:
                source = source_list

            original_length = len(source)

            # æ·»åŠ è¡Œå†…æ³¨é‡Š
            source = add_inline_comments(source)

            # æ·»åŠ å—æ³¨é‡Š
            source = add_block_comments(source)

            if len(source) > original_length:
                comments_added += 1
                # æ›´æ–°cellæºç 
                new_source = source.split('\n')
                cell['source'] = [line + '\n' for line in new_source[:-1]] + [new_source[-1]]

    return notebook, comments_added


def create_header_cell(title: str, description: str) -> dict:
    """åˆ›å»ºæ ‡é¢˜markdownå•å…ƒæ ¼"""
    content = f"""# {title}

{description}

---

## ğŸ“š æœ¬èŠ‚å†…å®¹

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œä½ å°†ï¼š
- ç†è§£æ ¸å¿ƒæ¦‚å¿µå’ŒåŸç†
- æŒæ¡ä»£ç å®ç°æ–¹æ³•
- èƒ½å¤Ÿåº”ç”¨åˆ°å®é™…é—®é¢˜

## â±ï¸ é¢„è®¡æ—¶é—´

15-25åˆ†é’Ÿ
"""
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": content.split('\n')
    }


def process_notebook(path: Path, dry_run: bool = False) -> dict:
    """å¤„ç†å•ä¸ªnotebook"""
    report = {
        'path': str(path),
        'comments_added': 0,
        'error': None
    }

    try:
        notebook = load_notebook(path)
        notebook, comments_added = enhance_notebook_comments(notebook)
        report['comments_added'] = comments_added

        if comments_added > 0 and not dry_run:
            save_notebook(path, notebook)
            report['status'] = 'modified'
        elif comments_added > 0:
            report['status'] = 'would_modify'
        else:
            report['status'] = 'no_change'

    except Exception as e:
        report['error'] = str(e)
        report['status'] = 'error'

    return report


def find_notebooks(root_dir: Path, exclude_optimized: bool = True) -> List[Path]:
    """æŸ¥æ‰¾éœ€è¦å¤„ç†çš„notebookæ–‡ä»¶"""
    notebooks = list(root_dir.rglob('*.ipynb'))
    filtered = []

    for nb in notebooks:
        if '.ipynb_checkpoints' in str(nb):
            continue
        if exclude_optimized and 'ä¼˜åŒ–ç‰ˆ' in str(nb):
            continue
        filtered.append(nb)

    return filtered


def main():
    import argparse

    parser = argparse.ArgumentParser(description='ä¸ºNotebooksæ·»åŠ è¯¦ç»†æ³¨é‡Š')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ£€æŸ¥ï¼Œä¸å®é™…ä¿®æ”¹')
    parser.add_argument('--path', type=str, default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--include-optimized', action='store_true', help='åŒ…å«ä¼˜åŒ–ç‰ˆnotebooks')
    args = parser.parse_args()

    root = Path(args.path)
    notebooks = find_notebooks(root, exclude_optimized=not args.include_optimized)

    print(f"æ‰¾åˆ° {len(notebooks)} ä¸ªnotebookæ–‡ä»¶")
    print("=" * 60)

    total_comments = 0
    modified_count = 0

    for nb_path in notebooks:
        report = process_notebook(nb_path, dry_run=args.dry_run)

        if report['comments_added'] > 0:
            print(f"\nğŸ““ {report['path']}")
            print(f"  âœ… æ·»åŠ æ³¨é‡Š: {report['comments_added']}å¤„")
            total_comments += report['comments_added']
            modified_count += 1

        if report['error']:
            print(f"\nğŸ““ {report['path']}")
            print(f"  âŒ é”™è¯¯: {report['error']}")

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ€»ç»“:")
    print(f"  - å¤„ç†æ–‡ä»¶: {len(notebooks)}")
    print(f"  - æ·»åŠ æ³¨é‡Š: {total_comments}å¤„")
    print(f"  - ä¿®æ”¹æ–‡ä»¶: {modified_count}")

    if args.dry_run:
        print("\nâš ï¸  è¿™æ˜¯dry-runæ¨¡å¼ï¼Œæ²¡æœ‰å®é™…ä¿®æ”¹æ–‡ä»¶")


if __name__ == '__main__':
    main()
