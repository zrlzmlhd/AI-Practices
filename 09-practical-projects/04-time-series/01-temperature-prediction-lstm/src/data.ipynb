{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cab97c7",
   "metadata": {},
   "source": [
    "# 时间序列数据处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345469c5",
   "metadata": {},
   "source": [
    "本模块负责：\n",
    "1. 加载Jena气候数据集\n",
    "2. 数据预处理和归一化\n",
    "3. 创建滑动窗口序列\n",
    "4. 数据集划分\n",
    "\n",
    "每个步骤都有详细的注释说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8ad73",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c324076",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f58cb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataProcessor:\n",
    "    \"\"\"\n",
    "    时间序列数据处理器\n",
    "\n",
    "    【是什么】：处理时间序列数据的工具类\n",
    "    【做什么】：\n",
    "        - 加载和清洗数据\n",
    "        - 特征工程\n",
    "        - 创建滑动窗口\n",
    "        - 数据归一化\n",
    "    【为什么】：\n",
    "        - 时间序列需要特殊的处理方式\n",
    "        - 滑动窗口是时间序列预测的核心\n",
    "        - 归一化提高模型性能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, target_column='T (degC)',\n",
    "                 selected_features=None, sampling_rate=6):\n",
    "        \"\"\"\n",
    "        初始化数据处理器\n",
    "\n",
    "        Args:\n",
    "            data_path: 数据文件路径\n",
    "            target_column: 目标列名（要预测的变量）\n",
    "            selected_features: 选择的特征列（None表示使用所有特征）\n",
    "            sampling_rate: 采样率（每隔多少条记录取一条）\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.target_column = target_column\n",
    "        self.selected_features = selected_features\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # 数据归一化器\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # 数据\n",
    "        self.df = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        加载数据\n",
    "\n",
    "        Returns:\n",
    "            DataFrame\n",
    "        \"\"\"\n",
    "        print(\"\\n加载数据...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 读取CSV文件\n",
    "        # ============================================\n",
    "        # 【是什么】：Jena气候数据集\n",
    "        # 【格式】：CSV文件，包含日期时间和14个气象特征\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "\n",
    "        print(f\"  原始数据形状: {self.df.shape}\")\n",
    "        print(f\"  时间跨度: {self.df['Date Time'].iloc[0]} 到 {self.df['Date Time'].iloc[-1]}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 处理日期时间\n",
    "        # ============================================\n",
    "        # 【是什么】：将字符串转换为datetime对象\n",
    "        # 【为什么】：\n",
    "        #   - 方便时间相关的操作\n",
    "        #   - 可以提取时间特征（小时、星期等）\n",
    "        if 'Date Time' in self.df.columns:\n",
    "            self.df['Date Time'] = pd.to_datetime(self.df['Date Time'])\n",
    "            self.df.set_index('Date Time', inplace=True)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 降采样\n",
    "        # ============================================\n",
    "        # 【是什么】：每隔sampling_rate条记录取一条\n",
    "        # 【为什么】：\n",
    "        #   - 原始数据每10分钟一条，数据量太大\n",
    "        #   - 降采样到每小时一条（sampling_rate=6）\n",
    "        #   - 减少计算量，保留主要模式\n",
    "        if self.sampling_rate > 1:\n",
    "            self.df = self.df[::self.sampling_rate]\n",
    "            print(f\"  降采样后形状: {self.df.shape}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: 选择特征\n",
    "        # ============================================\n",
    "        if self.selected_features is not None:\n",
    "            # 使用指定的特征\n",
    "            self.df = self.df[self.selected_features]\n",
    "            self.feature_names = self.selected_features\n",
    "        else:\n",
    "            # 使用所有数值特征\n",
    "            self.feature_names = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            self.df = self.df[self.feature_names]\n",
    "\n",
    "        print(f\"  使用特征数: {len(self.feature_names)}\")\n",
    "        print(f\"  特征列表: {self.feature_names}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤5: 处理缺失值\n",
    "        # ============================================\n",
    "        # 【是什么】：检查并处理NaN值\n",
    "        # 【为什么】：\n",
    "        #   - 缺失值会导致模型训练失败\n",
    "        #   - 时间序列常用前向填充\n",
    "        missing_count = self.df.isnull().sum().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  发现缺失值: {missing_count}\")\n",
    "            # 前向填充：用前一个值填充\n",
    "            self.df.fillna(method='ffill', inplace=True)\n",
    "            # 后向填充：处理开头的缺失值\n",
    "            self.df.fillna(method='bfill', inplace=True)\n",
    "            print(f\"  缺失值已处理\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def normalize_data(self, train_split=0.7):\n",
    "        \"\"\"\n",
    "        归一化数据\n",
    "\n",
    "        Args:\n",
    "            train_split: 训练集比例（用于fit scaler）\n",
    "\n",
    "        Returns:\n",
    "            归一化后的数据\n",
    "        \"\"\"\n",
    "        print(\"\\n归一化数据...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 重要：只在训练集上fit scaler\n",
    "        # ============================================\n",
    "        # 【是什么】：StandardScaler标准化\n",
    "        # 【为什么只用训练集】：\n",
    "        #   - 防止数据泄露\n",
    "        #   - 测试集应该用训练集的统计量\n",
    "        #   - 模拟真实预测场景\n",
    "\n",
    "        train_size = int(len(self.df) * train_split)\n",
    "        train_data = self.df.iloc[:train_size].values\n",
    "\n",
    "        # 在训练集上fit\n",
    "        self.scaler.fit(train_data)\n",
    "\n",
    "        # 转换所有数据\n",
    "        normalized_data = self.scaler.transform(self.df.values)\n",
    "\n",
    "        print(f\"  归一化完成\")\n",
    "        print(f\"  均值: {self.scaler.mean_[:3]}\")  # 显示前3个特征的均值\n",
    "        print(f\"  标准差: {self.scaler.scale_[:3]}\")  # 显示前3个特征的标准差\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    def create_sequences(self, data, lookback=168, forecast_horizon=24, step=1):\n",
    "        \"\"\"\n",
    "        创建滑动窗口序列\n",
    "\n",
    "        【是什么】：将时间序列转换为监督学习问题\n",
    "        【做什么】：\n",
    "            - 输入：过去lookback个时间步的数据\n",
    "            - 输出：未来forecast_horizon个时间步的目标值\n",
    "        【为什么】：\n",
    "            - LSTM需要固定长度的输入\n",
    "            - 滑动窗口捕获时间依赖关系\n",
    "\n",
    "        Args:\n",
    "            data: 归一化后的数据\n",
    "            lookback: 回看窗口大小（输入序列长度）\n",
    "            forecast_horizon: 预测时间范围（输出序列长度）\n",
    "            step: 滑动步长\n",
    "\n",
    "        Returns:\n",
    "            X, y: 输入序列和目标值\n",
    "\n",
    "        示例：\n",
    "            lookback=168 (7天 * 24小时)\n",
    "            forecast_horizon=24 (预测未来24小时)\n",
    "\n",
    "            输入: 过去7天的[温度, 湿度, 气压, ...]\n",
    "            输出: 未来24小时的温度\n",
    "        \"\"\"\n",
    "        print(\"\\n创建滑动窗口序列...\")\n",
    "        print(f\"  回看窗口: {lookback} 小时\")\n",
    "        print(f\"  预测范围: {forecast_horizon} 小时\")\n",
    "        print(f\"  滑动步长: {step}\")\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        # ============================================\n",
    "        # 滑动窗口算法\n",
    "        # ============================================\n",
    "        # 【原理】：\n",
    "        #   时刻t: 使用 [t-lookback, t] 预测 [t+1, t+forecast_horizon]\n",
    "        #   时刻t+step: 使用 [t+step-lookback, t+step] 预测 [t+step+1, t+step+forecast_horizon]\n",
    "\n",
    "        # 找到目标列的索引\n",
    "        target_idx = self.feature_names.index(self.target_column)\n",
    "\n",
    "        for i in range(0, len(data) - lookback - forecast_horizon + 1, step):\n",
    "            # 输入序列：过去lookback个时间步的所有特征\n",
    "            # 【形状】：(lookback, num_features)\n",
    "            X.append(data[i:i+lookback])\n",
    "\n",
    "            # 输出序列：未来forecast_horizon个时间步的目标值\n",
    "            # 【形状】：(forecast_horizon,)\n",
    "            # 【注意】：只预测目标列（温度）\n",
    "            y.append(data[i+lookback:i+lookback+forecast_horizon, target_idx])\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        print(f\"  生成序列数: {len(X)}\")\n",
    "        print(f\"  输入形状: {X.shape}\")  # (samples, lookback, features)\n",
    "        print(f\"  输出形状: {y.shape}\")  # (samples, forecast_horizon)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def split_data(self, X, y, train_split=0.7, val_split=0.15):\n",
    "        \"\"\"\n",
    "        划分数据集\n",
    "\n",
    "        【重要】：时间序列不能随机划分！\n",
    "        【为什么】：\n",
    "            - 必须保持时间顺序\n",
    "            - 训练集在前，验证集在中，测试集在后\n",
    "            - 模拟真实预测场景\n",
    "\n",
    "        Args:\n",
    "            X: 输入序列\n",
    "            y: 目标值\n",
    "            train_split: 训练集比例\n",
    "            val_split: 验证集比例\n",
    "\n",
    "        Returns:\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "        \"\"\"\n",
    "        print(\"\\n划分数据集...\")\n",
    "\n",
    "        n_samples = len(X)\n",
    "\n",
    "        # ============================================\n",
    "        # 按时间顺序划分\n",
    "        # ============================================\n",
    "        # 【示例】：\n",
    "        #   总样本: 10000\n",
    "        #   训练集: 0-7000 (70%)\n",
    "        #   验证集: 7000-8500 (15%)\n",
    "        #   测试集: 8500-10000 (15%)\n",
    "\n",
    "        train_size = int(n_samples * train_split)\n",
    "        val_size = int(n_samples * val_split)\n",
    "\n",
    "        # 训练集\n",
    "        X_train = X[:train_size]\n",
    "        y_train = y[:train_size]\n",
    "\n",
    "        # 验证集\n",
    "        X_val = X[train_size:train_size+val_size]\n",
    "        y_val = y[train_size:train_size+val_size]\n",
    "\n",
    "        # 测试集\n",
    "        X_test = X[train_size+val_size:]\n",
    "        y_test = y[train_size+val_size:]\n",
    "\n",
    "        print(f\"  训练集: {X_train.shape}\")\n",
    "        print(f\"  验证集: {X_val.shape}\")\n",
    "        print(f\"  测试集: {X_test.shape}\")\n",
    "\n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "    def inverse_transform_target(self, y):\n",
    "        \"\"\"\n",
    "        反归一化目标值\n",
    "\n",
    "        【是什么】：将归一化的预测值转换回原始尺度\n",
    "        【为什么】：\n",
    "            - 评估时需要原始尺度的值\n",
    "            - 便于理解预测结果（如温度20°C）\n",
    "\n",
    "        Args:\n",
    "            y: 归一化的目标值\n",
    "\n",
    "        Returns:\n",
    "            原始尺度的目标值\n",
    "        \"\"\"\n",
    "        # 找到目标列的索引\n",
    "        target_idx = self.feature_names.index(self.target_column)\n",
    "\n",
    "        # 获取目标列的均值和标准差\n",
    "        mean = self.scaler.mean_[target_idx]\n",
    "        scale = self.scaler.scale_[target_idx]\n",
    "\n",
    "        # 反归一化：y_original = y_normalized * scale + mean\n",
    "        return y * scale + mean\n",
    "\n",
    "    def save_scaler(self, filepath):\n",
    "        \"\"\"保存归一化器\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'scaler': self.scaler,\n",
    "                'feature_names': self.feature_names,\n",
    "                'target_column': self.target_column\n",
    "            }, f)\n",
    "        print(f\"✓ 归一化器已保存: {filepath}\")\n",
    "\n",
    "    def load_scaler(self, filepath):\n",
    "        \"\"\"加载归一化器\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.scaler = data['scaler']\n",
    "            self.feature_names = data['feature_names']\n",
    "            self.target_column = data['target_column']\n",
    "        print(f\"✓ 归一化器已加载: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ab489",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_temperature_data(data_path='data/jena_climate_2009_2016.csv',\n",
    "                             lookback=168,\n",
    "                             forecast_horizon=24,\n",
    "                             train_split=0.7,\n",
    "                             val_split=0.15,\n",
    "                             sampling_rate=6,\n",
    "                             selected_features=None):\n",
    "    \"\"\"\n",
    "    准备温度预测数据\n",
    "\n",
    "    Args:\n",
    "        data_path: 数据文件路径\n",
    "        lookback: 回看窗口大小（小时）\n",
    "        forecast_horizon: 预测范围（小时）\n",
    "        train_split: 训练集比例\n",
    "        val_split: 验证集比例\n",
    "        sampling_rate: 采样率\n",
    "        selected_features: 选择的特征\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), processor\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"温度预测数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤1: 创建数据处理器\n",
    "    # ============================================\n",
    "    processor = TimeSeriesDataProcessor(\n",
    "        data_path=data_path,\n",
    "        target_column='T (degC)',\n",
    "        selected_features=selected_features,\n",
    "        sampling_rate=sampling_rate\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤2: 加载数据\n",
    "    # ============================================\n",
    "    processor.load_data()\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤3: 归一化\n",
    "    # ============================================\n",
    "    normalized_data = processor.normalize_data(train_split=train_split)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤4: 创建序列\n",
    "    # ============================================\n",
    "    X, y = processor.create_sequences(\n",
    "        normalized_data,\n",
    "        lookback=lookback,\n",
    "        forecast_horizon=forecast_horizon\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤5: 划分数据集\n",
    "    # ============================================\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = processor.split_data(\n",
    "        X, y,\n",
    "        train_split=train_split,\n",
    "        val_split=val_split\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 数据统计\n",
    "    # ============================================\n",
    "    print(\"\\n数据统计:\")\n",
    "    print(f\"  特征数: {X_train.shape[2]}\")\n",
    "    print(f\"  输入序列长度: {X_train.shape[1]} 小时\")\n",
    "    print(f\"  预测范围: {y_train.shape[1]} 小时\")\n",
    "    print(f\"  训练样本数: {len(X_train)}\")\n",
    "    print(f\"  验证样本数: {len(X_val)}\")\n",
    "    print(f\"  测试样本数: {len(X_test)}\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c0b23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试数据处理\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 注意：需要先下载数据\n",
    "    # 这里使用模拟数据进行测试\n",
    "\n",
    "    # 创建模拟数据\n",
    "    print(\"\\n创建模拟数据...\")\n",
    "    n_samples = 1000\n",
    "    n_features = 5\n",
    "\n",
    "    # 模拟时间序列数据（带有趋势和周期性）\n",
    "    t = np.arange(n_samples)\n",
    "    data = np.zeros((n_samples, n_features))\n",
    "\n",
    "    for i in range(n_features):\n",
    "        # 趋势 + 周期 + 噪声\n",
    "        trend = 0.01 * t\n",
    "        seasonal = 10 * np.sin(2 * np.pi * t / 24)  # 24小时周期\n",
    "        noise = np.random.randn(n_samples)\n",
    "        data[:, i] = trend + seasonal + noise\n",
    "\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "    df.columns = ['T (degC)', 'p (mbar)', 'rh (%)', 'wv (m/s)', 'wd (deg)']\n",
    "\n",
    "    # 保存临时文件\n",
    "    temp_path = 'temp_test_data.csv'\n",
    "    df.to_csv(temp_path, index=False)\n",
    "\n",
    "    # 测试数据处理\n",
    "    try:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), processor = prepare_temperature_data(\n",
    "            data_path=temp_path,\n",
    "            lookback=24,\n",
    "            forecast_horizon=6,\n",
    "            sampling_rate=1\n",
    "        )\n",
    "\n",
    "        print(\"\\n✓ 数据处理测试通过！\")\n",
    "\n",
    "        # 测试反归一化\n",
    "        print(\"\\n测试反归一化...\")\n",
    "        y_original = processor.inverse_transform_target(y_test[0])\n",
    "        print(f\"  归一化值: {y_test[0][:5]}\")\n",
    "        print(f\"  原始值: {y_original[:5]}\")\n",
    "\n",
    "    finally:\n",
    "        # 清理临时文件\n",
    "        import os\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
