{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d824e8",
   "metadata": {},
   "source": [
    "# my_lib - lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a8ab0",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec4672",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "# https://github.com/nachiket273/lookahead_pytorch\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991a172",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class RAdam(Optimizer):\n",
    "    r\"\"\"Implements RAdam algorithm.\n",
    "    It has been proposed in `ON THE VARIANCE OF THE ADAPTIVE LEARNING\n",
    "    RATE AND BEYOND(https://arxiv.org/pdf/1908.03265.pdf)`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable):      iterable of parameters to optimize or dicts defining\n",
    "                                parameter groups\n",
    "        lr (float, optional):   learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional):  coefficients used for computing\n",
    "                                                running averages of gradient and\n",
    "                                                its square (default: (0.9, 0.999))\n",
    "        eps (float, optional):  term added to the denominator to improve\n",
    "                                numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional):    whether to use the AMSGrad variant of this\n",
    "                                        algorithm from the paper `On the Convergence\n",
    "                                        of Adam and Beyond`_(default: False)\n",
    "\n",
    "        sma_thresh:             simple moving average threshold.\n",
    "                                Length till where the variance of adaptive lr is intracable.\n",
    "                                Default: 4 (as per paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, sma_thresh=4):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "        self.sma_thresh = sma_thresh\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                old = p.data.float()\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                buffer = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if buffer[0] == state['step']:\n",
    "                    sma_t, step_size = buffer[1], buffer[2]\n",
    "                else:\n",
    "                    sma_max_len = 2 / (1 - beta2) - 1\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    sma_t = sma_max_len - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffer[0] = state['step']\n",
    "                    buffer[1] = sma_t\n",
    "\n",
    "                    if sma_t > self.sma_thresh:\n",
    "                        rt = math.sqrt(\n",
    "                            ((sma_t - 4) * (sma_t - 2) * sma_max_len) / ((sma_max_len - 4) * (sma_max_len - 2) * sma_t))\n",
    "                        step_size = group['lr'] * rt * math.sqrt((1 - beta2_t)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffer[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(old, alpha=-group['weight_decay'] * group['lr'])\n",
    "\n",
    "                if sma_t > self.sma_thresh:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p.data.addcdiv_(exp_avg, denom, value= -step_size, )\n",
    "                else:\n",
    "                    p.data.add_(exp_avg, alpha=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96617c63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    r'''Implements Lookahead optimizer.\n",
    "\n",
    "    It's been proposed in paper: Lookahead Optimizer: k steps forward, 1 step back\n",
    "    (https://arxiv.org/pdf/1907.08610.pdf)\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer object used in inner loop for fast weight updates.\n",
    "        alpha:     The learning rate for slow weight update.\n",
    "                   Default: 0.5\n",
    "        k:         Number of iterations of fast weights updates before updating slow\n",
    "                   weights.\n",
    "                   Default: 5\n",
    "\n",
    "    Example:\n",
    "        > optim = Lookahead(optimizer)\n",
    "        > optim = Lookahead(optimizer, alpha=0.6, k=10)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, alpha=0.5, k=5):\n",
    "        assert (0.0 <= alpha <= 1.0)\n",
    "        assert (k >= 1)\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.k_counter = 0\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.slow_weights = [[param.clone().detach() for param in group['params']] for group in self.param_groups]\n",
    "        self.defaults = {}\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self.k_counter += 1\n",
    "        if self.k_counter >= self.k:\n",
    "            for group, slow_weight in zip(self.param_groups, self.slow_weights):\n",
    "                for param, weight in zip(group['params'], slow_weight):\n",
    "                    weight.data.add_((param.data - weight.data), alpha=self.alpha)\n",
    "                    param.data.copy_(weight.data)\n",
    "            self.k_counter = 0\n",
    "        return loss\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'state': self.state,\n",
    "            'optimizer': self.optimizer,\n",
    "            'alpha': self.alpha,\n",
    "            'k': self.k,\n",
    "            'k_counter': self.k_counter\n",
    "        }\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
