{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f55e34",
   "metadata": {},
   "source": [
    "# src - run_ensemble_and_local_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a82231",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebaa46",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/05_Kaggle竞赛项目/04-RSNA-2024-Lumbar-Spine/src/scs_trainer/run_ensemble_and_local_validation.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/05_Kaggle竞赛项目/04-RSNA-2024-Lumbar-Spine/src/scs_trainer/run_ensemble_and_local_validation.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28e4de",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from dataset import *\n",
    "from model import *\n",
    "\n",
    "from my_lib.runner import *\n",
    "from my_lib.file import *\n",
    "from my_lib.net.rate import get_learning_rate\n",
    "from my_lib.draw import *\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd7b87",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#######################################################33\n",
    "\n",
    "def run_infer_and_save(cfg, ensemble_dir):\n",
    "\tos.makedirs(ensemble_dir, exist_ok=True)\n",
    "\n",
    "\tlog = Logger()\n",
    "\tlog.open(ensemble_dir + '/log.infer.txt', mode='a')\n",
    "\tlog.write(f'\\n--- [START {log.timestamp()}] {\"-\" * 64}')\n",
    "\tlog.write(f'__file__ = {__file__}\\n')\n",
    "\tlog.write(f'cfg:\\n{format_dict(cfg)}')\n",
    "\tlog.write(f'')\n",
    "\n",
    "\t# --- dataset ---\n",
    "\tprocessed_df = load_csv()\n",
    "\ttrain_id, valid_id = make_random_split(fold=cfg.fold)\n",
    "\tvalid_dataset = SplineDataset(processed_df, valid_id, cfg=cfg, augment=make_valid_augment, mode='valid')\n",
    "\n",
    "\tvalid_loader = DataLoader(\n",
    "\t\tvalid_dataset,\n",
    "\t\tsampler=SequentialSampler(valid_dataset),\n",
    "\t\tbatch_size=cfg.valid_batch_size,\n",
    "\t\tdrop_last=False,\n",
    "\t\tnum_workers=cfg.valid_num_worker,\n",
    "\t\tpin_memory=True,\n",
    "\t\tcollate_fn=null_collate,\n",
    "\t)\n",
    "\tlog.write(f'fold = {cfg.fold}')\n",
    "\tlog.write(f'valid_dataset : \\n{str(valid_dataset)}')\n",
    "\tlog.write('\\n')\n",
    "\n",
    "\t# ---model ---\n",
    "\tscaler = torch.cuda.amp.GradScaler(enabled=cfg.is_amp)\n",
    "\tnet = Net(pretrained=True, cfg=cfg)\n",
    "\tnet.cuda()\n",
    "\tlog.write(f'net:\\n\\t{str(net.arch)}')\n",
    "\n",
    "\tif cfg.resume_from.checkpoint is not None:\n",
    "\t\tf = torch.load(cfg.resume_from.checkpoint, map_location=lambda storage, loc: storage)\n",
    "\t\tstate_dict = f['state_dict']\n",
    "\t\tprint(net.load_state_dict(state_dict, strict=False))  # True\n",
    "\t\titeration = f.get('iteration', 0)\n",
    "\n",
    "\t### start here! ################################################\n",
    "\tif 1:\n",
    "\t\tresult = dotdict(\n",
    "\t\t\tD=[],\n",
    "\t\t\tgrade_truth=[],\n",
    "\t\t\tgrade=[],\n",
    "\t\t\txyz_truth=[],\n",
    "\t\t\txy=[],\n",
    "\t\t\tz=[],\n",
    "\t\t)\n",
    "\t\tnum_valid = 0\n",
    "\t\tstart_timer = timer()\n",
    "\n",
    "\t\tnet.cuda()\n",
    "\t\tnet.eval()\n",
    "\t\tnet.output_type = ['loss', 'infer']\n",
    "\n",
    "\t\tfor t, batch in enumerate(valid_loader):\n",
    "\t\t\twith torch.cuda.amp.autocast(enabled=cfg.is_amp):\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\toutput = net(batch)\n",
    "\n",
    "\t\t\tB = len(batch['index'])\n",
    "\t\t\tnum_valid += B\n",
    "\t\t\tresult.grade_truth.append(batch['grade'].data.cpu().numpy())\n",
    "\t\t\tresult.grade.append(output['grade'].data.cpu().numpy())\n",
    "\t\t\tresult.xyz_truth.append(batch['xyz'].data.cpu().numpy())\n",
    "\t\t\tresult.xy.append(output['xy'].data.cpu().numpy())\n",
    "\t\t\tresult.z.append(output['z'].data.cpu().numpy())\n",
    "\n",
    "\t\t\tprint(f'\\r validation: {num_valid}/{len(valid_dataset)}', time_to_str(timer() - start_timer, 'min'),\n",
    "\t\t\t\t  end='', flush=True)\n",
    "\n",
    "\t\tprint('')\n",
    "\t\t#-------------\n",
    "\t\txyz_truth = np.concatenate(result.xyz_truth)\n",
    "\t\tz = np.concatenate(result.z)\n",
    "\t\txy = np.concatenate(result.xy)\n",
    "\t\tgrade = np.concatenate(result.grade)\n",
    "\t\tgrade_truth = np.concatenate(result.grade_truth)\n",
    "\n",
    "\t\tnp.savez(f'{ensemble_dir}/{net.arch}-fold{cfg.fold}-{iteration:08d}.result.npz',\n",
    "\t\t\t\t grade=grade, grade_truth=grade_truth, xy=xy, z=z, xyz_truth=xyz_truth)\n",
    "\n",
    "\t\tlog.write('do_local_lb():')\n",
    "\t\tlog.write(str(do_local_lb(grade, grade_truth, True)))\n",
    "\t\tlog.write('do_compute_point_error():')\n",
    "\t\tlog.write(str(do_compute_point_error(xy, z, xyz_truth,)))\n",
    "\t\tlog.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e257d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_ensemble_and_eval(ensemble_dir, name):\n",
    "\n",
    "\tdf=[]\n",
    "\tfor fold in [0,1,2,3,4]:\n",
    "\t\tavg_grade,avg_z, avg_xy = 0.,0.,0.\n",
    "\t\tnum_name= len(name)\n",
    "\t\tfor n in name:\n",
    "\t\t\tnpz_file = glob(f'{ensemble_dir}/{n}-fold{fold}*.result.npz'.replace('[', '[[]'))[0]\n",
    "\t\t\tnpz =  np.load(npz_file)\n",
    "\n",
    "\t\t\tnpz_file = npz_file.split('/')[-1]\n",
    "\t\t\tprint(npz_file)\n",
    "\n",
    "\t\t\tloss, weighted_loss, any_loss = do_local_lb(npz['grade'], npz['grade_truth'], True)\n",
    "\t\t\tx_err,y_err,z_err,threshold = do_compute_point_error(npz['xy'], npz['z'], npz['xyz_truth'] )\n",
    "\t\t\tone_row = {\n",
    "\t\t\t\t#'weight':npz_file,\n",
    "\t\t\t\t'name' : f'{n}',\n",
    "\t\t\t\t'fold': fold,\n",
    "\t\t\t\t'lb': weighted_loss,\n",
    "\t\t\t\t'lb(any)': any_loss,\n",
    "\t\t\t\t'x<1': x_err[0],\n",
    "\t\t\t\t'x<2': x_err[1],\n",
    "\t\t\t\t'x<5': x_err[3],\n",
    "\t\t\t\t'y<1': y_err[0],\n",
    "\t\t\t\t'y<2': y_err[1],\n",
    "\t\t\t\t'y<5': y_err[3],\n",
    "\t\t\t\t'z<1': z_err[0],\n",
    "\t\t\t\t'z<2': z_err[1],\n",
    "\t\t\t\t'z<5': z_err[3],\n",
    "\t\t\t}\n",
    "\t\t\tdf.append(one_row)\n",
    "\t\t\tavg_grade += npz['grade']\n",
    "\t\t\tavg_xy += npz['xy']\n",
    "\t\t\tavg_z += npz['z']\n",
    "\n",
    "\t\t#----------------------------------------\n",
    "\t\tavg_grade /=num_name\n",
    "\t\tavg_xy /=num_name\n",
    "\t\tavg_z /=num_name\n",
    "\n",
    "\t\tloss, weighted_loss, any_loss = do_local_lb(avg_grade, npz['grade_truth'], True)\n",
    "\t\tx_err, y_err, z_err, threshold = do_compute_point_error(avg_xy, avg_z, npz['xyz_truth'])\n",
    "\t\tone_row = {\n",
    "\t\t\t'name' : f'ensemble',\n",
    "\t\t\t'fold': fold,\n",
    "\t\t\t'lb': weighted_loss,\n",
    "\t\t\t'lb(any)': any_loss,\n",
    "\t\t\t'x<1': x_err[0],\n",
    "\t\t\t'x<2': x_err[1],\n",
    "\t\t\t'x<5': x_err[3],\n",
    "\t\t\t'y<1': y_err[0],\n",
    "\t\t\t'y<2': y_err[1],\n",
    "\t\t\t'y<5': y_err[3],\n",
    "\t\t\t'z<1': z_err[0],\n",
    "\t\t\t'z<2': z_err[1],\n",
    "\t\t\t'z<5': z_err[3],\n",
    "\t\t}\n",
    "\t\tdf.append(one_row)\n",
    "\t#-----------------------------\n",
    "\tprint('')\n",
    "\tdf = pd.DataFrame(df)\n",
    "\tdf = df.sort_values(by=['name','fold'])\n",
    "\tdf.to_csv(f'{ensemble_dir}/ensemble.csv', index=False)\n",
    "\tprint(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57887c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "\tfrom configure import *\n",
    "\n",
    "\tcfg = deepcopy(default_cfg)\n",
    "\tensemble_dir = f'{RESULT_DIR}/one-stage-scs/ensemble'\n",
    "\tif 0:\n",
    "\t\tcfg.experiment_name = 'one-stage-scs/pvt_v2_b4-decoder2d-01'\n",
    "\t\tcfg.image_size = 320\n",
    "\t\tcfg.mask_size  = 320//4\n",
    "\t\tcfg.arch = 'pvt_v2_b4'\n",
    "\t\tfor f in [0,1,2,3,4]:\n",
    "\t\t\tcfg.fold = f\n",
    "\t\t\tcfg.resume_from.checkpoint = (f'{RESULT_DIR}/{cfg.experiment_name}' + [\n",
    "\t\t\t\t'/fold0-00009899.pth',\n",
    "\t\t\t\t'/fold1-00008416.pth',\n",
    "\t\t\t\t'/fold2-00012259.pth',\n",
    "\t\t\t\t'/fold3-00009486.pth',\n",
    "\t\t\t\t'/fold4-00006276.pth',\n",
    "\t\t\t][f])\n",
    "\t\t\t# cfg.fold_dir = f'{RESULT_DIR}/{cfg.experiment_name}/fold-{cfg.fold}'\n",
    "\t\t\t# cfg.resume_from.checkpoint = (cfg.fold_dir + [\n",
    "\t\t\t#\n",
    "\t\t\t# \t'/checkpoint/00032592.pth',\n",
    "\t\t\t# \t'/checkpoint/00032144.pth',\n",
    "\t\t\t# \t'/checkpoint/00036524.pth',\n",
    "\t\t\t# \t'/checkpoint/00032619.pth',\n",
    "\t\t\t# \t'/checkpoint/00029488.pth',\n",
    "\t\t\t# ][f])\n",
    "\t\t\trun_infer_and_save(cfg, ensemble_dir)\n",
    "\n",
    "\tif 0:\n",
    "\t\tcfg.experiment_name = 'one-stage-scs/convnext_base-decoder2d-01'\n",
    "\t\tcfg.image_size = 320\n",
    "\t\tcfg.mask_size  = 320//4\n",
    "\t\tcfg.arch = 'convnext_base.fb_in22k'\n",
    "\t\tfor f in [0,1,2,3,4]:\n",
    "\t\t\tcfg.fold = f\n",
    "\t\t\tcfg.resume_from.checkpoint = (f'{RESULT_DIR}/{cfg.experiment_name}' + [\n",
    "\t\t\t\t'/fold0-00010941.pth',\n",
    "\t\t\t\t'/fold1-00007890.pth',\n",
    "\t\t\t\t'/fold2-00010127.pth',\n",
    "\t\t\t\t'/fold3-00008959.pth',\n",
    "\t\t\t\t'/fold4-00008891.pth',\n",
    "\t\t\t][f])\n",
    "\t\t\trun_infer_and_save(cfg, ensemble_dir)\n",
    "\tif 0:\n",
    "\t\tcfg.experiment_name = 'one-stage-scs/effnet_b4-decoder2d-01'\n",
    "\t\tcfg.image_size = 384\n",
    "\t\tcfg.mask_size  = 384//4\n",
    "\t\tcfg.arch = 'tf_efficientnet_b4.ns_jft_in1k'\n",
    "\t\tcfg.valid_batch_size=16\n",
    "\t\tfor f in [0,1,2,3,4]:\n",
    "\t\t\tcfg.fold = f\n",
    "\t\t\tcfg.resume_from.checkpoint = (f'{RESULT_DIR}/{cfg.experiment_name}' + [\n",
    "\t\t\t\t'/fold0-00015109.pth',\n",
    "\t\t\t\t'/fold1-00014728.pth',\n",
    "\t\t\t\t'/fold2-00019721.pth',\n",
    "\t\t\t\t'/fold3-00011594.pth',\n",
    "\t\t\t\t'/fold4-00013075.pth',\n",
    "\t\t\t][f])\n",
    "\t\t\trun_infer_and_save(cfg, ensemble_dir)\n",
    "\n",
    "\t#exit(0)\n",
    "\trun_ensemble_and_eval(ensemble_dir,name=['pvt_v2_b4','convnext_base.fb_in22k','tf_efficientnet_b4.ns_jft_in1k'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
