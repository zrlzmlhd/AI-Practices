{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83580b92",
   "metadata": {},
   "source": [
    "# src - trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4221b",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40801128",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/05_Kaggle竞赛项目/04-RSNA-2024-Lumbar-Spine/src/scs_trainer/trainer.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/05_Kaggle竞赛项目/04-RSNA-2024-Lumbar-Spine/src/scs_trainer/trainer.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc051b5e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from dataset import *\n",
    "from model import *\n",
    "\n",
    "from my_lib.runner import *\n",
    "from my_lib.file import *\n",
    "from my_lib.net.rate import get_learning_rate\n",
    "from my_lib.draw import *\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from timm.utils import ModelEmaV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947a931",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "\n",
    "def run_trainer(cfg):\n",
    "\t# --- setup ---\n",
    "\tseed_everything(cfg.seed)\n",
    "\tos.makedirs(cfg.fold_dir, exist_ok=True)\n",
    "\tfor f in ['checkpoint', 'train', 'valid', 'etc']:\n",
    "\t\tos.makedirs(cfg.fold_dir + '/' + f, exist_ok=True)\n",
    "\n",
    "\tlog = Logger()\n",
    "\tlog.open(cfg.fold_dir + '/log.train.txt', mode='a')\n",
    "\tlog.write(f'\\n--- [START {log.timestamp()}] {\"-\" * 64}')\n",
    "\tlog.write(f'__file__ = {__file__}\\n')\n",
    "\tlog.write(f'cfg:\\n{format_dict(cfg)}')\n",
    "\tlog.write(f'')\n",
    "\n",
    "\t# --- dataset ---\n",
    "\tprocessed_df = load_csv()\n",
    "\ttrain_id, valid_id = make_random_split(fold=cfg.fold)\n",
    "\ttrain_dataset = SplineDataset(processed_df, train_id, cfg=cfg, augment=make_train_augment, mode='train')#make_train_augment\n",
    "\tvalid_dataset = SplineDataset(processed_df, valid_id, cfg=cfg, augment=make_valid_augment, mode='valid')\n",
    "\n",
    "\ttrain_loader = DataLoader(\n",
    "\t\ttrain_dataset,\n",
    "\t\tsampler=RandomSampler(train_dataset),\n",
    "\t\tbatch_size=cfg.train_batch_size,\n",
    "\t\tdrop_last=True,\n",
    "\t\tnum_workers=cfg.train_num_worker,\n",
    "\t\tpin_memory=True,\n",
    "\t\tworker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "\t\tcollate_fn=null_collate,\n",
    "\t)\n",
    "\tvalid_loader = DataLoader(\n",
    "\t\tvalid_dataset,\n",
    "\t\tsampler=SequentialSampler(valid_dataset),\n",
    "\t\tbatch_size=cfg.valid_batch_size,\n",
    "\t\tdrop_last=False,\n",
    "\t\tnum_workers=cfg.valid_num_worker,\n",
    "\t\tpin_memory=True,\n",
    "\t\tcollate_fn=null_collate,\n",
    "\t)\n",
    "\tnum_train_batch = len(train_loader)\n",
    "\n",
    "\tlog.write(f'fold = {cfg.fold}')\n",
    "\tlog.write(f'valid_dataset : \\n{str(valid_dataset)}')\n",
    "\tlog.write(f'train_dataset : \\n{str(train_dataset)}')\n",
    "\tlog.write(f'num_train_batch : \\n{num_train_batch}')\n",
    "\tlog.write('\\n')\n",
    "\n",
    "\t# ---model ---\n",
    "\tscaler = torch.cuda.amp.GradScaler(enabled=cfg.is_amp)\n",
    "\tnet = Net(pretrained=True, cfg=cfg)\n",
    "\tnet.cuda()\n",
    "\tema = ModelEmaV2(net, decay=0.99)\n",
    "\tlog.write(f'net:\\n\\t{str(net.arch)}')\n",
    "\n",
    "\toptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=cfg.lr)\n",
    "\tlog.write(f'optimizer:\\n\\t{str(optimizer)}')\n",
    "\tlog.write('')\n",
    "\n",
    "\t# --- loop ---\n",
    "\tstart_iteration = 0\n",
    "\tstart_epoch = 0\n",
    "\tif cfg.resume_from.checkpoint is not None:\n",
    "\t\tf = torch.load(cfg.resume_from.checkpoint, map_location=lambda storage, loc: storage)\n",
    "\t\tstate_dict = f['state_dict']\n",
    "\t\tprint(net.load_state_dict(state_dict, strict=False))  # True\n",
    "\t\tif cfg.resume_from.iteration < 0:\n",
    "\t\t\tstart_iteration = f.get('iteration', 0)\n",
    "\t\t\tstart_epoch = f.get('epoch', 0)\n",
    "\t\tema.set(net)\n",
    "\n",
    "\tif cfg.is_torch_compile:\n",
    "\t\tnet = torch.compile(net, dynamic=True)\n",
    "\n",
    "\titer_save = int(cfg.epoch_save * num_train_batch)\n",
    "\titer_valid = int(cfg.epoch_valid * num_train_batch)\n",
    "\titer_log = int(cfg.epoch_log * num_train_batch)\n",
    "\ttrain_loss = MyMeter(None, min(100, num_train_batch))  # window must be less than num_train_batch\n",
    "\tvalid_loss = [0, 0, ]\n",
    "\n",
    "\t# logging\n",
    "\tdef message_header():\n",
    "\t\ttext = ''\n",
    "\t\ttext += f'** start training here! **\\n'\n",
    "\t\ttext += f'   experiment_name = {cfg.experiment_name} \\n'\n",
    "\t\ttext += f'                            |---------- VALID--------------|------ TRAIN/BATCH --------------------\\n'\n",
    "\t\ttext += f'                            |        loss                  | loss              |                    \\n'\n",
    "\t\ttext += f'rate      iter       epoch  | y_acc  level_mask  grade lb  | level_mask  grade |  time  \\n'\n",
    "\t\ttext += f'-------------------------------------------------------------------------------------------------------\\n'\n",
    "\t\t\t    # 5.00e-5  00000521*    1.00  |  0.994  0.468  1.003  0.672  |  0.442  1.089  |  0 hr 03 min :  35 gb\n",
    "\t\ttext = text[:-1]\n",
    "\t\treturn text\n",
    "\n",
    "\tdef message(mode='print'):\n",
    "\t\tif mode == 'print':\n",
    "\t\t\tloss = batch_loss\n",
    "\t\tif mode == 'log':\n",
    "\t\t\tloss = train_loss\n",
    "\n",
    "\t\tif (iteration % iter_save == 0):\n",
    "\t\t\tasterisk = '*'\n",
    "\t\telse:\n",
    "\t\t\tasterisk = ' '\n",
    "\n",
    "\t\tlr = get_learning_rate(optimizer)[0]\n",
    "\t\tlr = short_e_format(f'{lr:0.2e}')\n",
    "\n",
    "\t\ttimestamp = time_to_str(timer() - start_timer, 'min')\n",
    "\t\ttext = ''\n",
    "\t\ttext += f'{lr}  {iteration:08d}{asterisk}  {epoch:6.2f}  |  '\n",
    "\n",
    "\t\tfor v in valid_loss:\n",
    "\t\t\ttext += f'{v:5.3f}  '\n",
    "\t\ttext += f'|  '\n",
    "\n",
    "\t\tfor v in loss:\n",
    "\t\t\ttext += f'{v:5.3f}  '\n",
    "\t\ttext += f'| '\n",
    "\n",
    "\t\ttext += f'{timestamp} : '\n",
    "\t\ttext += f'{get_used_mem():3d} gb'\n",
    "\t\treturn text\n",
    "\n",
    "\t### start training here! ################################################\n",
    "\n",
    "\tdef do_valid(net, iteration):\n",
    "\n",
    "\t\tresult = dotdict(\n",
    "\t\t\tD=[],\n",
    "\n",
    "\t\t\tgrade_loss=0,\n",
    "\t\t\tgrade_truth=[],\n",
    "\t\t\tgrade=[],\n",
    "\n",
    "\t\t\tlevel_mask_loss=0,\n",
    "\t\t\tzxy_loss=0,\n",
    "\t\t\txy_truth=[],\n",
    "\t\t\tz_truth=[],\n",
    "\t\t\txy=[],\n",
    "\t\t\tz=[],\n",
    "\t\t)\n",
    "\t\tnum_valid = 0\n",
    "\t\tstart_timer = timer()\n",
    "\n",
    "\t\tnet.cuda()\n",
    "\t\tnet.eval()\n",
    "\t\tnet.output_type = ['loss', 'infer']\n",
    "\t\tfor t, batch in enumerate(valid_loader):\n",
    "\n",
    "\t\t\twith torch.cuda.amp.autocast(enabled=cfg.is_amp):\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\toutput = net(batch)\n",
    "\n",
    "\n",
    "\t\t\tB = len(batch['index'])\n",
    "\t\t\tnum_valid += B\n",
    "\t\t\tresult.grade_loss += B * output['grade_loss'].item()\n",
    "\t\t\tresult.level_mask_loss += B * output['level_mask_loss'].item()\n",
    "\n",
    "\t\t\tresult.grade_truth.append(batch['grade'].data.cpu().numpy())\n",
    "\t\t\tresult.grade.append(output['grade'].data.cpu().numpy())\n",
    "\n",
    "\t\t\tresult.xy_truth.append(batch['xyz'][...,[0,1]].data.cpu().numpy())\n",
    "\t\t\tresult.xy.append(output['xy'].data.cpu().numpy())\n",
    "\t\t\t#--------------\n",
    "\t\t\tif 0:\n",
    "\t\t\t\t#legacy code (copmute z from level mask)\n",
    "\n",
    "\t\t\t\tD = batch['D']\n",
    "\t\t\t\tlevel_mask  = output['level_mask'].float().data.cpu().numpy()\n",
    "\t\t\t\tlevel_mask_truth  = batch['level_mask'].float().data.cpu().numpy()\n",
    "\t\t\t\tB = len(D)\n",
    "\t\t\t\tfor b in range(B):\n",
    "\t\t\t\t\tg  = level_mask[b] #d,_,h,w\n",
    "\t\t\t\t\tgz = g.sum((2))\n",
    "\t\t\t\t\tp  = gz.argmax(1) #predict\n",
    "\n",
    "\t\t\t\t\tg_truth = level_mask_truth[b]\n",
    "\t\t\t\t\tgz_truth = g_truth.sum((2))\n",
    "\t\t\t\t\tp_truth = gz_truth.argmax(1)\n",
    "\t\t\t\t\tvalid = g_truth.sum((1,2))!=0\n",
    "\n",
    "\t\t\t\t\tresult.z.append(p[valid])\n",
    "\t\t\t\t\tresult.z_truth.append(p_truth[valid])\n",
    "\n",
    "\t\t\t######################################################################\n",
    "\n",
    "\t\t\tprint(f'\\r validation: {num_valid}/{len(valid_dataset)}', time_to_str(timer() - start_timer, 'min'),\n",
    "\t\t\t\t  end='', flush=True)\n",
    "\n",
    "\t\t# ----\n",
    "\t\tgrade_loss = result.grade_loss/ num_valid\n",
    "\t\tlevel_mask_loss = result.level_mask_loss / num_valid\n",
    "\n",
    "\t\tgrade_truth = np.concatenate(result.grade_truth)\n",
    "\t\tgrade = np.concatenate(result.grade)\n",
    "\t\tloss, weighted_loss = do_local_lb(grade, grade_truth)\n",
    "\n",
    "\t\t#check if level is correct\n",
    "\t\txy = np.concatenate(result.xy)\n",
    "\t\txy_truth = np.concatenate(result.xy_truth)\n",
    "\t\tdiff = np.abs(xy[...,1]-xy_truth[...,1])\n",
    "\t\ty_acc = (diff<=2.5).mean()\n",
    "\n",
    "\n",
    "\t\tvalid_loss = [\n",
    "\t\t\ty_acc, level_mask_loss, grade_loss, weighted_loss\n",
    "\t\t]\n",
    "\t\treturn valid_loss\n",
    "\n",
    "\n",
    "\t# ---------------------------------------\n",
    "\titeration = start_iteration\n",
    "\tepoch = start_epoch\n",
    "\tstart_timer = timer()\n",
    "\tlog.write(message_header())\n",
    "\n",
    "\tbreak_while_loop=0\n",
    "\twhile break_while_loop==0:\n",
    "\n",
    "\t\tfor t, batch in enumerate(train_loader):\n",
    "\t\t\t# --- start of callback ---\n",
    "\t\t\tif iteration % iter_save == 0:\n",
    "\t\t\t\tif iteration != start_iteration:\n",
    "\t\t\t\t\ttorch.save({\n",
    "\t\t\t\t\t\t# 'state_dict': net.state_dict(),\n",
    "\t\t\t\t\t\t#'state_dict': getattr(net, '_orig_mod', net).state_dict(),\n",
    "\t\t\t\t\t\t'state_dict': ema.module.state_dict(),\n",
    "\t\t\t\t\t\t'iteration': iteration,\n",
    "\t\t\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t\t}, f'{cfg.fold_dir}/checkpoint/{iteration:08d}.pth')\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "\t\t\tif iteration % iter_valid == 0:\n",
    "\t\t\t\tvalid_loss = do_valid( ema.module, iteration )\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\tif (iteration % iter_log == 0) or (iteration % iter_valid == 0):\n",
    "\t\t\t\tprint('\\r', end='', flush=True)\n",
    "\t\t\t\tlog.write(message(mode='log'))\n",
    "\n",
    "\t\t\tif break_while_loop: break\n",
    "\t\t\t# --- end of callback ----\n",
    "\n",
    "\t\t\tnet.train()\n",
    "\t\t\tnet.output_type = ['loss', 'infer']\n",
    "\n",
    "\t\t\tif 1:# len(batch['xy']) !=0:\n",
    "\t\t\t\twith torch.cuda.amp.autocast(enabled=cfg.is_amp):\n",
    "\t\t\t\t\toutput = net(batch)\n",
    "\n",
    "\t\t\t\t\tlevel_mask_loss = output['level_mask_loss']\n",
    "\t\t\t\t\tgrade_loss = output['grade_loss']\n",
    "\t\t\t\t\tz_mask_loss = output['z_mask_loss']\n",
    "\t\t\t\t\txy_loss = output['xy_loss']\n",
    "\t\t\t\t\tloss =  level_mask_loss  + xy_loss+ z_mask_loss + grade_loss\n",
    "\t\t\t\t\tbatch_loss = [ level_mask_loss.item(), grade_loss.item(),]\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t# if batch['target'].sum()>0:\n",
    "\t\t\t\tif 1:\n",
    "\t\t\t\t\tif cfg.is_amp:\n",
    "\t\t\t\t\t\tscaler.scale(loss).backward()\n",
    "\t\t\t\t\t\t# scaler.unscale_(optimizer)\n",
    "\t\t\t\t\t\t# torch.nn.utils.clip_grad_norm_(net.parameters(), 2)\n",
    "\t\t\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\t\t\tscaler.update()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tema.update(net)\n",
    "\t\t\t\ttorch.clear_autocast_cache()\n",
    "\n",
    "\t\t\t\t# print---\n",
    "\t\t\t\ttrain_loss.step(batch_loss)\n",
    "\t\t\t\tprint('\\r', end='', flush=True)\n",
    "\t\t\t\tprint(message(mode='print'), end='', flush=True)\n",
    "\n",
    "\t\t\titeration += 1\n",
    "\t\t\tepoch += 1 / num_train_batch\n",
    "\t\t\tif epoch > cfg.num_epoch+2 / num_train_batch:\n",
    "\t\t\t\tbreak_while_loop=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32dea8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\t\t# print('')\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "\tfrom configure import *\n",
    "\n",
    "\tcfg = deepcopy(default_cfg)\n",
    "\tcfg.experiment_name = 'one-stage-scs/pvt_v2_b4-decoder2d-01'\n",
    "\tcfg.lr = 5e-5 # 5e-5 # 1e-4\n",
    "\tcfg.num_epoch = 30\n",
    "\tcfg.comment =  'xxx'\n",
    "\n",
    "\tcfg.train_batch_size = 3\n",
    "\tcfg.valid_batch_size = 2\n",
    "\n",
    "\tcfg.level_sigma=1\n",
    "\tcfg.image_size=320\n",
    "\tcfg.mask_size=320//4\n",
    "\tcfg.arch = 'pvt_v2_b4'\n",
    "\t\t#'tf_efficientnet_b4.ns_jft_in1k'\n",
    "\t    #'convnext_base.fb_in22k'\n",
    "     \t# 'pvt_v2_b4'\n",
    "\n",
    "\t#for f in [0,1,2,3,4]:\n",
    "\tfor f in [0,1,2,3,4]:\n",
    "\t\tcfg.fold = f\n",
    "\t\tcfg.fold_dir = f'{RESULT_DIR}/{cfg.experiment_name}/fold-{cfg.fold}'\n",
    "\t\tcfg.resume_from.iteration  = -1\n",
    "\t\tcfg.resume_from.checkpoint = \\\n",
    "\t\t    None #cfg.fold_dir + '/checkpoint/00011067.pth' # fold-4\n",
    "\n",
    "\t\trun_trainer(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
