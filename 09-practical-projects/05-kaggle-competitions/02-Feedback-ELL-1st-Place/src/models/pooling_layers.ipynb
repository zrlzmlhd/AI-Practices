{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f566c8e",
   "metadata": {},
   "source": [
    "# src - pooling_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f3772",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28e30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11fce5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_last_hidden_state(backbone_outputs):\n",
    "    last_hidden_state = backbone_outputs[0]\n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3084d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_all_hidden_states(backbone_outputs):\n",
    "    all_hidden_states = torch.stack(backbone_outputs[1])\n",
    "    return all_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d17aed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_input_ids(inputs):\n",
    "    return inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bc62e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_attention_mask(inputs):\n",
    "    return inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d2318",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        last_hidden_state = get_last_hidden_state(backbone_outputs)\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416e3b8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config, is_lstm=True):\n",
    "        super(LSTMPooling, self).__init__()\n",
    "\n",
    "        self.num_hidden_layers = backbone_config.num_hidden_layers\n",
    "        self.hidden_size = backbone_config.hidden_size\n",
    "        self.hidden_lstm_size = pooling_config.hidden_size\n",
    "        self.dropout_rate = pooling_config.dropout_rate\n",
    "        self.bidirectional = pooling_config.bidirectional\n",
    "\n",
    "        self.is_lstm = is_lstm\n",
    "        self.output_dim = pooling_config.hidden_size*2 if self.bidirectional else pooling_config.hidden_size\n",
    "\n",
    "        if self.is_lstm:\n",
    "            self.lstm = nn.LSTM(self.hidden_size,\n",
    "                                self.hidden_lstm_size,\n",
    "                                bidirectional=self.bidirectional,\n",
    "                                batch_first=True)\n",
    "        else:\n",
    "            self.lstm = nn.GRU(self.hidden_size,\n",
    "                               self.hidden_lstm_size,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97852c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "\n",
    "        self.num_hidden_layers = backbone_config.num_hidden_layers\n",
    "        self.layer_start = pooling_config.layer_start\n",
    "        self.layer_weights = pooling_config.layer_weights if pooling_config.layer_weights is not None else \\\n",
    "            nn.Parameter(torch.tensor([1] * (self.num_hidden_layers + 1 - self.layer_start), dtype=torch.float))\n",
    "\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd24997",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConcatPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(ConcatPooling, self, ).__init__()\n",
    "\n",
    "        self.n_layers = pooling_config.n_layers\n",
    "        self.output_dim = backbone_config.hidden_size*pooling_config.n_layers\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "\n",
    "        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n",
    "        concatenate_pooling = concatenate_pooling[:, 0]\n",
    "        return concatenate_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a7654",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.num_hidden_layers = backbone_config.num_hidden_layers\n",
    "        self.hidden_size = backbone_config.hidden_size\n",
    "        self.hiddendim_fc = pooling_config.hiddendim_fc\n",
    "        self.dropout = nn.Dropout(pooling_config.dropout)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n",
    "        self.q = nn.Parameter(torch.from_numpy(q_t)).float().to(self.device)\n",
    "        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n",
    "        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float().to(self.device)\n",
    "\n",
    "        self.output_dim = self.hiddendim_fc\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out = self.attention(hidden_states)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "    def attention(self, h):\n",
    "        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n",
    "        v = F.softmax(v, -1)\n",
    "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n",
    "        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2ace3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WKPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(WKPooling, self).__init__()\n",
    "\n",
    "        self.layer_start = pooling_config.layer_start\n",
    "        self.context_window_size = pooling_config.context_window_size\n",
    "\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "\n",
    "        ft_all_layers = all_hidden_states\n",
    "        org_device = ft_all_layers.device\n",
    "        all_layer_embedding = ft_all_layers.transpose(1, 0)\n",
    "        all_layer_embedding = all_layer_embedding[:, self.layer_start:, :, :]\n",
    "\n",
    "        all_layer_embedding = all_layer_embedding.cpu()\n",
    "\n",
    "        attention_mask = attention_mask.cpu().numpy()\n",
    "        unmask_num = np.array([sum(mask) for mask in attention_mask]) - 1\n",
    "        embedding = []\n",
    "\n",
    "        for sent_index in range(len(unmask_num)):\n",
    "            sentence_feature = all_layer_embedding[sent_index, :, :unmask_num[sent_index], :]\n",
    "            one_sentence_embedding = []\n",
    "\n",
    "            for token_index in range(sentence_feature.shape[1]):\n",
    "                token_feature = sentence_feature[:, token_index, :]\n",
    "                token_embedding = self.unify_token(token_feature)\n",
    "                one_sentence_embedding.append(token_embedding)\n",
    "\n",
    "            one_sentence_embedding = torch.stack(one_sentence_embedding)\n",
    "            sentence_embedding = self.unify_sentence(sentence_feature, one_sentence_embedding)\n",
    "            embedding.append(sentence_embedding)\n",
    "\n",
    "        output_vector = torch.stack(embedding).to(org_device)\n",
    "        return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000dcc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MeanMaxPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(MeanMaxPooling, self).__init__()\n",
    "        self.feat_mult = 1\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        x = get_input_ids(inputs)\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        sum_embeddings = torch.sum(x * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        embeddings = x.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim=1)\n",
    "        mean_max_embeddings = torch.cat((mean_embeddings, max_embeddings), 1)\n",
    "\n",
    "        return mean_max_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7f5a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        self.feat_mult = 1\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        x = get_input_ids(inputs)\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        embeddings = x.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim=1)\n",
    "        return max_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf97421",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(MinPooling, self).__init__()\n",
    "        self.feat_mult = 1\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        x = get_input_ids(inputs)\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        embeddings = x.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim=1)\n",
    "        return min_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a0bed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(GeMText, self).__init__()\n",
    "\n",
    "        self.dim = pooling_config.dim\n",
    "        self.eps = pooling_config.eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "        self.p = Parameter(torch.ones(1) * pooling_config.p)\n",
    "\n",
    "        self.output_dim = backbone_config.hidden_size\n",
    "\n",
    "    def forward(self, inputs, backbone_output):\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        x = get_input_ids(inputs)\n",
    "\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n",
    "        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb27cd5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pooling_layer(config, backbone_config):\n",
    "    if config.model.pooling_type == 'MeanPooling':\n",
    "        return MeanPooling(backbone_config, config.model.gru_pooling)\n",
    "\n",
    "    elif config.model.pooling_type == 'GRUPooling':\n",
    "        return LSTMPooling(backbone_config, config.model.gru_pooling, is_lstm=False)\n",
    "\n",
    "    elif config.model.pooling_type == 'LSTMPooling':\n",
    "        return LSTMPooling(backbone_config, config.model.lstm_pooling, is_lstm=True)\n",
    "\n",
    "    elif config.model.pooling_type == 'WeightedLayerPooling':\n",
    "        return WeightedLayerPooling(backbone_config, config.model.weighted_pooling)\n",
    "\n",
    "    elif config.model.pooling_type == 'WKPooling':\n",
    "        return WKPooling(backbone_config, config.model.wk_pooling)\n",
    "\n",
    "    elif config.model.pooling_type == 'ConcatPooling':\n",
    "        return ConcatPooling(backbone_config, config.model.concat_pooling)\n",
    "\n",
    "    elif config.model.pooling_type == 'AttentionPooling':\n",
    "        return AttentionPooling(backbone_config, config.model.attention_pooling)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Invalid pooling type: {config.model.pooling_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
