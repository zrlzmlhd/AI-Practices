{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12a740d",
   "metadata": {},
   "source": [
    "# src - parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc1ef7",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eaab7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef756542",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_optimizer_params_with_llrd(model, encoder_lr, decoder_lr, weight_decay=0.0, learning_rate_llrd_mult=1.0):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = []\n",
    "\n",
    "    init_lr = encoder_lr\n",
    "    head_lr = decoder_lr\n",
    "    lr = init_lr\n",
    "    print(f'Learning Rates: \\n\\tHead LR: {init_lr}')\n",
    "\n",
    "    params_0 = [p for n, p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n, p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "\n",
    "    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}\n",
    "    optimizer_parameters.append(head_params)\n",
    "\n",
    "    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": weight_decay}\n",
    "    optimizer_parameters.append(head_params)\n",
    "\n",
    "    for layer in range(24, -1, -1):\n",
    "        params_0 = [p for n, p in named_parameters if f\"encoder.layer.{layer}.\" in n\n",
    "                    and any(nd in n for nd in no_decay)]\n",
    "        params_1 = [p for n, p in named_parameters if f\"encoder.layer.{layer}.\" in n\n",
    "                    and not any(nd in n for nd in no_decay)]\n",
    "\n",
    "        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "        optimizer_parameters.append(layer_params)\n",
    "\n",
    "        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": weight_decay}\n",
    "        optimizer_parameters.append(layer_params)\n",
    "        print(f'\\tLayer {layer} LR: {lr}')\n",
    "        lr *= learning_rate_llrd_mult\n",
    "\n",
    "    print(f'\\tEmbeddings LR: {lr}')\n",
    "    params_0 = [p for n, p in named_parameters if \"embeddings\" in n\n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n, p in named_parameters if \"embeddings\" in n\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "\n",
    "    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "    optimizer_parameters.append(embed_params)\n",
    "\n",
    "    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": weight_decay}\n",
    "    optimizer_parameters.append(embed_params)\n",
    "\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b8fae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def deberta_base_adamw_grouped_llrd(model, encoder_lr, decoder_lr, init_weight_decay, factor):\n",
    "    opt_parameters = []\n",
    "    named_parameters = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    set_6 = [\"layer.0.\", \"layer.1.\", \"layer.2.\", \"layer.3.\"]\n",
    "    set_5 = [\"layer.4.\", \"layer.5.\", \"layer.6.\", \"layer.7.\"]\n",
    "    set_4 = [\"layer.8.\", \"layer.9.\", \"layer.10.\", \"layer.11.\"]\n",
    "    set_3 = [\"layer.12.\", \"layer.13.\", \"layer.14.\", \"layer.15.\"]\n",
    "    set_2 = [\"layer.16.\", \"layer.17.\", \"layer.18.\", \"layer.19.\"]\n",
    "    set_1 = [\"layer.20.\", \"layer.21.\", \"layer.22.\", \"layer.23.\"]\n",
    "    init_lr = 1e-6\n",
    "\n",
    "    for i, (name, params) in enumerate(named_parameters):\n",
    "\n",
    "        weight_decay = 0.0 if any(p in name for p in no_decay) else init_weight_decay\n",
    "\n",
    "        if name.startswith(\"backbone.embeddings\") or name.startswith(\"backbone.encoder\"):\n",
    "            lr = encoder_lr\n",
    "            lr = encoder_lr * factor ** 1 if any(p in name for p in set_1) else lr\n",
    "            lr = encoder_lr * factor ** 2 if any(p in name for p in set_2) else lr\n",
    "            lr = encoder_lr * factor ** 3 if any(p in name for p in set_3) else lr\n",
    "            lr = encoder_lr * factor ** 4 if any(p in name for p in set_4) else lr\n",
    "            lr = encoder_lr * factor ** 5 if any(p in name for p in set_5) else lr\n",
    "            lr = encoder_lr * factor ** 6 if any(p in name for p in set_6) else lr\n",
    "\n",
    "            opt_parameters.append({\"params\": params,\n",
    "                                   \"weight_decay\": weight_decay,\n",
    "                                   \"lr\": lr})\n",
    "\n",
    "        if name.startswith(\"fc\") or name.startswith(\"backbone.pooler\"):\n",
    "            lr = decoder_lr\n",
    "            opt_parameters.append({\"params\": params,\n",
    "                                   \"weight_decay\": weight_decay,\n",
    "                                   \"lr\": lr})\n",
    "\n",
    "    return opt_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff05079",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_parameters_groups(n_layers, n_groups):\n",
    "    layers = [f'backbone.encoder.layer.{n_layers - i - 1}.' for i in range(n_layers)]\n",
    "    step = math.ceil(n_layers / n_groups)\n",
    "    groups = []\n",
    "    for i in range(0, n_layers, step):\n",
    "        if i + step >= n_layers - 1:\n",
    "            group = layers[i:]\n",
    "            groups.append(group)\n",
    "            break\n",
    "        else:\n",
    "            group = layers[i:i + step]\n",
    "            groups.append(group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b506aaa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_grouped_llrd_parameters(model,\n",
    "                                encoder_lr,\n",
    "                                decoder_lr,\n",
    "                                embeddings_lr,\n",
    "                                lr_mult_factor,\n",
    "                                weight_decay,\n",
    "                                n_groups):\n",
    "    opt_parameters = []\n",
    "    named_parameters = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "    n_layers = model.backbone_config.num_hidden_layers\n",
    "    parameters_groups = get_parameters_groups(n_layers, n_groups)\n",
    "\n",
    "    for _, (name, params) in enumerate(named_parameters):\n",
    "\n",
    "        wd = 0.0 if any(p in name for p in no_decay) else weight_decay\n",
    "\n",
    "        if name.startswith(\"backbone.encoder\"):\n",
    "            lr = encoder_lr\n",
    "            for i, group in enumerate(parameters_groups):\n",
    "                lr = encoder_lr * (lr_mult_factor ** (i + 1)) if any(p in name for p in group) else lr\n",
    "\n",
    "            opt_parameters.append({\"params\": params,\n",
    "                                   \"weight_decay\": wd,\n",
    "                                   \"lr\": lr})\n",
    "\n",
    "        if name.startswith(\"backbone.embeddings\"):\n",
    "            lr = embeddings_lr\n",
    "            opt_parameters.append({\"params\": params,\n",
    "                                   \"weight_decay\": wd,\n",
    "                                   \"lr\": lr})\n",
    "\n",
    "        if name.startswith(\"fc\") or name.startswith('backbone.pooler'):\n",
    "            lr = decoder_lr\n",
    "            opt_parameters.append({\"params\": params,\n",
    "                                   \"weight_decay\": wd,\n",
    "                                   \"lr\": lr})\n",
    "\n",
    "    return opt_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f0174",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
