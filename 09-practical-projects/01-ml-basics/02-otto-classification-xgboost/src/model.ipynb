{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02946cc1",
   "metadata": {},
   "source": [
    "# Otto分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d12364",
   "metadata": {},
   "source": [
    "\n",
    "本模块实现：\n",
    "1. XGBoost多分类模型\n",
    "2. LightGBM模型\n",
    "3. CatBoost模型\n",
    "4. Stacking集成\n",
    "5. Voting集成\n",
    "\n",
    "每个模型都有详细的注释说明设计思路和参数选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4613f1c",
   "metadata": {},
   "source": [
    "## 核心算法阵列\n",
    "- `OttoXGBoostClassifier` 支持 basic/tuned/advanced 配置, 通过注释解释深度、采样、正则化、学习率选择。\n",
    "- 同时提供 `OttoLightGBMClassifier`、`OttoCatBoostClassifier` 以及 `OttoStackingEnsemble`, Notebook 可直接切换梯度提升/集成方案。\n",
    "- 集成实现展示如何用 cross_val_predict 生成二级特征以及如何做投票/加权融合。\n",
    "\n",
    "> **算法提示**: 整体目标是最小化 Kaggle Otto 的 multi-class logloss, 因此所有模型都使用 `objective='multi:softprob'` 并监控 `mlogloss`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Otto分类模型\n",
    "\n",
    "本模块实现：\n",
    "1. XGBoost多分类模型\n",
    "2. LightGBM模型\n",
    "3. CatBoost模型\n",
    "4. Stacking集成\n",
    "5. Voting集成\n",
    "\n",
    "每个模型都有详细的注释说明设计思路和参数选择。\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pickle\n",
    "\n",
    "\n",
    "class OttoXGBoostClassifier:\n",
    "    \"\"\"\n",
    "    XGBoost多分类器\n",
    "\n",
    "    【是什么】：基于XGBoost的多分类模型\n",
    "    【做什么】：预测产品属于9个类别中的哪一个\n",
    "    【为什么】：\n",
    "        - XGBoost在表格数据上表现优秀\n",
    "        - 支持多分类\n",
    "        - 可以输出概率\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=9, model_type='tuned', **kwargs):\n",
    "        \"\"\"\n",
    "        初始化分类器\n",
    "\n",
    "        Args:\n",
    "            n_classes: 类别数\n",
    "            model_type: 模型类型 ('basic', 'tuned', 'advanced')\n",
    "            **kwargs: 其他参数\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # 根据模型类型设置参数\n",
    "        self.params = self._get_model_params(model_type)\n",
    "        self.params.update(kwargs)\n",
    "\n",
    "        # 模型\n",
    "        self.model = None\n",
    "\n",
    "    def _get_model_params(self, model_type):\n",
    "        \"\"\"\n",
    "        获取模型参数\n",
    "\n",
    "        Args:\n",
    "            model_type: 模型类型\n",
    "\n",
    "        Returns:\n",
    "            参数字典\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'basic': {\n",
    "                # ============================================\n",
    "                # 基础配置\n",
    "                # ============================================\n",
    "                'objective': 'multi:softprob',\n",
    "                # 【是什么】：多分类 + 输出概率\n",
    "                # 【为什么】：\n",
    "                #   - multi:softprob: 输出每个类别的概率\n",
    "                #   - multi:softmax: 只输出类别（不适合Stacking）\n",
    "\n",
    "                'num_class': self.n_classes,\n",
    "                # 【是什么】：类别数量\n",
    "                # 【为什么】：XGBoost需要知道有多少个类别\n",
    "\n",
    "                'eval_metric': 'mlogloss',\n",
    "                # 【是什么】：多分类对数损失\n",
    "                # 【为什么】：\n",
    "                #   - Otto竞赛的评估指标\n",
    "                #   - 衡量概率预测的准确性\n",
    "\n",
    "                # 树的参数\n",
    "                'max_depth': 6,\n",
    "                # 【为什么=6】：\n",
    "                #   - 中等深度\n",
    "                #   - 平衡欠拟合和过拟合\n",
    "\n",
    "                'min_child_weight': 1,\n",
    "                # 【为什么=1】：\n",
    "                #   - 允许较小的叶子节点\n",
    "                #   - 捕获细节模式\n",
    "\n",
    "                # 提升参数\n",
    "                'learning_rate': 0.1,\n",
    "                # 【为什么=0.1】：\n",
    "                #   - 标准学习率\n",
    "                #   - 平衡速度和性能\n",
    "\n",
    "                'n_estimators': 100,\n",
    "                # 【为什么=100】：\n",
    "                #   - 基础配置，快速实验\n",
    "                #   - 可以通过early_stopping调整\n",
    "\n",
    "                # 正则化\n",
    "                'reg_alpha': 0,\n",
    "                'reg_lambda': 1,\n",
    "                # 【为什么】：\n",
    "                #   - 默认L2正则化\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                # 其他\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "            },\n",
    "\n",
    "            'tuned': {\n",
    "                # ============================================\n",
    "                # 调优配置\n",
    "                # ============================================\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': self.n_classes,\n",
    "                'eval_metric': 'mlogloss',\n",
    "\n",
    "                # 树的参数\n",
    "                'max_depth': 8,\n",
    "                # 【为什么=8】：\n",
    "                #   - 更深的树捕获更复杂的模式\n",
    "                #   - Otto数据特征多，需要更深的树\n",
    "\n",
    "                'min_child_weight': 3,\n",
    "                # 【为什么=3】：\n",
    "                #   - 增加正则化\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                'gamma': 0.1,\n",
    "                # 【是什么】：分裂所需的最小损失减少\n",
    "                # 【为什么=0.1】：\n",
    "                #   - 轻度剪枝\n",
    "                #   - 去除不重要的分裂\n",
    "\n",
    "                # 提升参数\n",
    "                'learning_rate': 0.05,\n",
    "                # 【为什么=0.05】：\n",
    "                #   - 较小的学习率\n",
    "                #   - 配合更多的树\n",
    "\n",
    "                'n_estimators': 500,\n",
    "                # 【为什么=500】：\n",
    "                #   - 更多的树\n",
    "                #   - 配合early_stopping\n",
    "\n",
    "                # 采样参数\n",
    "                'subsample': 0.8,\n",
    "                # 【是什么】：每棵树使用80%的样本\n",
    "                # 【为什么=0.8】：\n",
    "                #   - 增加随机性\n",
    "                #   - 防止过拟合\n",
    "                #   - 加速训练\n",
    "\n",
    "                'colsample_bytree': 0.8,\n",
    "                # 【是什么】：每棵树使用80%的特征\n",
    "                # 【为什么=0.8】：\n",
    "                #   - 特征采样\n",
    "                #   - 增加模型多样性\n",
    "\n",
    "                # 正则化\n",
    "                'reg_alpha': 0.1,\n",
    "                # 【是什么】：L1正则化\n",
    "                # 【为什么=0.1】：\n",
    "                #   - 特征选择\n",
    "                #   - 稀疏解\n",
    "\n",
    "                'reg_lambda': 1.0,\n",
    "                # 【是什么】：L2正则化\n",
    "                # 【为什么=1.0】：\n",
    "                #   - 权重平滑\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                # 其他\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "            },\n",
    "\n",
    "            'advanced': {\n",
    "                # ============================================\n",
    "                # 高级配置\n",
    "                # ============================================\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': self.n_classes,\n",
    "                'eval_metric': 'mlogloss',\n",
    "\n",
    "                # 树的参数\n",
    "                'max_depth': 10,\n",
    "                # 【为什么=10】：\n",
    "                #   - 深树捕获复杂交互\n",
    "                #   - 配合强正则化\n",
    "\n",
    "                'min_child_weight': 5,\n",
    "                # 【为什么=5】：\n",
    "                #   - 强正则化\n",
    "                #   - 平衡深度\n",
    "\n",
    "                'gamma': 0.2,\n",
    "                # 【为什么=0.2】：\n",
    "                #   - 更强的剪枝\n",
    "                #   - 去除噪声分裂\n",
    "\n",
    "                # 提升参数\n",
    "                'learning_rate': 0.03,\n",
    "                # 【为什么=0.03】：\n",
    "                #   - 小学习率\n",
    "                #   - 精细调整\n",
    "\n",
    "                'n_estimators': 1000,\n",
    "                # 【为什么=1000】：\n",
    "                #   - 大量的树\n",
    "                #   - 充分学习\n",
    "\n",
    "                # 采样参数\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                # 【为什么都=0.7】：\n",
    "                #   - 强随机性\n",
    "                #   - 防止过拟合\n",
    "                #   - 增加多样性\n",
    "\n",
    "                # 正则化\n",
    "                'reg_alpha': 0.5,\n",
    "                'reg_lambda': 2.0,\n",
    "                # 【为什么更大】：\n",
    "                #   - 深树需要强正则化\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                # 其他\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return params.get(model_type, params['tuned'])\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              early_stopping_rounds=50, verbose=True):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练特征\n",
    "            y_train: 训练标签\n",
    "            X_val: 验证特征\n",
    "            y_val: 验证标签\n",
    "            early_stopping_rounds: 早停轮数\n",
    "            verbose: 是否显示训练过程\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        # 准备验证集\n",
    "        eval_set = [(X_train, y_train)]\n",
    "        if X_val is not None and y_val is not None:\n",
    "            eval_set.append((X_val, y_val))\n",
    "\n",
    "        # 创建模型\n",
    "        self.model = xgb.XGBClassifier(**self.params)\n",
    "\n",
    "        # 训练\n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=eval_set,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return self.model.evals_result()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        评估模型\n",
    "\n",
    "        Args:\n",
    "            X: 特征\n",
    "            y: 标签\n",
    "\n",
    "        Returns:\n",
    "            评估指标字典\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'log_loss': log_loss(y, y_pred_proba)\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def get_feature_importance(self, top_n=20):\n",
    "        \"\"\"获取特征重要性\"\"\"\n",
    "        importance = self.model.feature_importances_\n",
    "        indices = np.argsort(importance)[::-1][:top_n]\n",
    "\n",
    "        import pandas as pd\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': [f'feat_{i}' for i in indices],\n",
    "            'importance': importance[indices]\n",
    "        })\n",
    "\n",
    "        return importance_df\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")\n",
    "\n",
    "\n",
    "class OttoLightGBMClassifier:\n",
    "    \"\"\"\n",
    "    LightGBM多分类器\n",
    "\n",
    "    【LightGBM vs XGBoost】：\n",
    "    - LightGBM: 基于直方图的算法，训练更快\n",
    "    - XGBoost: 基于预排序，精度可能更高\n",
    "    - LightGBM: 内存占用更少\n",
    "    - XGBoost: 更成熟，文档更完善\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=9, **kwargs):\n",
    "        \"\"\"初始化分类器\"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        self.params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': n_classes,\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': 8,\n",
    "            'min_child_samples': 20,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        self.params.update(kwargs)\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              num_boost_round=500, early_stopping_rounds=50):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "        valid_sets = [train_data]\n",
    "        valid_names = ['train']\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            valid_sets.append(val_data)\n",
    "            valid_names.append('valid')\n",
    "\n",
    "        self.model = lgb.train(\n",
    "            self.params,\n",
    "            train_data,\n",
    "            num_boost_round=num_boost_round,\n",
    "            valid_sets=valid_sets,\n",
    "            valid_names=valid_names,\n",
    "            callbacks=[lgb.early_stopping(early_stopping_rounds)]\n",
    "        )\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'log_loss': log_loss(y, y_pred_proba)\n",
    "        }\n",
    "\n",
    "\n",
    "class OttoCatBoostClassifier:\n",
    "    \"\"\"\n",
    "    CatBoost多分类器\n",
    "\n",
    "    【CatBoost特点】：\n",
    "    - 自动处理类别特征（Otto数据都是数值，用不上）\n",
    "    - 对称树结构\n",
    "    - 有序提升（Ordered Boosting）\n",
    "    - 训练速度适中\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=9, **kwargs):\n",
    "        \"\"\"初始化分类器\"\"\"\n",
    "        self.params = {\n",
    "            'loss_function': 'MultiClass',\n",
    "            'classes_count': n_classes,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 8,\n",
    "            'l2_leaf_reg': 3.0,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "        }\n",
    "        self.params.update(kwargs)\n",
    "        self.model = cb.CatBoostClassifier(**self.params)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              iterations=500, early_stopping_rounds=50):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        eval_set = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            eval_set = (X_val, y_val)\n",
    "\n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=eval_set,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'log_loss': log_loss(y, y_pred_proba)\n",
    "        }\n",
    "\n",
    "\n",
    "class OttoStackingEnsemble:\n",
    "    \"\"\"\n",
    "    Stacking集成\n",
    "\n",
    "    【是什么】：两层模型结构\n",
    "    【第一层】：多个基模型（XGBoost、LightGBM、CatBoost）\n",
    "    【第二层】：元模型（LogisticRegression）学习如何组合基模型\n",
    "\n",
    "    【为什么Stacking更强】：\n",
    "    - 不同模型有不同的偏差\n",
    "    - 元模型学习每个模型的优势\n",
    "    - 比简单平均更智能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=9):\n",
    "        \"\"\"初始化集成模型\"\"\"\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # 第一层：基模型\n",
    "        self.base_models = {\n",
    "            'xgboost': OttoXGBoostClassifier(n_classes, model_type='tuned'),\n",
    "            'lightgbm': OttoLightGBMClassifier(n_classes),\n",
    "            'catboost': OttoCatBoostClassifier(n_classes)\n",
    "        }\n",
    "\n",
    "        # 第二层：元模型\n",
    "        self.meta_model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # 存储基模型的预测\n",
    "        self.base_predictions = {}\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, cv_folds=None):\n",
    "        \"\"\"\n",
    "        训练Stacking集成\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练特征\n",
    "            y_train: 训练标签\n",
    "            X_val: 验证特征\n",
    "            y_val: 验证标签\n",
    "            cv_folds: 交叉验证折（用于生成元特征）\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        print(\"\\n训练Stacking集成...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 训练基模型\n",
    "        # ============================================\n",
    "        print(\"\\n步骤1: 训练基模型\")\n",
    "\n",
    "        for name, model in self.base_models.items():\n",
    "            print(f\"\\n训练 {name}...\")\n",
    "            model.train(X_train, y_train, X_val, y_val, verbose=False)\n",
    "\n",
    "            # 评估\n",
    "            metrics = model.evaluate(X_val, y_val)\n",
    "            print(f\"  验证集 - Accuracy: {metrics['accuracy']:.4f}, Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 生成元特征\n",
    "        # ============================================\n",
    "        print(\"\\n步骤2: 生成元特征\")\n",
    "\n",
    "        # 使用验证集生成元特征（Blending方式）\n",
    "        meta_features_val = self._generate_meta_features(X_val)\n",
    "        print(f\"  元特征形状: {meta_features_val.shape}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 训练元模型\n",
    "        # ============================================\n",
    "        print(\"\\n步骤3: 训练元模型\")\n",
    "        self.meta_model.fit(meta_features_val, y_val)\n",
    "        print(\"  元模型训练完成\")\n",
    "\n",
    "        return self.base_models\n",
    "\n",
    "    def _generate_meta_features(self, X):\n",
    "        \"\"\"\n",
    "        生成元特征\n",
    "\n",
    "        【是什么】：基模型的预测概率作为新特征\n",
    "        【形状】：(n_samples, n_models * n_classes)\n",
    "\n",
    "        Args:\n",
    "            X: 原始特征\n",
    "\n",
    "        Returns:\n",
    "            元特征\n",
    "        \"\"\"\n",
    "        meta_features = []\n",
    "\n",
    "        for name, model in self.base_models.items():\n",
    "            # 获取每个基模型的预测概率\n",
    "            proba = model.predict_proba(X)\n",
    "            meta_features.append(proba)\n",
    "\n",
    "        # 拼接所有基模型的预测\n",
    "        # 例如：3个模型 × 9个类别 = 27个元特征\n",
    "        return np.hstack(meta_features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        # 生成元特征\n",
    "        meta_features = self._generate_meta_features(X)\n",
    "\n",
    "        # 元模型预测\n",
    "        return self.meta_model.predict_proba(meta_features)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'log_loss': log_loss(y, y_pred_proba)\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Otto分类模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建模拟数据\n",
    "    n_samples = 1000\n",
    "    n_features = 93\n",
    "    n_classes = 9\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.rand(n_samples, n_features)\n",
    "    y_train = np.random.randint(0, n_classes, n_samples)\n",
    "    X_val = np.random.rand(200, n_features)\n",
    "    y_val = np.random.randint(0, n_classes, 200)\n",
    "\n",
    "    # 测试XGBoost\n",
    "    print(\"\\n测试XGBoost...\")\n",
    "    xgb_clf = OttoXGBoostClassifier(n_classes, model_type='basic')\n",
    "    xgb_clf.train(X_train, y_train, X_val, y_val, verbose=False)\n",
    "    metrics = xgb_clf.evaluate(X_val, y_val)\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "    # 测试LightGBM\n",
    "    print(\"\\n测试LightGBM...\")\n",
    "    lgb_clf = OttoLightGBMClassifier(n_classes)\n",
    "    lgb_clf.train(X_train, y_train, X_val, y_val, num_boost_round=100)\n",
    "    metrics = lgb_clf.evaluate(X_val, y_val)\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "    # 测试CatBoost\n",
    "    print(\"\\n测试CatBoost...\")\n",
    "    cb_clf = OttoCatBoostClassifier(n_classes)\n",
    "    cb_clf.train(X_train, y_train, X_val, y_val, iterations=100)\n",
    "    metrics = cb_clf.evaluate(X_val, y_val)\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "    # 测试Stacking\n",
    "    print(\"\\n测试Stacking...\")\n",
    "    stacking = OttoStackingEnsemble(n_classes)\n",
    "    stacking.train(X_train, y_train, X_val, y_val)\n",
    "    metrics = stacking.evaluate(X_val, y_val)\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
