{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc6d94e",
   "metadata": {},
   "source": [
    "# Otto分类数据处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473d65d",
   "metadata": {},
   "source": [
    "\n",
    "本模块负责：\n",
    "1. 加载Otto数据集\n",
    "2. 数据预处理和特征工程\n",
    "3. 数据集划分\n",
    "4. 特征统计分析\n",
    "\n",
    "每个步骤都有详细的注释说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f586258",
   "metadata": {},
   "source": [
    "## 数据处理与特征工程\n",
    "1. 从 Otto Group CSV 读取 93 个匿名特征, 检查缺失值与类别分布。\n",
    "2. 删除 id 列, 对 target 做 LabelEncoder, 确保多分类标签连续。\n",
    "3. `create_features` 构造行统计、交互、多项式等工程特征, 并可切换是否启用。\n",
    "4. 使用 StandardScaler/StratifiedKFold 做数值缩放与数据划分, 最终返回 train/val/test 以及 processor 对象。\n",
    "5. processor 负责保存编码器、缩放器与特征名, Notebook 中演示如何序列化以便复现。\n",
    "\n",
    "> **核心提示**: 每个特征工程步骤都带有\"是什么\"/\"为什么\"说明, 方便在 Notebook 中逐步实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Otto分类数据处理模块\n",
    "\n",
    "本模块负责：\n",
    "1. 加载Otto数据集\n",
    "2. 数据预处理和特征工程\n",
    "3. 数据集划分\n",
    "4. 特征统计分析\n",
    "\n",
    "每个步骤都有详细的注释说明。\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "class OttoDataProcessor:\n",
    "    \"\"\"\n",
    "    Otto数据处理器\n",
    "\n",
    "    【是什么】：处理Otto多分类数据的工具类\n",
    "    【做什么】：\n",
    "        - 加载和清洗数据\n",
    "        - 特征工程\n",
    "        - 数据划分\n",
    "        - 标签编码\n",
    "    【为什么】：\n",
    "        - Otto数据需要特殊处理\n",
    "        - 多分类问题需要标签编码\n",
    "        - 特征工程提升性能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        初始化数据处理器\n",
    "\n",
    "        Args:\n",
    "            data_path: 数据文件路径\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # 数据\n",
    "        self.df = None\n",
    "        self.feature_names = None\n",
    "        self.target_name = 'target'\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        加载数据\n",
    "\n",
    "        Returns:\n",
    "            DataFrame\n",
    "        \"\"\"\n",
    "        print(\"\\n加载Otto数据集...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 读取CSV文件\n",
    "        # ============================================\n",
    "        # 【是什么】：Otto Group Product Classification数据\n",
    "        # 【格式】：CSV文件，包含id、93个特征、1个目标列\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "\n",
    "        print(f\"  原始数据形状: {self.df.shape}\")\n",
    "        print(f\"  特征数量: {self.df.shape[1] - 2}\")  # 减去id和target\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 检查数据\n",
    "        # ============================================\n",
    "        print(f\"\\n数据概览:\")\n",
    "        print(f\"  列名: {self.df.columns.tolist()[:5]}...\")\n",
    "        print(f\"  前几行:\\n{self.df.head(3)}\")\n",
    "\n",
    "        # 检查缺失值\n",
    "        missing_count = self.df.isnull().sum().sum()\n",
    "        print(f\"  缺失值: {missing_count}\")\n",
    "\n",
    "        # 检查目标分布\n",
    "        if self.target_name in self.df.columns:\n",
    "            print(f\"\\n目标分布:\")\n",
    "            target_counts = self.df[self.target_name].value_counts().sort_index()\n",
    "            for target, count in target_counts.items():\n",
    "                percentage = count / len(self.df) * 100\n",
    "                print(f\"    {target}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        预处理数据\n",
    "\n",
    "        Returns:\n",
    "            X, y: 特征和标签\n",
    "        \"\"\"\n",
    "        print(\"\\n预处理数据...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 分离特征和标签\n",
    "        # ============================================\n",
    "        # 【是什么】：X是特征，y是目标\n",
    "\n",
    "        # 移除id列（如果存在）\n",
    "        if 'id' in self.df.columns:\n",
    "            self.df = self.df.drop('id', axis=1)\n",
    "\n",
    "        # 分离特征和标签\n",
    "        X = self.df.drop(self.target_name, axis=1)\n",
    "        y = self.df[self.target_name]\n",
    "\n",
    "        self.feature_names = X.columns.tolist()\n",
    "\n",
    "        print(f\"  特征形状: {X.shape}\")\n",
    "        print(f\"  标签形状: {y.shape}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 标签编码\n",
    "        # ============================================\n",
    "        # 【是什么】：将Class_1, Class_2, ... 转换为 0, 1, ...\n",
    "        # 【为什么】：\n",
    "        #   - XGBoost需要数值标签\n",
    "        #   - 从0开始的连续整数\n",
    "        #   - 便于计算和评估\n",
    "\n",
    "        # 原始标签：['Class_1', 'Class_2', ..., 'Class_9']\n",
    "        # 编码后：[0, 1, 2, ..., 8]\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "\n",
    "        print(f\"\\n标签编码:\")\n",
    "        print(f\"  原始标签: {self.label_encoder.classes_}\")\n",
    "        print(f\"  编码范围: 0 到 {len(self.label_encoder.classes_) - 1}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 特征统计\n",
    "        # ============================================\n",
    "        print(f\"\\n特征统计:\")\n",
    "        print(f\"  特征数量: {X.shape[1]}\")\n",
    "        print(f\"  特征范围: {X.min().min():.2f} 到 {X.max().max():.2f}\")\n",
    "        print(f\"  特征均值: {X.mean().mean():.2f}\")\n",
    "        print(f\"  特征标准差: {X.std().mean():.2f}\")\n",
    "\n",
    "        return X.values, y_encoded\n",
    "\n",
    "    def create_features(self, X):\n",
    "        \"\"\"\n",
    "        创建工程特征\n",
    "\n",
    "        【是什么】：基于原始特征创建新特征\n",
    "        【为什么】：\n",
    "            - 提升模型性能\n",
    "            - 捕获特征间的关系\n",
    "            - 增加模型的表达能力\n",
    "\n",
    "        Args:\n",
    "            X: 原始特征\n",
    "\n",
    "        Returns:\n",
    "            增强后的特征\n",
    "        \"\"\"\n",
    "        print(\"\\n创建工程特征...\")\n",
    "\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names)\n",
    "\n",
    "        # ============================================\n",
    "        # 特征1: 统计特征\n",
    "        # ============================================\n",
    "        # 【是什么】：每行的统计量\n",
    "        # 【为什么】：\n",
    "        #   - 捕获整体模式\n",
    "        #   - 不同产品可能有不同的统计特性\n",
    "\n",
    "        # 行求和\n",
    "        X_df['feat_sum'] = X_df.sum(axis=1)\n",
    "        # 【含义】：所有特征的总和\n",
    "        # 【用途】：某些产品可能总体数值更高\n",
    "\n",
    "        # 行均值\n",
    "        X_df['feat_mean'] = X_df.mean(axis=1)\n",
    "        # 【含义】：平均特征值\n",
    "        # 【用途】：标准化的总体水平\n",
    "\n",
    "        # 行标准差\n",
    "        X_df['feat_std'] = X_df.std(axis=1)\n",
    "        # 【含义】：特征的变化程度\n",
    "        # 【用途】：某些产品特征更分散\n",
    "\n",
    "        # 行最大值\n",
    "        X_df['feat_max'] = X_df.max(axis=1)\n",
    "        # 【含义】：最显著的特征\n",
    "        # 【用途】：某些产品有突出特征\n",
    "\n",
    "        # 行最小值\n",
    "        X_df['feat_min'] = X_df.min(axis=1)\n",
    "        # 【含义】：最弱的特征\n",
    "        # 【用途】：特征的下界\n",
    "\n",
    "        # ============================================\n",
    "        # 特征2: 非零特征数\n",
    "        # ============================================\n",
    "        # 【是什么】：有多少个特征不为0\n",
    "        # 【为什么】：\n",
    "        #   - Otto特征是计数型\n",
    "        #   - 非零特征数可能有区分度\n",
    "        X_df['feat_nonzero'] = (X_df[self.feature_names] != 0).sum(axis=1)\n",
    "\n",
    "        # ============================================\n",
    "        # 特征3: 特征比率\n",
    "        # ============================================\n",
    "        # 【是什么】：最大值/均值\n",
    "        # 【为什么】：\n",
    "        #   - 衡量特征的集中度\n",
    "        #   - 某些产品可能特征更集中\n",
    "        X_df['feat_max_mean_ratio'] = X_df['feat_max'] / (X_df['feat_mean'] + 1e-5)\n",
    "\n",
    "        print(f\"  原始特征数: {len(self.feature_names)}\")\n",
    "        print(f\"  新增特征数: {X_df.shape[1] - len(self.feature_names)}\")\n",
    "        print(f\"  总特征数: {X_df.shape[1]}\")\n",
    "\n",
    "        return X_df.values\n",
    "\n",
    "    def split_data(self, X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "        \"\"\"\n",
    "        划分数据集\n",
    "\n",
    "        【重要】：多分类问题使用分层划分\n",
    "        【为什么】：\n",
    "            - 保持各类别的比例\n",
    "            - 避免某些类别在验证集/测试集中缺失\n",
    "            - 更准确的性能评估\n",
    "\n",
    "        Args:\n",
    "            X: 特征\n",
    "            y: 标签\n",
    "            test_size: 测试集比例\n",
    "            val_size: 验证集比例（从训练集中划分）\n",
    "            random_state: 随机种子\n",
    "\n",
    "        Returns:\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "        \"\"\"\n",
    "        print(\"\\n划分数据集...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 划分训练集和测试集\n",
    "        # ============================================\n",
    "        # 【stratify=y】：分层划分，保持类别比例\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=y  # 关键：保持类别比例\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 从训练集中划分验证集\n",
    "        # ============================================\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # 调整比例\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val,\n",
    "            test_size=val_size_adjusted,\n",
    "            random_state=random_state,\n",
    "            stratify=y_train_val\n",
    "        )\n",
    "\n",
    "        print(f\"  训练集: {X_train.shape}\")\n",
    "        print(f\"  验证集: {X_val.shape}\")\n",
    "        print(f\"  测试集: {X_test.shape}\")\n",
    "\n",
    "        # 检查类别分布\n",
    "        print(f\"\\n类别分布检查:\")\n",
    "        for split_name, y_split in [('训练集', y_train), ('验证集', y_val), ('测试集', y_test)]:\n",
    "            unique, counts = np.unique(y_split, return_counts=True)\n",
    "            print(f\"  {split_name}: {len(unique)}个类别\")\n",
    "\n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "    def get_cv_folds(self, X, y, n_splits=5, random_state=42):\n",
    "        \"\"\"\n",
    "        获取交叉验证折\n",
    "\n",
    "        【是什么】：StratifiedKFold交叉验证\n",
    "        【为什么】：\n",
    "            - 用于Stacking集成\n",
    "            - 充分利用训练数据\n",
    "            - 保持类别比例\n",
    "\n",
    "        Args:\n",
    "            X: 特征\n",
    "            y: 标签\n",
    "            n_splits: 折数\n",
    "            random_state: 随机种子\n",
    "\n",
    "        Returns:\n",
    "            StratifiedKFold对象\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=True,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        print(f\"\\n创建{n_splits}折交叉验证\")\n",
    "        print(f\"  每折训练集: ~{len(X) * (n_splits-1) / n_splits:.0f}样本\")\n",
    "        print(f\"  每折验证集: ~{len(X) / n_splits:.0f}样本\")\n",
    "\n",
    "        return skf\n",
    "\n",
    "    def inverse_transform_labels(self, y_encoded):\n",
    "        \"\"\"\n",
    "        反编码标签\n",
    "\n",
    "        【是什么】：将数值标签转换回原始标签\n",
    "        【为什么】：\n",
    "            - 便于理解预测结果\n",
    "            - 提交Kaggle需要原始标签\n",
    "\n",
    "        Args:\n",
    "            y_encoded: 编码后的标签\n",
    "\n",
    "        Returns:\n",
    "            原始标签\n",
    "        \"\"\"\n",
    "        return self.label_encoder.inverse_transform(y_encoded)\n",
    "\n",
    "    def save_processor(self, filepath):\n",
    "        \"\"\"保存数据处理器\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'label_encoder': self.label_encoder,\n",
    "                'scaler': self.scaler,\n",
    "                'feature_names': self.feature_names\n",
    "            }, f)\n",
    "        print(f\"✓ 数据处理器已保存: {filepath}\")\n",
    "\n",
    "    def load_processor(self, filepath):\n",
    "        \"\"\"加载数据处理器\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.label_encoder = data['label_encoder']\n",
    "            self.scaler = data['scaler']\n",
    "            self.feature_names = data['feature_names']\n",
    "        print(f\"✓ 数据处理器已加载: {filepath}\")\n",
    "\n",
    "\n",
    "def prepare_otto_data(data_path='data/train.csv',\n",
    "                      test_size=0.2,\n",
    "                      val_size=0.1,\n",
    "                      random_state=42,\n",
    "                      create_features=True):\n",
    "    \"\"\"\n",
    "    准备Otto分类数据\n",
    "\n",
    "    Args:\n",
    "        data_path: 数据文件路径\n",
    "        test_size: 测试集比例\n",
    "        val_size: 验证集比例\n",
    "        random_state: 随机种子\n",
    "        create_features: 是否创建工程特征\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), processor\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Otto分类数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤1: 创建数据处理器\n",
    "    # ============================================\n",
    "    processor = OttoDataProcessor(data_path=data_path)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤2: 加载数据\n",
    "    # ============================================\n",
    "    processor.load_data()\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤3: 预处理\n",
    "    # ============================================\n",
    "    X, y = processor.preprocess_data()\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤4: 特征工程\n",
    "    # ============================================\n",
    "    if create_features:\n",
    "        X = processor.create_features(X)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤5: 划分数据集\n",
    "    # ============================================\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = processor.split_data(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        val_size=val_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 数据统计\n",
    "    # ============================================\n",
    "    print(\"\\n数据统计:\")\n",
    "    print(f\"  特征数: {X_train.shape[1]}\")\n",
    "    print(f\"  类别数: {len(np.unique(y))}\")\n",
    "    print(f\"  训练样本数: {len(X_train)}\")\n",
    "    print(f\"  验证样本数: {len(X_val)}\")\n",
    "    print(f\"  测试样本数: {len(X_test)}\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), processor\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试数据处理\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建模拟数据\n",
    "    print(\"\\n创建模拟数据...\")\n",
    "    n_samples = 1000\n",
    "    n_features = 93\n",
    "    n_classes = 9\n",
    "\n",
    "    # 模拟Otto数据\n",
    "    np.random.seed(42)\n",
    "    X_mock = np.random.randint(0, 100, (n_samples, n_features))\n",
    "    y_mock = np.random.randint(0, n_classes, n_samples)\n",
    "    y_mock_labels = [f'Class_{i+1}' for i in y_mock]\n",
    "\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(X_mock, columns=[f'feat_{i}' for i in range(n_features)])\n",
    "    df['id'] = range(n_samples)\n",
    "    df['target'] = y_mock_labels\n",
    "\n",
    "    # 保存临时文件\n",
    "    temp_path = 'temp_otto_data.csv'\n",
    "    df.to_csv(temp_path, index=False)\n",
    "\n",
    "    # 测试数据处理\n",
    "    try:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), processor = prepare_otto_data(\n",
    "            data_path=temp_path,\n",
    "            test_size=0.2,\n",
    "            val_size=0.1,\n",
    "            create_features=True\n",
    "        )\n",
    "\n",
    "        print(\"\\n✓ 数据处理测试通过！\")\n",
    "\n",
    "        # 测试标签反编码\n",
    "        print(\"\\n测试标签反编码...\")\n",
    "        y_original = processor.inverse_transform_labels(y_test[:5])\n",
    "        print(f\"  编码标签: {y_test[:5]}\")\n",
    "        print(f\"  原始标签: {y_original}\")\n",
    "\n",
    "        # 测试交叉验证\n",
    "        print(\"\\n测试交叉验证...\")\n",
    "        skf = processor.get_cv_folds(X_train, y_train, n_splits=5)\n",
    "        print(f\"  交叉验证折数: {skf.get_n_splits()}\")\n",
    "\n",
    "    finally:\n",
    "        # 清理临时文件\n",
    "        import os\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
