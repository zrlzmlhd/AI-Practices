# XGBoost Ottoåˆ†ç±»æŒ‘æˆ˜ - ä¸­çº§é¡¹ç›®

**éš¾åº¦**: â­â­â­â˜†â˜† (ä¸­çº§)

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®ä½¿ç”¨XGBoostã€LightGBMã€CatBoostç­‰æ¢¯åº¦æå‡ç®—æ³•è¿›è¡Œå¤šåˆ†ç±»ï¼Œå¹¶é€šè¿‡Stackingé›†æˆæå‡æ€§èƒ½ã€‚è¿™æ˜¯Kaggleç»å…¸ç«èµ›ï¼Œä½ å°†å­¦ä¹ æ¨¡å‹é›†æˆã€è¶…å‚æ•°è°ƒä¼˜ã€ä»¥åŠå¦‚ä½•è¾¾åˆ°ç«èµ›çº§åˆ«çš„æ€§èƒ½ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- âœ… æŒæ¡å¤šåˆ†ç±»é—®é¢˜çš„XGBooståº”ç”¨
- âœ… å­¦ä¹ XGBoostã€LightGBMã€CatBoostçš„å¯¹æ¯”
- âœ… æŒæ¡Stackingå’ŒBlendingé›†æˆæŠ€æœ¯
- âœ… å­¦ä¹ Optunaè‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- âœ… ç†è§£ä¸ºä»€ä¹ˆæ¨¡å‹é›†æˆèƒ½æå‡æ€§èƒ½

## ğŸ§  ä¸ºä»€ä¹ˆä½¿ç”¨æ¨¡å‹é›†æˆï¼Ÿ

### å•æ¨¡å‹çš„å±€é™

```
XGBoostå•æ¨¡å‹: å‡†ç¡®ç‡ 78%
              â†“
é—®é¢˜: æ¯ä¸ªæ¨¡å‹éƒ½æœ‰åå·®
     XGBoostå¯èƒ½åœ¨æŸäº›æ ·æœ¬ä¸Šè¡¨ç°å·®
              â†“
è§£å†³: é›†æˆå¤šä¸ªä¸åŒçš„æ¨¡å‹
```

### é›†æˆå­¦ä¹ çš„ä¼˜åŠ¿

```
XGBoost:    78% (æ“…é•¿æ•è·éçº¿æ€§å…³ç³»)
LightGBM:   77% (è®­ç»ƒå¿«ï¼Œç‰¹å¾é‡è¦æ€§ä¸åŒ)
CatBoost:   76% (å¤„ç†ç±»åˆ«ç‰¹å¾å¥½)
              â†“
Stackingé›†æˆ: 80% (ç»“åˆå„æ¨¡å‹ä¼˜åŠ¿)
              â†“
ç»“æœ: æ¯”ä»»ä½•å•æ¨¡å‹éƒ½å¥½
```

## ğŸ—ï¸ æ¨¡å‹é›†æˆåŸç†è¯¦è§£

### 1. Votingï¼ˆæŠ•ç¥¨ï¼‰

**ç¡¬æŠ•ç¥¨**ï¼š
```python
# 3ä¸ªæ¨¡å‹çš„é¢„æµ‹
æ¨¡å‹1: ç±»åˆ«2
æ¨¡å‹2: ç±»åˆ«2
æ¨¡å‹3: ç±»åˆ«3
       â†“
æœ€ç»ˆ: ç±»åˆ«2 (å¤šæ•°æŠ•ç¥¨)
```

**è½¯æŠ•ç¥¨**ï¼š
```python
# 3ä¸ªæ¨¡å‹çš„æ¦‚ç‡é¢„æµ‹
æ¨¡å‹1: [0.1, 0.7, 0.2]  # ç±»åˆ«2æ¦‚ç‡æœ€é«˜
æ¨¡å‹2: [0.2, 0.6, 0.2]  # ç±»åˆ«2æ¦‚ç‡æœ€é«˜
æ¨¡å‹3: [0.3, 0.3, 0.4]  # ç±»åˆ«3æ¦‚ç‡æœ€é«˜
       â†“
å¹³å‡: [0.2, 0.53, 0.27]
       â†“
æœ€ç»ˆ: ç±»åˆ«2 (å¹³å‡æ¦‚ç‡æœ€é«˜)
```

### 2. Stackingï¼ˆå †å ï¼‰

**ä¸¤å±‚ç»“æ„**ï¼š
```
ç¬¬ä¸€å±‚ï¼ˆåŸºæ¨¡å‹ï¼‰:
XGBoost â†’ é¢„æµ‹æ¦‚ç‡ [0.2, 0.5, 0.3]
LightGBM â†’ é¢„æµ‹æ¦‚ç‡ [0.3, 0.4, 0.3]
CatBoost â†’ é¢„æµ‹æ¦‚ç‡ [0.1, 0.6, 0.3]
              â†“
ç¬¬äºŒå±‚ï¼ˆå…ƒæ¨¡å‹ï¼‰:
è¾“å…¥: 9ä¸ªç‰¹å¾ï¼ˆ3ä¸ªæ¨¡å‹Ã—3ä¸ªç±»åˆ«æ¦‚ç‡ï¼‰
é€»è¾‘å›å½’ â†’ æœ€ç»ˆé¢„æµ‹
```

**ä¸ºä»€ä¹ˆStackingæ›´å¼ºï¼Ÿ**
- Votingï¼šç®€å•å¹³å‡ï¼Œæ²¡æœ‰å­¦ä¹ 
- Stackingï¼šå…ƒæ¨¡å‹å­¦ä¹ å¦‚ä½•ç»„åˆåŸºæ¨¡å‹
- ä¾‹å¦‚ï¼šå…ƒæ¨¡å‹å¯èƒ½å­¦åˆ°"XGBooståœ¨ç±»åˆ«1ä¸Šæ›´å‡†ç¡®"

### 3. Blendingï¼ˆæ··åˆï¼‰

**ä¸Stackingçš„åŒºåˆ«**ï¼š
```
Stacking:
- ä½¿ç”¨äº¤å‰éªŒè¯ç”Ÿæˆå…ƒç‰¹å¾
- å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®
- è®­ç»ƒæ—¶é—´é•¿

Blending:
- ä½¿ç”¨holdoutéªŒè¯é›†ç”Ÿæˆå…ƒç‰¹å¾
- å®ç°ç®€å•
- è®­ç»ƒæ—¶é—´çŸ­
```

## ğŸ“Š æ•°æ®é›†

**Otto Group Product Classification**ï¼š
- è®­ç»ƒé›†ï¼š61,878ä¸ªæ ·æœ¬
- æµ‹è¯•é›†ï¼š144,368ä¸ªæ ·æœ¬
- ç‰¹å¾ï¼š93ä¸ªåŒ¿åç‰¹å¾ï¼ˆéƒ½æ˜¯æ•°å€¼å‹ï¼‰
- ç±»åˆ«ï¼š9ä¸ªäº§å“ç±»åˆ«ï¼ˆClass_1 åˆ° Class_9ï¼‰

**æ•°æ®ç‰¹ç‚¹**ï¼š
```
1. ç‰¹å¾å·²è„±æ•ï¼ˆä¸çŸ¥é“å…·ä½“å«ä¹‰ï¼‰
2. ç‰¹å¾å€¼éƒ½æ˜¯æ•´æ•°ï¼ˆå¯èƒ½æ˜¯è®¡æ•°ï¼‰
3. ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæŸäº›ç±»åˆ«æ ·æœ¬å°‘ï¼‰
4. è¯„ä¼°æŒ‡æ ‡ï¼šMulti-class Log Loss
```

## ğŸ—ï¸ XGBoostå¤šåˆ†ç±»è¯¦è§£

### å¤šåˆ†ç±» vs äºŒåˆ†ç±»

**äºŒåˆ†ç±»**ï¼š
```python
# è¾“å‡º1ä¸ªæ¦‚ç‡
output = sigmoid(score)  # 0-1ä¹‹é—´
```

**å¤šåˆ†ç±»**ï¼š
```python
# è¾“å‡º9ä¸ªæ¦‚ç‡ï¼ˆOttoæœ‰9ä¸ªç±»åˆ«ï¼‰
output = softmax(scores)  # å’Œä¸º1
# ä¾‹å¦‚: [0.05, 0.3, 0.1, 0.2, 0.05, 0.1, 0.15, 0.03, 0.02]
```

### XGBoostå¤šåˆ†ç±»å‚æ•°

```python
XGBClassifier(
    objective='multi:softprob',  # å¤šåˆ†ç±» + è¾“å‡ºæ¦‚ç‡
    num_class=9,                 # 9ä¸ªç±»åˆ«
    eval_metric='mlogloss',      # å¤šåˆ†ç±»å¯¹æ•°æŸå¤±

    # æ ‘çš„å‚æ•°
    max_depth=8,                 # æ¯”äºŒåˆ†ç±»ç¨æ·±
    min_child_weight=1,

    # æå‡å‚æ•°
    learning_rate=0.05,
    n_estimators=500,

    # æ­£åˆ™åŒ–
    reg_alpha=0.1,
    reg_lambda=1,

    # é‡‡æ ·
    subsample=0.8,               # è¡Œé‡‡æ ·
    colsample_bytree=0.8,        # åˆ—é‡‡æ ·
)
```

**ä¸ºä»€ä¹ˆmax_depth=8ï¼Ÿ**
- äºŒåˆ†ç±»ï¼šé€šå¸¸3-6
- å¤šåˆ†ç±»ï¼šéœ€è¦æ›´æ·±çš„æ ‘
- åŸå› ï¼š9ä¸ªç±»åˆ«éœ€è¦æ›´å¤æ‚çš„å†³ç­–è¾¹ç•Œ

## ğŸ—ï¸ ä¸‰å¤§æ¢¯åº¦æå‡ç®—æ³•å¯¹æ¯”

### XGBoost vs LightGBM vs CatBoost

| ç‰¹æ€§ | XGBoost | LightGBM | CatBoost |
|-----|---------|----------|----------|
| **è®­ç»ƒé€Ÿåº¦** | ä¸­ç­‰ | æœ€å¿« | è¾ƒæ…¢ |
| **å†…å­˜å ç”¨** | ä¸­ç­‰ | æœ€å° | è¾ƒå¤§ |
| **å‡†ç¡®ç‡** | é«˜ | é«˜ | æœ€é«˜ |
| **ç±»åˆ«ç‰¹å¾** | éœ€ç¼–ç  | éœ€ç¼–ç  | è‡ªåŠ¨å¤„ç† |
| **è¿‡æ‹Ÿåˆ** | ä¸­ç­‰ | å®¹æ˜“ | ä¸æ˜“ |
| **æœ€ä½³åœºæ™¯** | é€šç”¨ | å¤§æ•°æ® | ç±»åˆ«ç‰¹å¾å¤š |

### LightGBMçš„ä¼˜åŠ¿

**Leaf-wiseç”Ÿé•¿**ï¼š
```
XGBoost (Level-wise):
    æ ¹
   /  \
  A    B    # å…ˆåˆ†è£‚å®Œè¿™ä¸€å±‚
 / \  / \
C  D E  F  # å†åˆ†è£‚ä¸‹ä¸€å±‚

LightGBM (Leaf-wise):
    æ ¹
   /  \
  A    B
 / \       # åªåˆ†è£‚å¢ç›Šæœ€å¤§çš„å¶å­
C  D
```

**ä¸ºä»€ä¹ˆæ›´å¿«ï¼Ÿ**
- åªåˆ†è£‚å¢ç›Šæœ€å¤§çš„å¶å­
- å‡å°‘ä¸å¿…è¦çš„åˆ†è£‚
- è®­ç»ƒé€Ÿåº¦æå‡2-3å€

### CatBoostçš„ä¼˜åŠ¿

**è‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾**ï¼š
```python
# XGBoost/LightGBM: éœ€è¦æ‰‹åŠ¨ç¼–ç 
df['category'] = LabelEncoder().fit_transform(df['category'])

# CatBoost: è‡ªåŠ¨å¤„ç†
model = CatBoostClassifier(cat_features=['category'])
model.fit(X, y)  # ç›´æ¥ä½¿ç”¨åŸå§‹ç±»åˆ«ç‰¹å¾
```

**Ordered Boosting**ï¼š
- é˜²æ­¢ç›®æ ‡æ³„éœ²
- æé«˜æ³›åŒ–èƒ½åŠ›

## ğŸ“ é¡¹ç›®ç»“æ„

```
02_Ottoåˆ†ç±»æŒ‘æˆ˜_XGBoostä¸­çº§/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_å¤šåˆ†ç±»é—®é¢˜åŸºç¡€.ipynb         # å¤šåˆ†ç±»æ¦‚å¿µ
â”‚   â”œâ”€â”€ 01_æ•°æ®æ¢ç´¢.ipynb               # EDA
â”‚   â”œâ”€â”€ 02_XGBooståŸºç¡€æ¨¡å‹.ipynb        # å•æ¨¡å‹
â”‚   â”œâ”€â”€ 03_LightGBMæ¨¡å‹.ipynb           # LightGBM
â”‚   â”œâ”€â”€ 04_CatBoostæ¨¡å‹.ipynb           # CatBoost
â”‚   â”œâ”€â”€ 05_æ¨¡å‹å¯¹æ¯”åˆ†æ.ipynb           # â­ ä¸‰å¤§ç®—æ³•å¯¹æ¯”
â”‚   â”œâ”€â”€ 06_Stackingé›†æˆ.ipynb           # â­ æ¨¡å‹å †å 
â”‚   â”œâ”€â”€ 07_Optunaè°ƒå‚.ipynb             # â­ è‡ªåŠ¨è°ƒå‚
â”‚   â””â”€â”€ 08_ç‰¹å¾å·¥ç¨‹.ipynb               # é«˜çº§ç‰¹å¾
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ models.py                        # â­ ä¸‰å¤§æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ ensemble.py                      # â­ é›†æˆæ–¹æ³•
â”‚   â”œâ”€â”€ tuning.py                        # â­ Optunaè°ƒå‚
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â””â”€â”€ results/
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. ä¸‹è½½æ•°æ®
cd data && python download_data.py

# 3. è®­ç»ƒå•æ¨¡å‹
python src/train.py --model xgboost

# 4. è®­ç»ƒé›†æˆæ¨¡å‹
python src/train.py --model stacking

# 5. è‡ªåŠ¨è°ƒå‚
python src/tuning.py --trials 100
```

## ğŸ“ˆ é¢„æœŸç»“æœ

| æ¨¡å‹ | Log Loss | è®­ç»ƒæ—¶é—´ | Kaggleæ’å |
|-----|----------|---------|-----------|
| XGBoost | 0.45 | 5åˆ†é’Ÿ | Top 30% |
| LightGBM | 0.46 | 2åˆ†é’Ÿ | Top 35% |
| CatBoost | 0.44 | 10åˆ†é’Ÿ | Top 25% |
| **Voting** | 0.43 | 17åˆ†é’Ÿ | Top 20% |
| **Stacking** | **0.41** | 25åˆ†é’Ÿ | **Top 10%** |

## ğŸ“ å­¦ä¹ è¦ç‚¹

### 1. å¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡

**Log Lossï¼ˆå¯¹æ•°æŸå¤±ï¼‰**ï¼š
```python
# å…¬å¼
LogLoss = -1/N * Î£ Î£ y_ij * log(p_ij)

# ä¾‹å­
çœŸå®ç±»åˆ«: Class_2
é¢„æµ‹æ¦‚ç‡: [0.1, 0.7, 0.2, ...]
         â†“
LogLoss = -log(0.7) = 0.36

# è¶Šå°è¶Šå¥½
# å®Œç¾é¢„æµ‹: LogLoss = 0
# éšæœºçŒœæµ‹: LogLoss = 2.2 (9ä¸ªç±»åˆ«)
```

### 2. Stackingå®ç°æŠ€å·§

**äº¤å‰éªŒè¯ç”Ÿæˆå…ƒç‰¹å¾**ï¼š
```python
# 5æŠ˜äº¤å‰éªŒè¯
for fold in range(5):
    # è®­ç»ƒåŸºæ¨¡å‹
    model.fit(X_train_fold, y_train_fold)

    # é¢„æµ‹éªŒè¯é›†ï¼ˆä½œä¸ºå…ƒç‰¹å¾ï¼‰
    meta_features[val_idx] = model.predict_proba(X_val_fold)

    # é¢„æµ‹æµ‹è¯•é›†ï¼ˆå–å¹³å‡ï¼‰
    test_meta += model.predict_proba(X_test) / 5

# å…ƒæ¨¡å‹è®­ç»ƒ
meta_model.fit(meta_features, y_train)
```

### 3. Optunaè‡ªåŠ¨è°ƒå‚

**å®šä¹‰æœç´¢ç©ºé—´**ï¼š
```python
def objective(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
    }

    model = XGBClassifier(**params)
    score = cross_val_score(model, X, y, cv=5).mean()
    return score

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

**ä¸ºä»€ä¹ˆç”¨Optunaï¼Ÿ**
- GridSearchï¼šç©·ä¸¾æ‰€æœ‰ç»„åˆï¼Œå¤ªæ…¢
- RandomSearchï¼šéšæœºæœç´¢ï¼Œæ•ˆç‡ä½
- Optunaï¼šè´å¶æ–¯ä¼˜åŒ–ï¼Œæ™ºèƒ½æœç´¢

### 4. å¸¸è§é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆé›†æˆèƒ½æå‡æ€§èƒ½ï¼Ÿ**
A: åå·®-æ–¹å·®åˆ†è§£
```
å•æ¨¡å‹è¯¯å·® = åå·®Â² + æ–¹å·® + å™ªå£°

é›†æˆæ•ˆæœ:
- é™ä½æ–¹å·®ï¼šå¤šä¸ªæ¨¡å‹å¹³å‡ï¼Œå‡å°‘éšæœºæ€§
- ä¿æŒåå·®ï¼šæ¨¡å‹èƒ½åŠ›ä¸å˜
- ç»“æœï¼šæ€»è¯¯å·®é™ä½
```

**Q2: å¦‚ä½•é€‰æ‹©åŸºæ¨¡å‹ï¼Ÿ**
A: å¤šæ ·æ€§åŸåˆ™
```
å¥½çš„ç»„åˆ:
- XGBoost + LightGBM + é€»è¾‘å›å½’
- ä¸åŒç®—æ³•ï¼Œé¢„æµ‹å·®å¼‚å¤§

åçš„ç»„åˆ:
- XGBoost + XGBoost(ä¸åŒå‚æ•°)
- åŒä¸€ç®—æ³•ï¼Œé¢„æµ‹ç›¸ä¼¼
```

**Q3: Stackingä¼šè¿‡æ‹Ÿåˆå—ï¼Ÿ**
A: ä¼šï¼Œéœ€è¦æ³¨æ„
```
é˜²æ­¢è¿‡æ‹Ÿåˆ:
1. ä½¿ç”¨äº¤å‰éªŒè¯ç”Ÿæˆå…ƒç‰¹å¾
2. å…ƒæ¨¡å‹ä½¿ç”¨ç®€å•æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰
3. å…ƒæ¨¡å‹åŠ æ­£åˆ™åŒ–
4. ä¸è¦å †å å¤ªå¤šå±‚ï¼ˆ1-2å±‚è¶³å¤Ÿï¼‰
```

**Q4: å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ**
A:
```python
# æ–¹æ³•1: è°ƒæ•´æ ·æœ¬æƒé‡
model = XGBClassifier(scale_pos_weight=10)

# æ–¹æ³•2: ä½¿ç”¨åˆ†å±‚é‡‡æ ·
cv = StratifiedKFold(n_splits=5)

# æ–¹æ³•3: è¿‡é‡‡æ ·å°‘æ•°ç±»
from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(X, y)
```

## ğŸ”§ è¿›é˜¶ä¼˜åŒ–

### 1. ä¼ªæ ‡ç­¾ï¼ˆPseudo Labelingï¼‰
```python
# ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†
test_pred = model.predict(X_test)

# é€‰æ‹©é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾
confident_idx = test_pred.max(axis=1) > 0.9
X_pseudo = X_test[confident_idx]
y_pseudo = test_pred[confident_idx].argmax(axis=1)

# åŠ å…¥è®­ç»ƒé›†é‡æ–°è®­ç»ƒ
X_train_new = np.vstack([X_train, X_pseudo])
y_train_new = np.hstack([y_train, y_pseudo])
```

### 2. ç‰¹å¾å·¥ç¨‹
```python
# ç»Ÿè®¡ç‰¹å¾
df['sum'] = df.iloc[:, :93].sum(axis=1)
df['mean'] = df.iloc[:, :93].mean(axis=1)
df['std'] = df.iloc[:, :93].std(axis=1)
df['max'] = df.iloc[:, :93].max(axis=1)
df['min'] = df.iloc[:, :93].min(axis=1)

# ç‰¹å¾äº¤äº’
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=True)
X_poly = poly.fit_transform(X)
```

### 3. æ¨¡å‹æ ¡å‡†
```python
# æ ¡å‡†é¢„æµ‹æ¦‚ç‡
from sklearn.calibration import CalibratedClassifierCV
calibrated = CalibratedClassifierCV(model, cv=5)
calibrated.fit(X_train, y_train)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Otto Group Kaggleç«èµ›](https://www.kaggle.com/c/otto-group-product-classification-challenge)
- [Optunaæ–‡æ¡£](https://optuna.readthedocs.io/)
- [Stackingæ•™ç¨‹](https://mlwave.com/kaggle-ensembling-guide/)

## ğŸ¯ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬é¡¹ç›®åï¼Œå¯ä»¥å°è¯•ï¼š
1. **é«˜çº§é¡¹ç›®**ï¼šKaggleç«èµ›çº§åˆ«ï¼ˆSHAPè§£é‡Š + æ·±åº¦ç‰¹å¾å·¥ç¨‹ï¼‰
2. **æ·±åº¦å­¦ä¹ **ï¼šTabNetï¼ˆæ·±åº¦å­¦ä¹ å¤„ç†è¡¨æ ¼æ•°æ®ï¼‰
3. **AutoML**ï¼šH2O AutoMLã€AutoGluon

---

**éš¾åº¦ç­‰çº§**: â­â­â­â˜†â˜† (ä¸­çº§)
**é¢„è®¡å­¦ä¹ æ—¶é—´**: 2-3å‘¨
**å‰ç½®çŸ¥è¯†**: XGBooståŸºç¡€ã€äº¤å‰éªŒè¯
**Kaggleæ’å**: Top 10%ï¼ˆStackingåï¼‰
