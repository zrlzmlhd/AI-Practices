{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf454b60",
   "metadata": {},
   "source": [
    "# XGBoost高级技巧 - 模型模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad856db",
   "metadata": {},
   "source": [
    "\n",
    "本模块展示竞赛级别的XGBoost使用技巧：\n",
    "1. 高级超参数调优（贝叶斯优化、Optuna）\n",
    "2. 早停和学习率调度\n",
    "3. 自定义目标函数和评估指标\n",
    "4. 模型集成（Stacking、Blending）\n",
    "5. 特征重要性分析\n",
    "6. 模型解释（SHAP）\n",
    "\n",
    "【核心技巧】：\n",
    "- 学习率衰减\n",
    "- 正则化组合\n",
    "- 树的深度和数量平衡\n",
    "- 样本和特征采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe6fd2",
   "metadata": {},
   "source": [
    "## 核心算法与工具\n",
    "- `AdvancedXGBoostClassifier` 集成了学习率调度、早停、正则化、采样策略, 默认参数即竞赛配置。\n",
    "- `XGBoostHyperparameterOptimizer` (Optuna) 演示如何进行贝叶斯优化并记录 trial 历史。\n",
    "- `XGBoostEnsemble` 则构建 Bagging 式集成, 通过多模型平均提升稳定性。\n",
    "\n",
    "> **说明**: Notebook 中可以直接切换是否使用 GPU、如何保存最佳迭代, 并可结合 SHAP/特征重要性进行解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad228ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost高级技巧 - 模型模块\n",
    "\n",
    "本模块展示竞赛级别的XGBoost使用技巧：\n",
    "1. 高级超参数调优（贝叶斯优化、Optuna）\n",
    "2. 早停和学习率调度\n",
    "3. 自定义目标函数和评估指标\n",
    "4. 模型集成（Stacking、Blending）\n",
    "5. 特征重要性分析\n",
    "6. 模型解释（SHAP）\n",
    "\n",
    "【核心技巧】：\n",
    "- 学习率衰减\n",
    "- 正则化组合\n",
    "- 树的深度和数量平衡\n",
    "- 样本和特征采样\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# 模型选择和评估\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# 超参数优化\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"警告: optuna未安装，贝叶斯优化功能不可用\")\n",
    "\n",
    "\n",
    "class AdvancedXGBoostClassifier:\n",
    "    \"\"\"\n",
    "    高级XGBoost分类器\n",
    "\n",
    "    【是什么】：集成竞赛技巧的XGBoost模型\n",
    "    【包含技巧】：\n",
    "        - 学习率调度\n",
    "        - 早停策略\n",
    "        - 正则化组合\n",
    "        - 类别不平衡处理\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params=None, use_gpu=False):\n",
    "        \"\"\"\n",
    "        初始化分类器\n",
    "\n",
    "        Args:\n",
    "            params: 模型参数字典\n",
    "            use_gpu: 是否使用GPU\n",
    "        \"\"\"\n",
    "        # 【默认参数】：竞赛级别的配置\n",
    "        self.default_params = {\n",
    "            # ============================================\n",
    "            # 基础参数\n",
    "            # ============================================\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'tree_method': 'gpu_hist' if use_gpu else 'hist',\n",
    "            'random_state': 42,\n",
    "\n",
    "            # ============================================\n",
    "            # 学习参数\n",
    "            # ============================================\n",
    "            'learning_rate': 0.05,\n",
    "            # 【为什么=0.05】：\n",
    "            #   - 较小的学习率 + 更多树 = 更好的泛化\n",
    "            #   - 竞赛中常用0.01-0.1\n",
    "\n",
    "            'n_estimators': 1000,\n",
    "            # 【为什么=1000】：\n",
    "            #   - 配合早停，让模型自己决定最优树数\n",
    "            #   - 实际训练可能在200-500棵树停止\n",
    "\n",
    "            # ============================================\n",
    "            # 树结构参数\n",
    "            # ============================================\n",
    "            'max_depth': 6,\n",
    "            # 【为什么=6】：\n",
    "            #   - 平衡复杂度和泛化能力\n",
    "            #   - 太深容易过拟合，太浅欠拟合\n",
    "\n",
    "            'min_child_weight': 3,\n",
    "            # 【为什么=3】：\n",
    "            #   - 控制叶子节点最小样本权重\n",
    "            #   - 防止过拟合\n",
    "\n",
    "            # ============================================\n",
    "            # 采样参数\n",
    "            # ============================================\n",
    "            'subsample': 0.8,\n",
    "            # 【为什么=0.8】：\n",
    "            #   - 每棵树使用80%的样本\n",
    "            #   - 增加随机性，防止过拟合\n",
    "\n",
    "            'colsample_bytree': 0.8,\n",
    "            # 【为什么=0.8】：\n",
    "            #   - 每棵树使用80%的特征\n",
    "            #   - 类似随机森林的特征采样\n",
    "\n",
    "            'colsample_bylevel': 0.8,\n",
    "            # 【为什么=0.8】：\n",
    "            #   - 每层使用80%的特征\n",
    "            #   - 进一步增加随机性\n",
    "\n",
    "            # ============================================\n",
    "            # 正则化参数\n",
    "            # ============================================\n",
    "            'reg_alpha': 0.1,\n",
    "            # 【L1正则化】：\n",
    "            #   - 特征选择效果\n",
    "            #   - 使部分权重为0\n",
    "\n",
    "            'reg_lambda': 1.0,\n",
    "            # 【L2正则化】：\n",
    "            #   - 权重平滑\n",
    "            #   - 防止过拟合\n",
    "\n",
    "            'gamma': 0.1,\n",
    "            # 【最小分裂损失】：\n",
    "            #   - 分裂节点需要的最小损失减少\n",
    "            #   - 控制树的复杂度\n",
    "        }\n",
    "\n",
    "        # 合并用户参数\n",
    "        if params:\n",
    "            self.default_params.update(params)\n",
    "\n",
    "        self.model = None\n",
    "        self.best_iteration = None\n",
    "        self.feature_importance = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None,\n",
    "            early_stopping_rounds=50, verbose=True):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        【技巧】：\n",
    "        - 使用验证集进行早停\n",
    "        - 监控训练和验证指标\n",
    "        - 保存最佳迭代\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练特征\n",
    "            y_train: 训练标签\n",
    "            X_val: 验证特征\n",
    "            y_val: 验证标签\n",
    "            early_stopping_rounds: 早停轮数\n",
    "            verbose: 是否显示训练过程\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n训练XGBoost模型...\")\n",
    "            print(f\"  参数配置:\")\n",
    "            for key, value in self.default_params.items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 创建DMatrix（XGBoost的数据格式）\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "        # 准备验证集\n",
    "        evals = [(dtrain, 'train')]\n",
    "        if X_val is not None and y_val is not None:\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            evals.append((dval, 'val'))\n",
    "\n",
    "        # 训练\n",
    "        evals_result = {}\n",
    "        self.model = xgb.train(\n",
    "            self.default_params,\n",
    "            dtrain,\n",
    "            num_boost_round=self.default_params.get('n_estimators', 1000),\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=50 if verbose else False\n",
    "        )\n",
    "\n",
    "        self.best_iteration = self.model.best_iteration\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n训练完成:\")\n",
    "            print(f\"  耗时: {elapsed:.2f}秒\")\n",
    "            print(f\"  最佳迭代: {self.best_iteration}\")\n",
    "            if 'val' in evals_result:\n",
    "                best_score = evals_result['val'][self.default_params['eval_metric']][self.best_iteration]\n",
    "                print(f\"  最佳验证分数: {best_score:.4f}\")\n",
    "\n",
    "        # 保存特征重要性\n",
    "        self.feature_importance = self.model.get_score(importance_type='gain')\n",
    "\n",
    "        return evals_result\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        y_pred_proba = self.model.predict(dtest)\n",
    "        return (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        y_pred_proba = self.model.predict(dtest)\n",
    "        return np.vstack([1 - y_pred_proba, y_pred_proba]).T\n",
    "\n",
    "    def get_feature_importance(self, feature_names=None, top_n=20):\n",
    "        \"\"\"\n",
    "        获取特征重要性\n",
    "\n",
    "        【重要性类型】：\n",
    "        - weight: 特征被使用的次数\n",
    "        - gain: 特征带来的平均增益（推荐）\n",
    "        - cover: 特征覆盖的样本数\n",
    "        \"\"\"\n",
    "        if self.feature_importance is None:\n",
    "            return None\n",
    "\n",
    "        # 转换为DataFrame\n",
    "        importance_df = pd.DataFrame([\n",
    "            {'feature': k, 'importance': v}\n",
    "            for k, v in self.feature_importance.items()\n",
    "        ])\n",
    "\n",
    "        # 排序\n",
    "        importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        # 如果提供了特征名称，进行映射\n",
    "        if feature_names:\n",
    "            importance_df['feature'] = importance_df['feature'].apply(\n",
    "                lambda x: feature_names[int(x.replace('f', ''))] if x.startswith('f') else x\n",
    "            )\n",
    "\n",
    "        return importance_df.head(top_n)\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        self.model.save_model(filepath)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        self.model = xgb.Booster()\n",
    "        self.model.load_model(filepath)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")\n",
    "\n",
    "\n",
    "class XGBoostHyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    XGBoost超参数优化器（使用Optuna）\n",
    "\n",
    "    【是什么】：贝叶斯优化超参数\n",
    "    【为什么】：\n",
    "        - 比网格搜索更高效\n",
    "        - 自动探索参数空间\n",
    "        - 早停不佳的试验\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_type='classification', n_trials=100, cv=5):\n",
    "        \"\"\"\n",
    "        初始化优化器\n",
    "\n",
    "        Args:\n",
    "            task_type: 任务类型\n",
    "            n_trials: 优化试验次数\n",
    "            cv: 交叉验证折数\n",
    "        \"\"\"\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            raise ImportError(\"需要安装optuna: pip install optuna\")\n",
    "\n",
    "        self.task_type = task_type\n",
    "        self.n_trials = n_trials\n",
    "        self.cv = cv\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "\n",
    "    def objective(self, trial, X, y):\n",
    "        \"\"\"\n",
    "        优化目标函数\n",
    "\n",
    "        【参数空间】：竞赛常用的参数范围\n",
    "        \"\"\"\n",
    "        # 定义参数空间\n",
    "        params = {\n",
    "            'objective': 'binary:logistic' if self.task_type == 'classification' else 'reg:squarederror',\n",
    "            'eval_metric': 'auc' if self.task_type == 'classification' else 'rmse',\n",
    "            'tree_method': 'hist',\n",
    "            'random_state': 42,\n",
    "\n",
    "            # 【学习率】：对数空间采样\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "\n",
    "            # 【树结构】\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "\n",
    "            # 【采样】\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "\n",
    "            # 【正则化】\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        # 交叉验证\n",
    "        cv_scores = []\n",
    "        kfold = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_idx, val_idx in kfold.split(X, y):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            # 训练\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=[(dval, 'val')],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            # 预测\n",
    "            y_pred = model.predict(dval)\n",
    "\n",
    "            # 评估\n",
    "            if self.task_type == 'classification':\n",
    "                score = roc_auc_score(y_val, y_pred)\n",
    "            else:\n",
    "                score = -mean_squared_error(y_val, y_pred, squared=False)  # 负RMSE\n",
    "\n",
    "            cv_scores.append(score)\n",
    "\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    def optimize(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        执行优化\n",
    "\n",
    "        Args:\n",
    "            X: 特征\n",
    "            y: 标签\n",
    "            verbose: 是否显示进度\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"XGBoost超参数优化（Optuna）\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  试验次数: {self.n_trials}\")\n",
    "        print(f\"  交叉验证: {self.cv}折\")\n",
    "\n",
    "        # 创建study\n",
    "        direction = 'maximize' if self.task_type == 'classification' else 'maximize'\n",
    "        self.study = optuna.create_study(direction=direction)\n",
    "\n",
    "        # 优化\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.study.optimize(\n",
    "            lambda trial: self.objective(trial, X, y),\n",
    "            n_trials=self.n_trials,\n",
    "            show_progress_bar=verbose\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # 最佳参数\n",
    "        self.best_params = self.study.best_params\n",
    "\n",
    "        print(f\"\\n优化完成，耗时: {elapsed:.2f}秒\")\n",
    "        print(f\"\\n最佳参数:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\n最佳分数: {self.study.best_value:.4f}\")\n",
    "\n",
    "        return self.best_params\n",
    "\n",
    "    def get_optimization_history(self):\n",
    "        \"\"\"获取优化历史\"\"\"\n",
    "        if self.study is None:\n",
    "            return None\n",
    "\n",
    "        history = []\n",
    "        for trial in self.study.trials:\n",
    "            history.append({\n",
    "                'trial': trial.number,\n",
    "                'value': trial.value,\n",
    "                'params': trial.params\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(history)\n",
    "\n",
    "\n",
    "class XGBoostEnsemble:\n",
    "    \"\"\"\n",
    "    XGBoost集成模型\n",
    "\n",
    "    【是什么】：组合多个XGBoost模型\n",
    "    【方法】：\n",
    "        - Bagging：训练多个模型，平均预测\n",
    "        - Stacking：使用元模型组合基模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_models=5, ensemble_method='bagging'):\n",
    "        \"\"\"\n",
    "        初始化集成模型\n",
    "\n",
    "        Args:\n",
    "            n_models: 基模型数量\n",
    "            ensemble_method: 集成方法\n",
    "        \"\"\"\n",
    "        self.n_models = n_models\n",
    "        self.ensemble_method = ensemble_method\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, verbose=True):\n",
    "        \"\"\"\n",
    "        训练集成模型\n",
    "\n",
    "        【Bagging策略】：\n",
    "        - 每个模型使用不同的随机种子\n",
    "        - 每个模型使用不同的采样\n",
    "        \"\"\"\n",
    "        print(f\"\\n训练{self.ensemble_method}集成模型...\")\n",
    "        print(f\"  基模型数量: {self.n_models}\")\n",
    "\n",
    "        for i in range(self.n_models):\n",
    "            if verbose:\n",
    "                print(f\"\\n训练模型 {i+1}/{self.n_models}...\")\n",
    "\n",
    "            # 创建模型（不同随机种子）\n",
    "            params = {\n",
    "                'random_state': 42 + i,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8\n",
    "            }\n",
    "\n",
    "            model = AdvancedXGBoostClassifier(params=params)\n",
    "\n",
    "            # 训练\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                X_val, y_val,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "        print(f\"\\n✓ 集成模型训练完成\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        预测概率（平均）\n",
    "\n",
    "        【集成策略】：对所有模型的预测概率求平均\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for model in self.models:\n",
    "            pred = model.predict_proba(X)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # 平均\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"测试模型模块\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"XGBoost高级技巧 - 模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建模拟数据\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=20,\n",
    "        n_informative=15,\n",
    "        n_classes=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_val = X[:800], X[800:]\n",
    "    y_train, y_val = y[:800], y[800:]\n",
    "\n",
    "    # 测试基础模型\n",
    "    print(\"\\n测试基础XGBoost模型...\")\n",
    "    model = AdvancedXGBoostClassifier()\n",
    "    model.fit(X_train, y_train, X_val, y_val, verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"  验证集准确率: {accuracy:.4f}\")\n",
    "\n",
    "    # 测试特征重要性\n",
    "    importance = model.get_feature_importance(top_n=5)\n",
    "    print(f\"\\n  Top 5 重要特征:\")\n",
    "    print(importance)\n",
    "\n",
    "    # 测试集成模型\n",
    "    print(\"\\n测试集成模型...\")\n",
    "    ensemble = XGBoostEnsemble(n_models=3)\n",
    "    ensemble.fit(X_train, y_train, X_val, y_val, verbose=False)\n",
    "\n",
    "    y_pred_ensemble = ensemble.predict(X_val)\n",
    "    accuracy_ensemble = accuracy_score(y_val, y_pred_ensemble)\n",
    "    print(f\"  集成模型准确率: {accuracy_ensemble:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ 模型测试通过！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
