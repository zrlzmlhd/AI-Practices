{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbf2559",
   "metadata": {},
   "source": [
    "# SVM文本分类数据处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e8cb3",
   "metadata": {},
   "source": [
    "\n",
    "本模块负责：\n",
    "1. 加载文本分类数据集（20 Newsgroups、IMDB等）\n",
    "2. 文本预处理（清洗、分词、去停用词）\n",
    "3. 特征提取（TF-IDF、Word2Vec、Doc2Vec）\n",
    "4. 数据划分和保存\n",
    "\n",
    "【推荐数据集】：\n",
    "- 20 Newsgroups（新闻分类）\n",
    "- IMDB电影评论（情感分类）\n",
    "- AG News（新闻分类）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abd413",
   "metadata": {},
   "source": [
    "## 文本数据处理步骤\n",
    "1. `load_20newsgroups_data` 等函数负责读取 20 News / IMDB / AG News 等数据集, 支持筛选子类别。\n",
    "2. `TextPreprocessor` 依次执行 HTML/URL/邮箱清洗、分词、停用词过滤与词形还原, 输出干净文本。\n",
    "3. `TextFeatureExtractor` 根据 `feature_method` 生成 TF-IDF、Count 或 Word2Vec 特征, 并保留向量化器以便推理。\n",
    "4. `prepare_text_classification_data` 负责划分 train/val/test 并统计类别分布, 方便 Notebook 检查是否平衡。\n",
    "\n",
    "> **核心提示**: 每一步都可单独运行, 在 Notebook 中观察样例文本和特征空间维度, 很适合教学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50232255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM文本分类数据处理模块\n",
    "\n",
    "本模块负责：\n",
    "1. 加载文本分类数据集（20 Newsgroups、IMDB等）\n",
    "2. 文本预处理（清洗、分词、去停用词）\n",
    "3. 特征提取（TF-IDF、Word2Vec、Doc2Vec）\n",
    "4. 数据划分和保存\n",
    "\n",
    "【推荐数据集】：\n",
    "- 20 Newsgroups（新闻分类）\n",
    "- IMDB电影评论（情感分类）\n",
    "- AG News（新闻分类）\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# 文本处理\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 特征提取\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    print(\"警告: gensim未安装，Word2Vec功能不可用\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    文本预处理器\n",
    "\n",
    "    【是什么】：文本清洗和标准化工具\n",
    "    【做什么】：\n",
    "        - 去除HTML标签、特殊字符\n",
    "        - 转小写\n",
    "        - 分词\n",
    "        - 去停用词\n",
    "        - 词形还原\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, lemmatize=True):\n",
    "        \"\"\"\n",
    "        初始化预处理器\n",
    "\n",
    "        Args:\n",
    "            remove_stopwords: 是否去除停用词\n",
    "            lemmatize: 是否进行词形还原\n",
    "        \"\"\"\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        # 下载必要的NLTK数据\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except LookupError:\n",
    "            print(\"下载停用词列表...\")\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        try:\n",
    "            word_tokenize(\"test\")\n",
    "        except LookupError:\n",
    "            print(\"下载分词器...\")\n",
    "            nltk.download('punkt', quiet=True)\n",
    "\n",
    "        if lemmatize:\n",
    "            try:\n",
    "                self.lemmatizer = WordNetLemmatizer()\n",
    "                self.lemmatizer.lemmatize(\"test\")\n",
    "            except LookupError:\n",
    "                print(\"下载词形还原数据...\")\n",
    "                nltk.download('wordnet', quiet=True)\n",
    "                self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        清洗文本\n",
    "\n",
    "        【步骤】：\n",
    "        1. 去除HTML标签\n",
    "        2. 去除URL\n",
    "        3. 去除邮箱\n",
    "        4. 去除特殊字符（保留字母和空格）\n",
    "        5. 转小写\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # 去除HTML标签\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "        # 去除URL\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "        # 去除邮箱\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # 去除特殊字符（保留字母和空格）\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # 转小写\n",
    "        text = text.lower()\n",
    "\n",
    "        # 去除多余空格\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"分词\"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords_func(self, tokens):\n",
    "        \"\"\"去除停用词\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"词形还原\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        完整的预处理流程\n",
    "\n",
    "        Args:\n",
    "            text: 原始文本\n",
    "\n",
    "        Returns:\n",
    "            处理后的文本（字符串）\n",
    "        \"\"\"\n",
    "        # 清洗\n",
    "        text = self.clean_text(text)\n",
    "\n",
    "        # 分词\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        # 去停用词\n",
    "        if self.remove_stopwords:\n",
    "            tokens = self.remove_stopwords_func(tokens)\n",
    "\n",
    "        # 词形还原\n",
    "        if self.lemmatize:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "\n",
    "        # 重新组合\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def preprocess_batch(self, texts, verbose=True):\n",
    "        \"\"\"\n",
    "        批量预处理\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            verbose: 是否显示进度\n",
    "\n",
    "        Returns:\n",
    "            处理后的文本列表\n",
    "        \"\"\"\n",
    "        processed = []\n",
    "        total = len(texts)\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            if verbose and (i + 1) % 1000 == 0:\n",
    "                print(f\"  已处理: {i+1}/{total}\")\n",
    "\n",
    "            processed.append(self.preprocess(text))\n",
    "\n",
    "        return processed\n",
    "\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    \"\"\"\n",
    "    文本特征提取器\n",
    "\n",
    "    【是什么】：将文本转换为数值特征\n",
    "    【支持的方法】：\n",
    "        - TF-IDF（词频-逆文档频率）\n",
    "        - Count Vectorizer（词频统计）\n",
    "        - Word2Vec（词向量平均）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='tfidf', max_features=5000, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化特征提取器\n",
    "\n",
    "        Args:\n",
    "            method: 特征提取方法 ('tfidf', 'count', 'word2vec')\n",
    "            max_features: 最大特征数\n",
    "            **kwargs: 其他参数\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = None\n",
    "        self.word2vec_model = None\n",
    "\n",
    "        if method == 'tfidf':\n",
    "            # 【TF-IDF】：\n",
    "            # - TF（词频）：词在文档中出现的频率\n",
    "            # - IDF（逆文档频率）：log(总文档数/包含该词的文档数)\n",
    "            # - TF-IDF = TF * IDF\n",
    "            # 【为什么】：降低常见词的权重，提高区分性词的权重\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=kwargs.get('ngram_range', (1, 2)),  # 1-gram和2-gram\n",
    "                min_df=kwargs.get('min_df', 2),  # 最小文档频率\n",
    "                max_df=kwargs.get('max_df', 0.95),  # 最大文档频率\n",
    "                sublinear_tf=True  # 使用对数TF\n",
    "            )\n",
    "\n",
    "        elif method == 'count':\n",
    "            # 【Count Vectorizer】：简单的词频统计\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=kwargs.get('ngram_range', (1, 2)),\n",
    "                min_df=kwargs.get('min_df', 2),\n",
    "                max_df=kwargs.get('max_df', 0.95)\n",
    "            )\n",
    "\n",
    "        elif method == 'word2vec':\n",
    "            if not GENSIM_AVAILABLE:\n",
    "                raise ImportError(\"Word2Vec需要安装gensim: pip install gensim\")\n",
    "            # Word2Vec参数将在fit时设置\n",
    "            self.vector_size = kwargs.get('vector_size', 100)\n",
    "            self.window = kwargs.get('window', 5)\n",
    "            self.min_count = kwargs.get('min_count', 2)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        训练特征提取器\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "        \"\"\"\n",
    "        print(f\"\\n训练{self.method}特征提取器...\")\n",
    "\n",
    "        if self.method in ['tfidf', 'count']:\n",
    "            self.vectorizer.fit(texts)\n",
    "            print(f\"  特征维度: {len(self.vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "        elif self.method == 'word2vec':\n",
    "            # 分词\n",
    "            tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "            # 训练Word2Vec\n",
    "            self.word2vec_model = Word2Vec(\n",
    "                sentences=tokenized_texts,\n",
    "                vector_size=self.vector_size,\n",
    "                window=self.window,\n",
    "                min_count=self.min_count,\n",
    "                workers=4,\n",
    "                sg=1  # Skip-gram\n",
    "            )\n",
    "            print(f\"  词汇表大小: {len(self.word2vec_model.wv)}\")\n",
    "            print(f\"  向量维度: {self.vector_size}\")\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\"\n",
    "        转换文本为特征\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "\n",
    "        Returns:\n",
    "            特征矩阵\n",
    "        \"\"\"\n",
    "        if self.method in ['tfidf', 'count']:\n",
    "            return self.vectorizer.transform(texts)\n",
    "\n",
    "        elif self.method == 'word2vec':\n",
    "            # 【Word2Vec特征】：对句子中所有词向量求平均\n",
    "            features = []\n",
    "            for text in texts:\n",
    "                words = text.split()\n",
    "                word_vectors = []\n",
    "\n",
    "                for word in words:\n",
    "                    if word in self.word2vec_model.wv:\n",
    "                        word_vectors.append(self.word2vec_model.wv[word])\n",
    "\n",
    "                if word_vectors:\n",
    "                    # 平均词向量\n",
    "                    features.append(np.mean(word_vectors, axis=0))\n",
    "                else:\n",
    "                    # 零向量\n",
    "                    features.append(np.zeros(self.vector_size))\n",
    "\n",
    "            return np.array(features)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"训练并转换\"\"\"\n",
    "        self.fit(texts)\n",
    "        return self.transform(texts)\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"获取特征名称\"\"\"\n",
    "        if self.method in ['tfidf', 'count']:\n",
    "            return self.vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def load_20newsgroups_data(subset='train', categories=None, remove=('headers', 'footers', 'quotes')):\n",
    "    \"\"\"\n",
    "    加载20 Newsgroups数据集\n",
    "\n",
    "    【是什么】：经典的文本分类数据集\n",
    "    【包含】：20个新闻组的文章\n",
    "\n",
    "    Args:\n",
    "        subset: 'train', 'test', 或 'all'\n",
    "        categories: 类别列表（None表示全部）\n",
    "        remove: 要移除的部分\n",
    "\n",
    "    Returns:\n",
    "        texts, labels, label_names\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "    print(f\"\\n加载20 Newsgroups数据集...\")\n",
    "    print(f\"  子集: {subset}\")\n",
    "\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=subset,\n",
    "        categories=categories,\n",
    "        remove=remove,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    texts = data.data\n",
    "    labels = data.target\n",
    "    label_names = data.target_names\n",
    "\n",
    "    print(f\"  样本数: {len(texts)}\")\n",
    "    print(f\"  类别数: {len(label_names)}\")\n",
    "    print(f\"  类别: {label_names}\")\n",
    "\n",
    "    return texts, labels, label_names\n",
    "\n",
    "\n",
    "def prepare_text_classification_data(\n",
    "    data_source='20newsgroups',\n",
    "    feature_method='tfidf',\n",
    "    max_features=5000,\n",
    "    test_size=0.2,\n",
    "    categories=None,\n",
    "    preprocess=True\n",
    "):\n",
    "    \"\"\"\n",
    "    准备文本分类数据\n",
    "\n",
    "    【完整流程】：\n",
    "    1. 加载数据\n",
    "    2. 文本预处理\n",
    "    3. 特征提取\n",
    "    4. 数据划分\n",
    "\n",
    "    Args:\n",
    "        data_source: 数据源 ('20newsgroups')\n",
    "        feature_method: 特征提取方法 ('tfidf', 'count', 'word2vec')\n",
    "        max_features: 最大特征数\n",
    "        test_size: 测试集比例\n",
    "        categories: 类别列表\n",
    "        preprocess: 是否预处理\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_test, y_test), feature_extractor, label_names\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"文本分类数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ============================================\n",
    "    # 1. 加载数据\n",
    "    # ============================================\n",
    "    if data_source == '20newsgroups':\n",
    "        texts_train, labels_train, label_names = load_20newsgroups_data(\n",
    "            subset='train',\n",
    "            categories=categories\n",
    "        )\n",
    "        texts_test, labels_test, _ = load_20newsgroups_data(\n",
    "            subset='test',\n",
    "            categories=categories\n",
    "        )\n",
    "\n",
    "        texts = texts_train + texts_test\n",
    "        labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的数据源: {data_source}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 2. 文本预处理\n",
    "    # ============================================\n",
    "    if preprocess:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"文本预处理\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        preprocessor = TextPreprocessor(\n",
    "            remove_stopwords=True,\n",
    "            lemmatize=True\n",
    "        )\n",
    "\n",
    "        texts = preprocessor.preprocess_batch(texts, verbose=True)\n",
    "        print(f\"  预处理完成\")\n",
    "\n",
    "    # ============================================\n",
    "    # 3. 特征提取\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"特征提取\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    feature_extractor = TextFeatureExtractor(\n",
    "        method=feature_method,\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # 提取特征\n",
    "    X_train = feature_extractor.fit_transform(X_train_text)\n",
    "    X_test = feature_extractor.transform(X_test_text)\n",
    "\n",
    "    print(f\"\\n数据集统计:\")\n",
    "    print(f\"  训练集: {X_train.shape}\")\n",
    "    print(f\"  测试集: {X_test.shape}\")\n",
    "    print(f\"  类别数: {len(label_names)}\")\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), feature_extractor, label_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"测试数据处理模块\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"文本分类数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试预处理\n",
    "    print(\"\\n测试文本预处理...\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "\n",
    "    test_text = \"This is a TEST! <html>Remove HTML</html> http://example.com\"\n",
    "    processed = preprocessor.preprocess(test_text)\n",
    "    print(f\"  原文: {test_text}\")\n",
    "    print(f\"  处理后: {processed}\")\n",
    "\n",
    "    # 测试数据加载（使用少量类别）\n",
    "    print(\"\\n测试数据加载...\")\n",
    "    try:\n",
    "        (X_train, y_train), (X_test, y_test), extractor, labels = \\\n",
    "            prepare_text_classification_data(\n",
    "                categories=['alt.atheism', 'talk.religion.misc'],\n",
    "                max_features=1000,\n",
    "                preprocess=True\n",
    "            )\n",
    "        print(\"\\n✓ 数据处理测试通过！\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ 测试失败: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
