{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950b8266",
   "metadata": {},
   "source": [
    "# 04_Transformer机器翻译_高级 - data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d03db",
   "metadata": {},
   "source": [
    "机器翻译数据处理模块\n",
    "\n",
    "本模块负责：\n",
    "1. 加载翻译数据集（使用公开数据集）\n",
    "2. 构建源语言和目标语言词汇表\n",
    "3. 序列编码和填充\n",
    "4. 创建训练数据\n",
    "\n",
    "使用数据集：可以使用WMT、IWSLT等公开数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0537d81",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码拆分为多个小单元, 便于逐步运行和检查。\n",
    "- Notebook 会尽量保持原脚本的注释, 方便理解核心算法与数据处理步骤。\n",
    "- 若脚本依赖 `__file__`, 已自动注入路径解析逻辑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ec9d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac21e03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TranslationDataProcessor:\n",
    "    \"\"\"\n",
    "    翻译数据处理器\n",
    "\n",
    "    【是什么】：处理机器翻译数据的工具类\n",
    "    【做什么】：\n",
    "        - 加载平行语料\n",
    "        - 构建双语词汇表\n",
    "        - 序列编码和填充\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len=50, max_vocab_size=10000):\n",
    "        \"\"\"\n",
    "        初始化数据处理器\n",
    "\n",
    "        Args:\n",
    "            max_len: 最大序列长度\n",
    "            max_vocab_size: 最大词汇表大小\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "        # 特殊token\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.SOS_TOKEN = '<SOS>'  # Start of Sequence\n",
    "        self.EOS_TOKEN = '<EOS>'  # End of Sequence\n",
    "\n",
    "        self.special_tokens = [\n",
    "            self.PAD_TOKEN,\n",
    "            self.UNK_TOKEN,\n",
    "            self.SOS_TOKEN,\n",
    "            self.EOS_TOKEN\n",
    "        ]\n",
    "\n",
    "        # 词汇表\n",
    "        self.src_word2idx = {}\n",
    "        self.src_idx2word = {}\n",
    "        self.tgt_word2idx = {}\n",
    "        self.tgt_idx2word =\n",
    "\n",
    "    def load_parallel_data(self, src_path, tgt_path, max_samples=None):\n",
    "        \"\"\"\n",
    "        加载平行语料\n",
    "\n",
    "        【格式】：\n",
    "        源语言文件：每行一个句子\n",
    "        目标语言文件：每行一个句子（对应翻译）\n",
    "\n",
    "        Args:\n",
    "            src_path: 源语言文件路径\n",
    "            tgt_path: 目标语言文件路径\n",
    "            max_samples: 最大样本数\n",
    "\n",
    "        Returns:\n",
    "            src_sentences: 源语言句子列表\n",
    "            tgt_sentences: 目标语言句子列表\n",
    "        \"\"\"\n",
    "        print(f\"\\n加载平行语料...\")\n",
    "        print(f\"  源语言: {src_path}\")\n",
    "        print(f\"  目标语言: {tgt_path}\")\n",
    "\n",
    "        src_sentences = []\n",
    "        tgt_sentences = []\n",
    "\n",
    "        try:\n",
    "            with open(src_path, 'r', encoding='utf-8') as f_src, \\\n",
    "                 open(tgt_path, 'r', encoding='utf-8') as f_tgt:\n",
    "\n",
    "                for src_line, tgt_line in zip(f_src, f_tgt):\n",
    "                    src_line = src_line.strip()\n",
    "                    tgt_line = tgt_line.strip()\n",
    "\n",
    "                    if src_line and tgt_line:\n",
    "                        # 简单的分词（按空格）\n",
    "                        src_words = src_line.lower().split()\n",
    "                        tgt_words = tgt_line.lower().split()\n",
    "\n",
    "                        # 过滤过长的句子\n",
    "                        if len(src_words) <= self.max_len and len(tgt_words) <= self.max_len:\n",
    "                            src_sentences.append(src_words)\n",
    "                            tgt_sentences.append(tgt_words)\n",
    "\n",
    "                    if max_samples and len(src_sentences) >= max_samples:\n",
    "                        break\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  ✗ 文件不存在: {e}\")\n",
    "            return [], []\n",
    "\n",
    "        print(f\"  加载句对数: {len(src_sentences)}\")\n",
    "        if src_sentences:\n",
    "            print(f\"  源语言平均长度: {np.mean([len(s) for s in src_sentences]):.1f}\")\n",
    "            print(f\"  目标语言平均长度: {np.mean([len(s) for s in tgt_sentences]):.1f}\")\n",
    "\n",
    "        return src_sentences, tgt_sentences\n",
    "\n",
    "    def build_vocab(self, sentences, vocab_type='src'):\n",
    "        \"\"\"\n",
    "        构建词汇表\n",
    "\n",
    "        Args:\n",
    "            sentences: 句子列表\n",
    "            vocab_type: 'src' 或 'tgt'\n",
    "        \"\"\"\n",
    "        print(f\"\\n构建{vocab_type}词汇表...\")\n",
    "\n",
    "        # 统计词频\n",
    "        word_freq = Counter()\n",
    "        for sentence in sentences:\n",
    "            word_freq.update(sentence)\n",
    "\n",
    "        print(f\"  总词数: {len(word_freq)}\")\n",
    "\n",
    "        # 选择最常见的词\n",
    "        most_common = word_freq.most_common(self.max_vocab_size - len(self.special_tokens))\n",
    "\n",
    "        # 构建映射\n",
    "        word2idx = {}\n",
    "        idx2word = {}\n",
    "\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            word2idx[token] = idx\n",
    "            idx2word[idx] = token\n",
    "\n",
    "        for idx, (word, _) in enumerate(most_common, start=len(self.special_tokens)):\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "\n",
    "        if vocab_type == 'src':\n",
    "            self.src_word2idx = word2idx\n",
    "            self.src_idx2word = idx2word\n",
    "        else:\n",
    "            self.tgt_word2idx = word2idx\n",
    "            self.tgt_idx2word = idx2word\n",
    "\n",
    "        print(f\"  词汇表大小: {len(word2idx)}\")\n",
    "\n",
    "    def encode_sentences(self, sentences, vocab_type='src', add_sos_eos=False):\n",
    "        \"\"\"\n",
    "        编码句子\n",
    "\n",
    "        Args:\n",
    "            sentences: 句子列表\n",
    "            vocab_type: 'src' 或 'tgt'\n",
    "            add_sos_eos: 是否添加SOS/EOS标记\n",
    "\n",
    "        Returns:\n",
    "            编码后的句子\n",
    "        \"\"\"\n",
    "        word2idx = self.src_word2idx if vocab_type == 'src' else self.tgt_word2idx\n",
    "        unk_idx = word2idx[self.UNK_TOKEN]\n",
    "        sos_idx = word2idx[self.SOS_TOKEN]\n",
    "        eos_idx = word2idx[self.EOS_TOKEN]\n",
    "\n",
    "        encoded_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            encoded = [word2idx.get(word, unk_idx) for word in sentence]\n",
    "\n",
    "            if add_sos_eos:\n",
    "                encoded = [sos_idx] + encoded + [eos_idx]\n",
    "\n",
    "            encoded_sentences.append(encoded)\n",
    "\n",
    "        return encoded_sentences\n",
    "\n",
    "    def pad_sequences(self, sequences):\n",
    "        \"\"\"填充序列\"\"\"\n",
    "        padded = pad_sequences(\n",
    "            sequences,\n",
    "            maxlen=self.max_len,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=self.src_word2idx[self.PAD_TOKEN]\n",
    "        )\n",
    "        return padded\n",
    "\n",
    "    def save_processor(self, filepath):\n",
    "        \"\"\"保存数据处理器\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'src_word2idx': self.src_word2idx,\n",
    "                'src_idx2word': self.src_idx2word,\n",
    "                'tgt_word2idx': self.tgt_word2idx,\n",
    "                'tgt_idx2word': self.tgt_idx2word,\n",
    "                'max_len': self.max_len\n",
    "            }, f)\n",
    "        print(f\"✓ 数据处理器已保存: {filepath}\")\n",
    "\n",
    "    def load_processor(self, filepath):\n",
    "        \"\"\"加载数据处理器\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.src_word2idx = data['src_word2idx']\n",
    "            self.src_idx2word = data['src_idx2word']\n",
    "            self.tgt_word2idx = data['tgt_word2idx']\n",
    "            self.tgt_idx2word = data['tgt_idx2word']\n",
    "            self.max_len = data['max_len']\n",
    "        print(f\"✓ 数据处理器已加载: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ba127",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_translation_data(src_path, tgt_path, max_len=50, max_samples=None):\n",
    "    \"\"\"\n",
    "    准备翻译数据\n",
    "\n",
    "    Args:\n",
    "        src_path: 源语言文件路径\n",
    "        tgt_path: 目标语言文件路径\n",
    "        max_len: 最大序列长度\n",
    "        max_samples: 最大样本数\n",
    "\n",
    "    Returns:\n",
    "        训练集、验证集、处理器\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"机器翻译数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    processor = TranslationDataProcessor(max_len=max_len)\n",
    "\n",
    "    # 加载数据\n",
    "    src_sentences, tgt_sentences = processor.load_parallel_data(\n",
    "        src_path, tgt_path, max_samples\n",
    "    )\n",
    "\n",
    "    if not src_sentences:\n",
    "        raise FileNotFoundError(\"无法加载数据\")\n",
    "\n",
    "    # 构建词汇表\n",
    "    processor.build_vocab(src_sentences, 'src')\n",
    "    processor.build_vocab(tgt_sentences, 'tgt')\n",
    "\n",
    "    # 编码\n",
    "    src_encoded = processor.encode_sentences(src_sentences, 'src')\n",
    "    tgt_encoded = processor.encode_sentences(tgt_sentences, 'tgt', add_sos_eos=True)\n",
    "\n",
    "    # 填充\n",
    "    src_padded = processor.pad_sequences(src_encoded)\n",
    "    tgt_padded = processor.pad_sequences(tgt_encoded)\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    src_train, src_val, tgt_train, tgt_val = train_test_split(\n",
    "        src_padded, tgt_padded, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n训练集: {src_train.shape}\")\n",
    "    print(f\"验证集: {src_val.shape}\")\n",
    "\n",
    "    return (src_train, tgt_train), (src_val, tgt_val), processor\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=\"*60)\n",
    "    print(\"翻译数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建模拟数据\n",
    "    src_data = \"hello world\\nhow are you\\ngood morning\\n\"\n",
    "    tgt_data = \"你好 世界\\n你 好 吗\\n早上 好\\n\"\n",
    "\n",
    "    with open('temp_src.txt', 'w') as f:\n",
    "        f.write(src_data)\n",
    "    with open('temp_tgt.txt', 'w') as f:\n",
    "        f.write(tgt_data)\n",
    "\n",
    "    try:\n",
    "        (src_train, tgt_train), (src_val, tgt_val), processor = prepare_translation_data(\n",
    "            'temp_src.txt', 'temp_tgt.txt', max_len=20\n",
    "        )\n",
    "        print(\"\\n✓ 数据处理测试通过！\")\n",
    "    finally:\n",
    "        import os\n",
    "        os.remove('temp_src.txt')\n",
    "        os.remove('temp_tgt.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
