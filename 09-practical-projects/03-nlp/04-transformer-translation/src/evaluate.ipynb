{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774a7ea8",
   "metadata": {},
   "source": [
    "# Transformer机器翻译评估脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f39e8c",
   "metadata": {},
   "source": [
    "使用方法:\n",
    "    python src/evaluate.py --model_path models/transformer_translation_model.h5 --processor_path models/translation_processor.pkl\n",
    "    python src/evaluate.py --test_src data/test.en --test_tgt data/test.zh\n",
    "\n",
    "【评估指标】:\n",
    "- BLEU分数（机器翻译标准指标）\n",
    "- 词级准确率\n",
    "- 翻译样本展示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f9966",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b6b6d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/03_自然语言处理项目/04_Transformer机器翻译_高级/src/evaluate.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/03_自然语言处理项目/04_Transformer机器翻译_高级/src/evaluate.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3453ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data import TranslationDataProcessor\n",
    "from src.model import TransformerTranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241b07d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='评估Transformer机器翻译模型')\n",
    "\n",
    "    parser.add_argument('--model_path', type=str, required=True,\n",
    "                       help='模型文件路径')\n",
    "    parser.add_argument('--processor_path', type=str, required=True,\n",
    "                       help='处理器文件路径')\n",
    "    parser.add_argument('--test_src', type=str, default='data/test.en',\n",
    "                       help='测试集源语言文件')\n",
    "    parser.add_argument('--test_tgt', type=str, default='data/test.zh',\n",
    "                       help='测试集目标语言文件')\n",
    "    parser.add_argument('--max_samples', type=int, default=None,\n",
    "                       help='最大测试样本数')\n",
    "    parser.add_argument('--num_display', type=int, default=10,\n",
    "                       help='展示的翻译样本数')\n",
    "    parser.add_argument('--result_dir', type=str, default='results',\n",
    "                       help='结果保存目录')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114fef9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_bleu_score(references, hypotheses, max_n=4):\n",
    "    \"\"\"\n",
    "    计算BLEU分数\n",
    "\n",
    "    【是什么】：机器翻译质量评估的标准指标\n",
    "    【如何计算】：\n",
    "        - 计算n-gram精确率（n=1,2,3,4）\n",
    "        - 应用简短惩罚（BP）\n",
    "        - BLEU = BP * exp(sum(log(p_n)))\n",
    "\n",
    "    Args:\n",
    "        references: 参考翻译列表（每个是词列表）\n",
    "        hypotheses: 模型翻译列表（每个是词列表）\n",
    "        max_n: 最大n-gram长度\n",
    "\n",
    "    Returns:\n",
    "        BLEU分数字典\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def get_ngrams(tokens, n):\n",
    "        \"\"\"获取n-gram\"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngrams.append(tuple(tokens[i:i+n]))\n",
    "        return ngrams\n",
    "\n",
    "    def count_ngrams(tokens, n):\n",
    "        \"\"\"统计n-gram频率\"\"\"\n",
    "        ngrams = get_ngrams(tokens, n)\n",
    "        return Counter(ngrams)\n",
    "\n",
    "    # 计算每个n的精确率\n",
    "    precisions = []\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        matched = 0\n",
    "        total = 0\n",
    "\n",
    "        for ref, hyp in zip(references, hypotheses):\n",
    "            ref_ngrams = count_ngrams(ref, n)\n",
    "            hyp_ngrams = count_ngrams(hyp, n)\n",
    "\n",
    "            # 计算匹配的n-gram数量\n",
    "            for ngram, count in hyp_ngrams.items():\n",
    "                matched += min(count, ref_ngrams.get(ngram, 0))\n",
    "\n",
    "            total += max(len(hyp) - n + 1, 0)\n",
    "\n",
    "        if total > 0:\n",
    "            precisions.append(matched / total)\n",
    "        else:\n",
    "            precisions.append(0.0)\n",
    "\n",
    "    # 计算简短惩罚（Brevity Penalty）\n",
    "    ref_len = sum(len(ref) for ref in references)\n",
    "    hyp_len = sum(len(hyp) for hyp in hypotheses)\n",
    "\n",
    "    if hyp_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = np.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0.0\n",
    "\n",
    "    # 计算BLEU分数\n",
    "    if min(precisions) > 0:\n",
    "        log_precisions = [np.log(p) for p in precisions]\n",
    "        bleu = bp * np.exp(sum(log_precisions) / len(log_precisions))\n",
    "    else:\n",
    "        bleu = 0.0\n",
    "\n",
    "    return {\n",
    "        'BLEU': bleu,\n",
    "        'BLEU-1': precisions[0] if len(precisions) > 0 else 0.0,\n",
    "        'BLEU-2': precisions[1] if len(precisions) > 1 else 0.0,\n",
    "        'BLEU-3': precisions[2] if len(precisions) > 2 else 0.0,\n",
    "        'BLEU-4': precisions[3] if len(precisions) > 3 else 0.0,\n",
    "        'BP': bp,\n",
    "        'ref_len': ref_len,\n",
    "        'hyp_len': hyp_len\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc514b73",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def translate_and_evaluate(model, processor, src_sequences, tgt_sequences):\n",
    "    \"\"\"\n",
    "    翻译并评估\n",
    "\n",
    "    【流程】：\n",
    "    1. 对每个源句子进行翻译\n",
    "    2. 与参考翻译对比\n",
    "    3. 计算BLEU分数\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"翻译测试集\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for i, (src_seq, tgt_seq) in enumerate(zip(src_sequences, tgt_sequences)):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  已翻译: {i+1}/{len(src_sequences)}\")\n",
    "\n",
    "        # 参考翻译（去除特殊token）\n",
    "        ref_words = []\n",
    "        for idx in tgt_seq:\n",
    "            if idx == 0:  # PAD\n",
    "                break\n",
    "            word = processor.tgt_idx2word.get(idx, '<UNK>')\n",
    "            if word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n",
    "                ref_words.append(word)\n",
    "\n",
    "        # 模型翻译\n",
    "        translation = model.translate(\n",
    "            src_seq,\n",
    "            processor.tgt_word2idx,\n",
    "            processor.tgt_idx2word,\n",
    "            max_len=processor.max_len\n",
    "        )\n",
    "        hyp_words = translation.split()\n",
    "\n",
    "        references.append(ref_words)\n",
    "        hypotheses.append(hyp_words)\n",
    "\n",
    "    print(f\"  完成翻译: {len(src_sequences)}个句子\")\n",
    "\n",
    "    # 计算BLEU分数\n",
    "    bleu_scores = calculate_bleu_score(references, hypotheses)\n",
    "\n",
    "    return references, hypotheses, bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766148a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_translation_samples(processor, src_sequences, references, hypotheses, num_samples=10):\n",
    "    \"\"\"\n",
    "    展示翻译样本\n",
    "\n",
    "    【是什么】：对比源句子、参考翻译和模型翻译\n",
    "    【为什么】：直观评估翻译质量\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"翻译样本展示\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for i in range(min(num_samples, len(src_sequences))):\n",
    "        # 源句子\n",
    "        src_seq = src_sequences[i]\n",
    "        src_words = []\n",
    "        for idx in src_seq:\n",
    "            if idx == 0:\n",
    "                break\n",
    "            word = processor.src_idx2word.get(idx, '<UNK>')\n",
    "            if word not in ['<PAD>', '<UNK>']:\n",
    "                src_words.append(word)\n",
    "\n",
    "        print(f\"\\n样本 {i+1}:\")\n",
    "        print(f\"  源句子: {' '.join(src_words)}\")\n",
    "        print(f\"  参考翻译: {' '.join(references[i])}\")\n",
    "        print(f\"  模型翻译: {' '.join(hypotheses[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb04dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_bleu_scores(bleu_scores, save_path):\n",
    "    \"\"\"\n",
    "    绘制BLEU分数\n",
    "\n",
    "    【可视化】：展示不同n-gram的BLEU分数\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    metrics = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'BLEU']\n",
    "    values = [bleu_scores[m] for m in metrics]\n",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "\n",
    "    bars = ax.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    # 添加数值标签\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('BLEU分数评估', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ BLEU分数图已保存: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558885a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_translation_length(references, hypotheses, save_path):\n",
    "    \"\"\"\n",
    "    分析翻译长度分布\n",
    "\n",
    "    【是什么】：对比参考翻译和模型翻译的长度\n",
    "    【为什么】：检查模型是否倾向于生成过长或过短的翻译\n",
    "    \"\"\"\n",
    "    ref_lengths = [len(ref) for ref in references]\n",
    "    hyp_lengths = [len(hyp) for hyp in hypotheses]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # 长度分布直方图\n",
    "    axes[0].hist(ref_lengths, bins=30, alpha=0.7, label='参考翻译', color='blue', edgecolor='black')\n",
    "    axes[0].hist(hyp_lengths, bins=30, alpha=0.7, label='模型翻译', color='red', edgecolor='black')\n",
    "    axes[0].set_xlabel('句子长度（词数）', fontsize=12)\n",
    "    axes[0].set_ylabel('频数', fontsize=12)\n",
    "    axes[0].set_title('翻译长度分布', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 长度对比散点图\n",
    "    axes[1].scatter(ref_lengths, hyp_lengths, alpha=0.5, s=20)\n",
    "    max_len = max(max(ref_lengths), max(hyp_lengths))\n",
    "    axes[1].plot([0, max_len], [0, max_len], 'r--', linewidth=2, label='理想线')\n",
    "    axes[1].set_xlabel('参考翻译长度', fontsize=12)\n",
    "    axes[1].set_ylabel('模型翻译长度', fontsize=12)\n",
    "    axes[1].set_title('翻译长度对比', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 长度分析图已保存: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150b82c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主评估流程\"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer机器翻译 - 模型评估\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建结果目录\n",
    "    project_dir = Path(__file__).parent.parent\n",
    "    result_dir = project_dir / args.result_dir\n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # ============================================\n",
    "    # 1. 加载模型和处理器\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤1: 加载模型和处理器\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 加载处理器\n",
    "    processor = TranslationDataProcessor()\n",
    "    processor.load_processor(args.processor_path)\n",
    "    print(f\"✓ 处理器已加载\")\n",
    "    print(f\"  源语言词汇表: {len(processor.src_word2idx)}\")\n",
    "    print(f\"  目标语言词汇表: {len(processor.tgt_word2idx)}\")\n",
    "\n",
    "    # 加载模型\n",
    "    model = keras.models.load_model(args.model_path)\n",
    "    print(f\"✓ 模型已加载\")\n",
    "\n",
    "    # 重新包装为TransformerTranslationModel\n",
    "    translator = TransformerTranslationModel(\n",
    "        src_vocab_size=len(processor.src_word2idx),\n",
    "        tgt_vocab_size=len(processor.tgt_word2idx),\n",
    "        max_len=processor.max_len\n",
    "    )\n",
    "    translator.model = model\n",
    "\n",
    "    # ============================================\n",
    "    # 2. 加载测试数据\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤2: 加载测试数据\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        src_sentences, tgt_sentences = processor.load_parallel_data(\n",
    "            args.test_src,\n",
    "            args.test_tgt,\n",
    "            max_samples=args.max_samples\n",
    "        )\n",
    "\n",
    "        if not src_sentences:\n",
    "            raise FileNotFoundError(\"无法加载测试数据\")\n",
    "\n",
    "        # 编码和填充\n",
    "        src_encoded = processor.encode_sentences(src_sentences, 'src')\n",
    "        tgt_encoded = processor.encode_sentences(tgt_sentences, 'tgt', add_sos_eos=True)\n",
    "\n",
    "        src_test = processor.pad_sequences(src_encoded)\n",
    "        tgt_test = processor.pad_sequences(tgt_encoded)\n",
    "\n",
    "        print(f\"✓ 测试数据已加载\")\n",
    "        print(f\"  测试样本数: {len(src_test)}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n✗ 测试数据文件不存在: {e}\")\n",
    "        print(\"请确保测试数据文件存在\")\n",
    "        return\n",
    "\n",
    "    # ============================================\n",
    "    # 3. 翻译和评估\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤3: 翻译和评估\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    references, hypotheses, bleu_scores = translate_and_evaluate(\n",
    "        translator, processor, src_test, tgt_test\n",
    "    )\n",
    "\n",
    "    # 打印BLEU分数\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BLEU分数\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  BLEU: {bleu_scores['BLEU']:.4f}\")\n",
    "    print(f\"  BLEU-1: {bleu_scores['BLEU-1']:.4f}\")\n",
    "    print(f\"  BLEU-2: {bleu_scores['BLEU-2']:.4f}\")\n",
    "    print(f\"  BLEU-3: {bleu_scores['BLEU-3']:.4f}\")\n",
    "    print(f\"  BLEU-4: {bleu_scores['BLEU-4']:.4f}\")\n",
    "    print(f\"  简短惩罚(BP): {bleu_scores['BP']:.4f}\")\n",
    "    print(f\"  参考长度: {bleu_scores['ref_len']}\")\n",
    "    print(f\"  翻译长度: {bleu_scores['hyp_len']}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 4. 展示翻译样本\n",
    "    # ============================================\n",
    "    display_translation_samples(\n",
    "        processor, src_test, references, hypotheses,\n",
    "        num_samples=args.num_display\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 5. 保存结果\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤4: 保存结果\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 绘制BLEU分数\n",
    "    bleu_plot_path = result_dir / 'bleu_scores.png'\n",
    "    plot_bleu_scores(bleu_scores, bleu_plot_path)\n",
    "\n",
    "    # 分析翻译长度\n",
    "    length_plot_path = result_dir / 'translation_length_analysis.png'\n",
    "    analyze_translation_length(references, hypotheses, length_plot_path)\n",
    "\n",
    "    # 保存评估结果\n",
    "    eval_results_path = result_dir / 'evaluation_results.txt'\n",
    "    with open(eval_results_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"Transformer机器翻译 - 评估结果\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"BLEU分数:\\n\")\n",
    "        f.write(f\"  BLEU: {bleu_scores['BLEU']:.4f}\\n\")\n",
    "        f.write(f\"  BLEU-1: {bleu_scores['BLEU-1']:.4f}\\n\")\n",
    "        f.write(f\"  BLEU-2: {bleu_scores['BLEU-2']:.4f}\\n\")\n",
    "        f.write(f\"  BLEU-3: {bleu_scores['BLEU-3']:.4f}\\n\")\n",
    "        f.write(f\"  BLEU-4: {bleu_scores['BLEU-4']:.4f}\\n\")\n",
    "        f.write(f\"  简短惩罚(BP): {bleu_scores['BP']:.4f}\\n\\n\")\n",
    "\n",
    "        f.write(\"翻译样本:\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i in range(min(args.num_display, len(src_test))):\n",
    "            src_words = []\n",
    "            for idx in src_test[i]:\n",
    "                if idx == 0:\n",
    "                    break\n",
    "                word = processor.src_idx2word.get(idx, '<UNK>')\n",
    "                if word not in ['<PAD>', '<UNK>']:\n",
    "                    src_words.append(word)\n",
    "\n",
    "            f.write(f\"\\n样本 {i+1}:\\n\")\n",
    "            f.write(f\"  源句子: {' '.join(src_words)}\\n\")\n",
    "            f.write(f\"  参考翻译: {' '.join(references[i])}\\n\")\n",
    "            f.write(f\"  模型翻译: {' '.join(hypotheses[i])}\\n\")\n",
    "\n",
    "    print(f\"✓ 评估结果已保存: {eval_results_path}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 总结\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"评估总结\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ BLEU分数: {bleu_scores['BLEU']:.4f}\")\n",
    "    print(f\"✓ 评估结果已保存: {eval_results_path}\")\n",
    "    print(f\"✓ BLEU分数图已保存: {bleu_plot_path}\")\n",
    "    print(f\"✓ 长度分析图已保存: {length_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da9228",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
