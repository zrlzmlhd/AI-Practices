{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b519c8b",
   "metadata": {},
   "source": [
    "# Transformer机器翻译训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd58410",
   "metadata": {},
   "source": [
    "使用方法:\n",
    "    python src/train.py --src_path data/train.en --tgt_path data/train.zh --epochs 50\n",
    "    python src/train.py --max_samples 10000 --epochs 30 --batch_size 64\n",
    "\n",
    "【数据集建议】:\n",
    "- WMT系列数据集（英中、英德等）\n",
    "- IWSLT数据集（TED演讲翻译）\n",
    "- Tatoeba数据集（多语言句对）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1419390",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac49fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/03_自然语言处理项目/04_Transformer机器翻译_高级/src/train.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/03_自然语言处理项目/04_Transformer机器翻译_高级/src/train.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d481021",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data import prepare_translation_data\n",
    "from src.model import TransformerTranslationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6659a82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='训练Transformer机器翻译模型')\n",
    "\n",
    "    # 数据参数\n",
    "    parser.add_argument('--src_path', type=str, default='data/train.en',\n",
    "                       help='源语言训练文件路径')\n",
    "    parser.add_argument('--tgt_path', type=str, default='data/train.zh',\n",
    "                       help='目标语言训练文件路径')\n",
    "    parser.add_argument('--max_len', type=int, default=50,\n",
    "                       help='最大序列长度')\n",
    "    parser.add_argument('--max_samples', type=int, default=None,\n",
    "                       help='最大样本数（用于快速实验）')\n",
    "    parser.add_argument('--max_vocab_size', type=int, default=10000,\n",
    "                       help='最大词汇表大小')\n",
    "\n",
    "    # 模型参数\n",
    "    parser.add_argument('--num_layers', type=int, default=4,\n",
    "                       help='编码器/解码器层数')\n",
    "    parser.add_argument('--d_model', type=int, default=256,\n",
    "                       help='模型维度')\n",
    "    parser.add_argument('--num_heads', type=int, default=8,\n",
    "                       help='注意力头数')\n",
    "    parser.add_argument('--d_ff', type=int, default=1024,\n",
    "                       help='前馈网络维度')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.1,\n",
    "                       help='Dropout比率')\n",
    "\n",
    "    # 训练参数\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                       help='训练轮数')\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                       help='批大小')\n",
    "    parser.add_argument('--learning_rate', type=float, default=None,\n",
    "                       help='学习率（None表示使用自定义调度）')\n",
    "\n",
    "    # 保存路径\n",
    "    parser.add_argument('--model_dir', type=str, default='models',\n",
    "                       help='模型保存目录')\n",
    "    parser.add_argument('--result_dir', type=str, default='results',\n",
    "                       help='结果保存目录')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0535e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"\n",
    "    绘制训练历史\n",
    "\n",
    "    【可视化内容】:\n",
    "    - 训练/验证损失曲线\n",
    "    - 训练/验证准确率曲线\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # 损失曲线\n",
    "    axes[0].plot(history.history['loss'], label='训练损失', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='验证损失', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('训练/验证损失曲线', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 准确率曲线\n",
    "    if 'masked_accuracy' in history.history:\n",
    "        axes[1].plot(history.history['masked_accuracy'], label='训练准确率', linewidth=2)\n",
    "        axes[1].plot(history.history['val_masked_accuracy'], label='验证准确率', linewidth=2)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "        axes[1].set_title('训练/验证准确率曲线', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ 训练历史已保存: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5174fe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def translate_samples(model, processor, src_sequences, num_samples=5):\n",
    "    \"\"\"\n",
    "    翻译样本句子\n",
    "\n",
    "    【是什么】：展示模型的翻译效果\n",
    "    【为什么】：直观评估模型质量\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"翻译样本展示\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for i in range(min(num_samples, len(src_sequences))):\n",
    "        # 源句子\n",
    "        src_seq = src_sequences[i]\n",
    "        src_words = [processor.src_idx2word.get(idx, '<UNK>')\n",
    "                     for idx in src_seq if idx != 0]\n",
    "\n",
    "        # 翻译\n",
    "        translation = model.translate(\n",
    "            src_seq,\n",
    "            processor.tgt_word2idx,\n",
    "            processor.tgt_idx2word,\n",
    "            max_len=processor.max_len\n",
    "        )\n",
    "\n",
    "        print(f\"\\n样本 {i+1}:\")\n",
    "        print(f\"  源句子: {' '.join(src_words)}\")\n",
    "        print(f\"  翻译结果: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c15762",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主训练流程\"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer机器翻译 - 模型训练\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n训练配置:\")\n",
    "    print(f\"  源语言文件: {args.src_path}\")\n",
    "    print(f\"  目标语言文件: {args.tgt_path}\")\n",
    "    print(f\"  最大序列长度: {args.max_len}\")\n",
    "    print(f\"  最大样本数: {args.max_samples}\")\n",
    "    print(f\"  模型层数: {args.num_layers}\")\n",
    "    print(f\"  模型维度: {args.d_model}\")\n",
    "    print(f\"  注意力头数: {args.num_heads}\")\n",
    "    print(f\"  训练轮数: {args.epochs}\")\n",
    "    print(f\"  批大小: {args.batch_size}\")\n",
    "\n",
    "    # 创建保存目录\n",
    "    project_dir = Path(__file__).parent.parent\n",
    "    model_dir = project_dir / args.model_dir\n",
    "    result_dir = project_dir / args.result_dir\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # ============================================\n",
    "    # 1. 准备数据\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤1: 数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        (src_train, tgt_train), (src_val, tgt_val), processor = prepare_translation_data(\n",
    "            args.src_path,\n",
    "            args.tgt_path,\n",
    "            max_len=args.max_len,\n",
    "            max_samples=args.max_samples\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n✗ 数据文件不存在: {e}\")\n",
    "        print(\"\\n【数据集获取建议】:\")\n",
    "        print(\"1. WMT数据集: http://www.statmt.org/wmt19/translation-task.html\")\n",
    "        print(\"2. IWSLT数据集: https://wit3.fbk.eu/\")\n",
    "        print(\"3. Tatoeba数据集: https://tatoeba.org/en/downloads\")\n",
    "        print(\"\\n请下载数据集并放置在data/目录下\")\n",
    "        return\n",
    "\n",
    "    # 保存处理器\n",
    "    processor_path = model_dir / 'translation_processor.pkl'\n",
    "    processor.save_processor(processor_path)\n",
    "\n",
    "    print(f\"\\n数据统计:\")\n",
    "    print(f\"  训练集大小: {len(src_train)}\")\n",
    "    print(f\"  验证集大小: {len(src_val)}\")\n",
    "    print(f\"  源语言词汇表: {len(processor.src_word2idx)}\")\n",
    "    print(f\"  目标语言词汇表: {len(processor.tgt_word2idx)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 2. 创建模型\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤2: 创建模型\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    translator = TransformerTranslationModel(\n",
    "        src_vocab_size=len(processor.src_word2idx),\n",
    "        tgt_vocab_size=len(processor.tgt_word2idx),\n",
    "        max_len=args.max_len,\n",
    "        num_layers=args.num_layers,\n",
    "        d_model=args.d_model,\n",
    "        num_heads=args.num_heads,\n",
    "        d_ff=args.d_ff,\n",
    "        dropout_rate=args.dropout_rate\n",
    "    )\n",
    "\n",
    "    print(f\"\\n模型结构:\")\n",
    "    translator.summary()\n",
    "\n",
    "    # 计算参数量\n",
    "    total_params = translator.model.count_params()\n",
    "    print(f\"\\n总参数量: {total_params:,}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 3. 训练模型\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤3: 训练模型\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 回调函数\n",
    "    model_path = model_dir / 'transformer_translation_model.h5'\n",
    "    callbacks = [\n",
    "        # 【ModelCheckpoint】：保存最佳模型\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # 【EarlyStopping】：早停\n",
    "        # 【为什么】：防止过拟合，节省训练时间\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # 【ReduceLROnPlateau】：学习率衰减\n",
    "        # 【为什么】：当验证损失不再下降时降低学习率\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # 【TensorBoard】：可视化训练过程\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=result_dir / 'logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 训练\n",
    "    print(f\"\\n开始训练...\")\n",
    "    history = translator.train(\n",
    "        src_train, tgt_train,\n",
    "        src_val, tgt_val,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✓ 训练完成！\")\n",
    "    print(f\"  最佳验证损失: {min(history.history['val_loss']):.4f}\")\n",
    "    if 'val_masked_accuracy' in history.history:\n",
    "        print(f\"  最佳验证准确率: {max(history.history['val_masked_accuracy']):.4f}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 4. 保存结果\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤4: 保存结果\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 绘制训练历史\n",
    "    plot_path = result_dir / 'training_history.png'\n",
    "    plot_training_history(history, plot_path)\n",
    "\n",
    "    # 保存训练历史\n",
    "    history_path = result_dir / 'training_history.npy'\n",
    "    np.save(history_path, history.history)\n",
    "    print(f\"✓ 训练历史已保存: {history_path}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 5. 翻译样本\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤5: 翻译样本\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    translate_samples(translator, processor, src_val, num_samples=5)\n",
    "\n",
    "    # ============================================\n",
    "    # 总结\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"训练总结\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ 模型已保存: {model_path}\")\n",
    "    print(f\"✓ 处理器已保存: {processor_path}\")\n",
    "    print(f\"✓ 训练历史已保存: {plot_path}\")\n",
    "    print(f\"\\n使用以下命令进行评估:\")\n",
    "    print(f\"  python src/evaluate.py --model_path {model_path} --processor_path {processor_path}\")\n",
    "    print(\"\\n使用TensorBoard查看训练过程:\")\n",
    "    print(f\"  tensorboard --logdir {result_dir / 'logs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fd760",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置随机种子\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
