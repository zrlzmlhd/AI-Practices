{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81ed572",
   "metadata": {},
   "source": [
    "# Transformer机器翻译模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489d8b2",
   "metadata": {},
   "source": [
    "本模块实现完整的Transformer Seq2Seq架构：\n",
    "1. Transformer Encoder（编码器）\n",
    "2. Transformer Decoder（解码器）\n",
    "3. 完整的翻译模型\n",
    "4. Beam Search解码\n",
    "\n",
    "每个组件都有详细的注释说明设计思路和参数选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6299c",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cbcfd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1743da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    位置编码\n",
    "\n",
    "    【是什么】：为序列添加位置信息\n",
    "    【为什么】：Transformer没有位置信息，需要显式添加\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, d_model, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self._positional_encoding(max_len, d_model)\n",
    "\n",
    "    def _positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"生成位置编码\"\"\"\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        # 偶数维度用sin，奇数维度用cos\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pos_encoding[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0633c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer编码器\n",
    "\n",
    "    【是什么】：多层编码器层的堆叠\n",
    "    【做什么】：将源语言序列编码为上下文表示\n",
    "    【为什么】：\n",
    "        - 捕获源语言的语义信息\n",
    "        - 提供给解码器使用\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                 vocab_size, max_len, dropout_rate=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 词嵌入\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        # 编码器层\n",
    "        self.encoder_layers = [\n",
    "            self._create_encoder_layer(d_model, num_heads, d_ff, dropout_rate, i)\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def _create_encoder_layer(self, d_model, num_heads, d_ff, dropout_rate, layer_id):\n",
    "        \"\"\"创建单个编码器层\"\"\"\n",
    "        # Multi-Head Attention\n",
    "        mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name=f'encoder_mha_{layer_id}'\n",
    "        )\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn = keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu', name=f'encoder_ffn1_{layer_id}'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(d_model, name=f'encoder_ffn2_{layer_id}')\n",
    "        ], name=f'encoder_ffn_{layer_id}')\n",
    "\n",
    "        # Layer Normalization\n",
    "        layernorm1 = layers.LayerNormalization(epsilon=1e-6, name=f'encoder_ln1_{layer_id}')\n",
    "        layernorm2 = layers.LayerNormalization(epsilon=1e-6, name=f'encoder_ln2_{layer_id}')\n",
    "\n",
    "        # Dropout\n",
    "        dropout1 = layers.Dropout(dropout_rate, name=f'encoder_dropout1_{layer_id}')\n",
    "        dropout2 = layers.Dropout(dropout_rate, name=f'encoder_dropout2_{layer_id}')\n",
    "\n",
    "        return {\n",
    "            'mha': mha,\n",
    "            'ffn': ffn,\n",
    "            'layernorm1': layernorm1,\n",
    "            'layernorm2': layernorm2,\n",
    "            'dropout1': dropout1,\n",
    "            'dropout2': dropout2\n",
    "        }\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 输入序列（词ID）\n",
    "            mask: 填充掩码\n",
    "            training: 是否训练模式\n",
    "\n",
    "        Returns:\n",
    "            编码后的表示\n",
    "        \"\"\"\n",
    "        # 词嵌入\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # 位置编码\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 通过所有编码器层\n",
    "        for layer_dict in self.encoder_layers:\n",
    "            # Multi-Head Attention\n",
    "            attn_output = layer_dict['mha'](x, x, attention_mask=mask)\n",
    "            attn_output = layer_dict['dropout1'](attn_output, training=training)\n",
    "            x = layer_dict['layernorm1'](x + attn_output)\n",
    "\n",
    "            # Feed Forward\n",
    "            ffn_output = layer_dict['ffn'](x, training=training)\n",
    "            ffn_output = layer_dict['dropout2'](ffn_output, training=training)\n",
    "            x = layer_dict['layernorm2'](x + ffn_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce24d13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer解码器\n",
    "\n",
    "    【是什么】：多层解码器层的堆叠\n",
    "    【做什么】：根据编码器输出生成目标语言序列\n",
    "    【为什么】：\n",
    "        - 自回归生成（一个词一个词生成）\n",
    "        - 使用编码器的上下文信息\n",
    "        - 通过交叉注意力连接编码器和解码器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                 vocab_size, max_len, dropout_rate=0.1, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 词嵌入\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(max_len, d_model)\n",
    "\n",
    "        # 解码器层\n",
    "        self.decoder_layers = [\n",
    "            self._create_decoder_layer(d_model, num_heads, d_ff, dropout_rate, i)\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def _create_decoder_layer(self, d_model, num_heads, d_ff, dropout_rate, layer_id):\n",
    "        \"\"\"创建单个解码器层\"\"\"\n",
    "        # Masked Multi-Head Attention（自注意力）\n",
    "        masked_mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name=f'decoder_masked_mha_{layer_id}'\n",
    "        )\n",
    "\n",
    "        # Multi-Head Attention（交叉注意力）\n",
    "        # 【是什么】：解码器关注编码器的输出\n",
    "        # 【为什么】：\n",
    "        #   - 获取源语言的上下文信息\n",
    "        #   - 决定翻译哪个源词\n",
    "        cross_mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name=f'decoder_cross_mha_{layer_id}'\n",
    "        )\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn = keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu', name=f'decoder_ffn1_{layer_id}'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(d_model, name=f'decoder_ffn2_{layer_id}')\n",
    "        ], name=f'decoder_ffn_{layer_id}')\n",
    "\n",
    "        # Layer Normalization\n",
    "        layernorm1 = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln1_{layer_id}')\n",
    "        layernorm2 = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln2_{layer_id}')\n",
    "        layernorm3 = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln3_{layer_id}')\n",
    "\n",
    "        # Dropout\n",
    "        dropout1 = layers.Dropout(dropout_rate, name=f'decoder_dropout1_{layer_id}')\n",
    "        dropout2 = layers.Dropout(dropout_rate, name=f'decoder_dropout2_{layer_id}')\n",
    "        dropout3 = layers.Dropout(dropout_rate, name=f'decoder_dropout3_{layer_id}')\n",
    "\n",
    "        return {\n",
    "            'masked_mha': masked_mha,\n",
    "            'cross_mha': cross_mha,\n",
    "            'ffn': ffn,\n",
    "            'layernorm1': layernorm1,\n",
    "            'layernorm2': layernorm2,\n",
    "            'layernorm3': layernorm3,\n",
    "            'dropout1': dropout1,\n",
    "            'dropout2': dropout2,\n",
    "            'dropout3': dropout3\n",
    "        }\n",
    "\n",
    "    def call(self, x, encoder_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 目标序列（词ID）\n",
    "            encoder_output: 编码器输出\n",
    "            look_ahead_mask: 前瞻掩码（防止看到未来）\n",
    "            padding_mask: 填充掩码\n",
    "            training: 是否训练模式\n",
    "\n",
    "        Returns:\n",
    "            解码后的表示\n",
    "        \"\"\"\n",
    "        # 词嵌入\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # 位置编码\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 通过所有解码器层\n",
    "        for layer_dict in self.decoder_layers:\n",
    "            # ============================================\n",
    "            # 子层1: Masked Self-Attention\n",
    "            # ============================================\n",
    "            # 【是什么】：解码器的自注意力\n",
    "            # 【为什么需要mask】：\n",
    "            #   - 防止看到未来的词\n",
    "            #   - 保持自回归特性\n",
    "            attn1 = layer_dict['masked_mha'](\n",
    "                x, x,\n",
    "                attention_mask=look_ahead_mask\n",
    "            )\n",
    "            attn1 = layer_dict['dropout1'](attn1, training=training)\n",
    "            x = layer_dict['layernorm1'](x + attn1)\n",
    "\n",
    "            # ============================================\n",
    "            # 子层2: Cross-Attention\n",
    "            # ============================================\n",
    "            # 【是什么】：解码器关注编码器输出\n",
    "            # 【为什么】：\n",
    "            #   - 获取源语言信息\n",
    "            #   - Query来自解码器，Key/Value来自编码器\n",
    "            attn2 = layer_dict['cross_mha'](\n",
    "                x, encoder_output,\n",
    "                attention_mask=padding_mask\n",
    "            )\n",
    "            attn2 = layer_dict['dropout2'](attn2, training=training)\n",
    "            x = layer_dict['layernorm2'](x + attn2)\n",
    "\n",
    "            # ============================================\n",
    "            # 子层3: Feed Forward\n",
    "            # ============================================\n",
    "            ffn_output = layer_dict['ffn'](x, training=training)\n",
    "            ffn_output = layer_dict['dropout3'](ffn_output, training=training)\n",
    "            x = layer_dict['layernorm3'](x + ffn_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7993c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerTranslationModel:\n",
    "    \"\"\"\n",
    "    Transformer机器翻译模型\n",
    "\n",
    "    【是什么】：完整的Encoder-Decoder Transformer\n",
    "    【做什么】：将源语言翻译为目标语言\n",
    "    【为什么】：\n",
    "        - Encoder捕获源语言语义\n",
    "        - Decoder生成目标语言\n",
    "        - 注意力机制对齐源词和目标词\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 max_len=50,\n",
    "                 num_layers=4,\n",
    "                 d_model=256,\n",
    "                 num_heads=8,\n",
    "                 d_ff=1024,\n",
    "                 dropout_rate=0.1,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        初始化翻译模型\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size: 源语言词汇表大小\n",
    "            tgt_vocab_size: 目标语言词汇表大小\n",
    "            max_len: 最大序列长度\n",
    "            num_layers: 编码器/解码器层数\n",
    "            d_model: 模型维度\n",
    "            num_heads: 注意力头数\n",
    "            d_ff: 前馈网络维度\n",
    "            dropout_rate: Dropout比率\n",
    "        \"\"\"\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # 创建模型\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        构建完整的Transformer模型\n",
    "\n",
    "        【结构】：\n",
    "        源语言输入 → Encoder → 编码表示\n",
    "                                    ↓\n",
    "        目标语言输入 → Decoder ← 交叉注意力 → 输出概率\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 输入层\n",
    "        # ============================================\n",
    "        # 源语言输入\n",
    "        encoder_inputs = layers.Input(\n",
    "            shape=(self.max_len,),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs'\n",
    "        )\n",
    "\n",
    "        # 目标语言输入（训练时使用）\n",
    "        decoder_inputs = layers.Input(\n",
    "            shape=(self.max_len,),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_inputs'\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 编码器\n",
    "        # ============================================\n",
    "        # 【是什么】：处理源语言\n",
    "        # 【做什么】：将源语言编码为上下文表示\n",
    "        encoder = TransformerEncoder(\n",
    "            num_layers=self.num_layers,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "            vocab_size=self.src_vocab_size,\n",
    "            max_len=self.max_len,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            name='encoder'\n",
    "        )\n",
    "\n",
    "        # 创建编码器填充掩码\n",
    "        encoder_padding_mask = self._create_padding_mask(encoder_inputs)\n",
    "\n",
    "        # 编码\n",
    "        encoder_output = encoder(\n",
    "            encoder_inputs,\n",
    "            mask=encoder_padding_mask,\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 解码器\n",
    "        # ============================================\n",
    "        # 【是什么】：生成目标语言\n",
    "        # 【做什么】：根据编码器输出生成翻译\n",
    "        decoder = TransformerDecoder(\n",
    "            num_layers=self.num_layers,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "            vocab_size=self.tgt_vocab_size,\n",
    "            max_len=self.max_len,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            name='decoder'\n",
    "        )\n",
    "\n",
    "        # 创建解码器掩码\n",
    "        # 【前瞻掩码】：防止看到未来的词\n",
    "        look_ahead_mask = self._create_look_ahead_mask(self.max_len)\n",
    "        decoder_padding_mask = self._create_padding_mask(decoder_inputs)\n",
    "        combined_mask = tf.maximum(decoder_padding_mask, look_ahead_mask)\n",
    "\n",
    "        # 解码\n",
    "        decoder_output = decoder(\n",
    "            decoder_inputs,\n",
    "            encoder_output,\n",
    "            look_ahead_mask=combined_mask,\n",
    "            padding_mask=encoder_padding_mask,\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 输出层\n",
    "        # ============================================\n",
    "        # 【是什么】：线性层 + Softmax\n",
    "        # 【做什么】：输出每个词的概率分布\n",
    "        outputs = layers.Dense(\n",
    "            self.tgt_vocab_size,\n",
    "            activation='softmax',\n",
    "            name='output'\n",
    "        )(decoder_output)\n",
    "\n",
    "        # 创建模型\n",
    "        model = keras.Model(\n",
    "            inputs=[encoder_inputs, decoder_inputs],\n",
    "            outputs=outputs,\n",
    "            name='transformer_translation'\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _create_padding_mask(self, seq):\n",
    "        \"\"\"创建填充掩码\"\"\"\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def _create_look_ahead_mask(self, size):\n",
    "        \"\"\"创建前瞻掩码\"\"\"\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask[tf.newaxis, tf.newaxis, :, :]\n",
    "\n",
    "    def compile_model(self, learning_rate=0.0001):\n",
    "        \"\"\"\n",
    "        编译模型\n",
    "\n",
    "        Args:\n",
    "            learning_rate: 学习率\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 学习率调度\n",
    "        # ============================================\n",
    "        # 【是什么】：Warmup + 衰减\n",
    "        # 【为什么】：\n",
    "        #   - Warmup：开始时小学习率，逐渐增大\n",
    "        #   - 衰减：后期逐渐减小学习率\n",
    "        #   - Transformer训练的标准做法\n",
    "\n",
    "        class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "            def __init__(self, d_model, warmup_steps=4000):\n",
    "                super(CustomSchedule, self).__init__()\n",
    "                self.d_model = tf.cast(d_model, tf.float32)\n",
    "                self.warmup_steps = warmup_steps\n",
    "\n",
    "            def __call__(self, step):\n",
    "                step = tf.cast(step, tf.float32)\n",
    "                arg1 = tf.math.rsqrt(step)\n",
    "                arg2 = step * (self.warmup_steps ** -1.5)\n",
    "                return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        learning_rate = CustomSchedule(self.d_model)\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.98,\n",
    "            epsilon=1e-9\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 损失函数\n",
    "        # ============================================\n",
    "        # 【是什么】：Sparse Categorical Crossentropy\n",
    "        # 【为什么】：\n",
    "        #   - 多分类问题（预测下一个词）\n",
    "        #   - 忽略填充位置的损失\n",
    "\n",
    "        def masked_loss(y_true, y_pred):\n",
    "            \"\"\"带掩码的损失函数\"\"\"\n",
    "            loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=False,\n",
    "                reduction='none'\n",
    "            )\n",
    "            loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "            # 创建掩码（忽略填充位置）\n",
    "            mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "            loss *= mask\n",
    "\n",
    "            return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "        def masked_accuracy(y_true, y_pred):\n",
    "            \"\"\"带掩码的准确率\"\"\"\n",
    "            y_pred = tf.argmax(y_pred, axis=-1)\n",
    "            y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "            match = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "            mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "\n",
    "            return tf.reduce_sum(match * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=masked_loss,\n",
    "            metrics=[masked_accuracy]\n",
    "        )\n",
    "\n",
    "    def train(self, src_train, tgt_train, src_val, tgt_val,\n",
    "              epochs=50, batch_size=64, callbacks=None, verbose=1):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        Args:\n",
    "            src_train: 源语言训练数据\n",
    "            tgt_train: 目标语言训练数据\n",
    "            src_val: 源语言验证数据\n",
    "            tgt_val: 目标语言验证数据\n",
    "            epochs: 训练轮数\n",
    "            batch_size: 批大小\n",
    "            callbacks: 回调函数\n",
    "            verbose: 详细程度\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        # 编译模型\n",
    "        self.compile_model()\n",
    "\n",
    "        # 准备训练数据\n",
    "        # 【重要】：解码器输入是目标序列去掉最后一个词\n",
    "        #          解码器输出是目标序列去掉第一个词\n",
    "        decoder_input_train = tgt_train[:, :-1]\n",
    "        decoder_output_train = tgt_train[:, 1:]\n",
    "\n",
    "        decoder_input_val = tgt_val[:, :-1]\n",
    "        decoder_output_val = tgt_val[:, 1:]\n",
    "\n",
    "        # 训练\n",
    "        history = self.model.fit(\n",
    "            [src_train, decoder_input_train],\n",
    "            decoder_output_train,\n",
    "            validation_data=([src_val, decoder_input_val], decoder_output_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def translate(self, src_sequence, tgt_word2idx, tgt_idx2word, max_len=50):\n",
    "        \"\"\"\n",
    "        翻译单个句子（贪心解码）\n",
    "\n",
    "        Args:\n",
    "            src_sequence: 源语言序列\n",
    "            tgt_word2idx: 目标语言词到ID映射\n",
    "            tgt_idx2word: 目标语言ID到词映射\n",
    "            max_len: 最大生成长度\n",
    "\n",
    "        Returns:\n",
    "            翻译结果\n",
    "        \"\"\"\n",
    "        # 编码源语言\n",
    "        src_sequence = tf.expand_dims(src_sequence, 0)\n",
    "\n",
    "        # 初始化目标序列（从SOS开始）\n",
    "        sos_id = tgt_word2idx['<SOS>']\n",
    "        eos_id = tgt_word2idx['<EOS>']\n",
    "\n",
    "        output_sequence = [sos_id]\n",
    "\n",
    "        # 自回归生成\n",
    "        for _ in range(max_len):\n",
    "            # 准备解码器输入\n",
    "            decoder_input = tf.expand_dims(output_sequence, 0)\n",
    "            decoder_input = tf.pad(\n",
    "                decoder_input,\n",
    "                [[0, 0], [0, self.max_len - len(output_sequence)]]\n",
    "            )\n",
    "\n",
    "            # 预测下一个词\n",
    "            predictions = self.model([src_sequence, decoder_input], training=False)\n",
    "\n",
    "            # 取最后一个位置的预测\n",
    "            predicted_id = tf.argmax(predictions[0, len(output_sequence)-1, :]).numpy()\n",
    "\n",
    "            # 如果预测到EOS，停止生成\n",
    "            if predicted_id == eos_id:\n",
    "                break\n",
    "\n",
    "            output_sequence.append(predicted_id)\n",
    "\n",
    "        # 解码为词\n",
    "        translated_words = [\n",
    "            tgt_idx2word.get(idx, '<UNK>')\n",
    "            for idx in output_sequence[1:]  # 跳过SOS\n",
    "        ]\n",
    "\n",
    "        return ' '.join(translated_words)\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        self.model.save(filepath)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"打印模型摘要\"\"\"\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84184962",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer翻译模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试参数\n",
    "    src_vocab_size = 1000\n",
    "    tgt_vocab_size = 1000\n",
    "    max_len = 20\n",
    "    batch_size = 4\n",
    "\n",
    "    # 创建随机数据\n",
    "    src_train = np.random.randint(1, src_vocab_size, (100, max_len))\n",
    "    tgt_train = np.random.randint(1, tgt_vocab_size, (100, max_len))\n",
    "    src_val = np.random.randint(1, src_vocab_size, (20, max_len))\n",
    "    tgt_val = np.random.randint(1, tgt_vocab_size, (20, max_len))\n",
    "\n",
    "    # 创建模型\n",
    "    translator = TransformerTranslationModel(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        max_len=max_len,\n",
    "        num_layers=2,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        d_ff=512\n",
    "    )\n",
    "\n",
    "    print(f\"\\n模型结构:\")\n",
    "    translator.summary()\n",
    "\n",
    "    # 训练\n",
    "    print(f\"\\n训练模型...\")\n",
    "    history = translator.train(\n",
    "        src_train, tgt_train,\n",
    "        src_val, tgt_val,\n",
    "        epochs=2,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    print(\"\\n✓ 模型测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
