{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83d26f7",
   "metadata": {},
   "source": [
    "# 数据加载和预处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b84514",
   "metadata": {},
   "source": [
    "本模块负责：\n",
    "1. 加载IMDB数据集\n",
    "2. 文本预处理（分词、去除停用词等）\n",
    "3. 构建词汇表\n",
    "4. 序列填充和截断"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ec9fd",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302640a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/03_自然语言处理项目/01_情感分析_LSTM入门/src/data.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/03_自然语言处理项目/01_情感分析_LSTM入门/src/data.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7409f63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "project_root = Path(__file__).parent.parent.parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.common import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a441f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_imdb_data(max_words=10000, max_len=200, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    加载IMDB数据集（使用Keras内置数据集）\n",
    "\n",
    "    Args:\n",
    "        max_words: 词汇表最大大小\n",
    "        max_len: 序列最大长度\n",
    "        test_size: 测试集比例\n",
    "        random_state: 随机种子\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"加载IMDB数据集\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    set_seed(random_state)\n",
    "\n",
    "    # 加载数据\n",
    "    print(\"\\n1. 从Keras加载数据...\")\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = keras.datasets.imdb.load_data(\n",
    "        num_words=max_words\n",
    "    )\n",
    "\n",
    "    print(f\"   训练集大小: {len(X_train_full)}\")\n",
    "    print(f\"   测试集大小: {len(X_test)}\")\n",
    "\n",
    "    # 填充序列\n",
    "    print(f\"\\n2. 填充序列到固定长度 {max_len}...\")\n",
    "    X_train_full = pad_sequences(X_train_full, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    print(f\"\\n3. 划分训练集和验证集 (test_size={test_size})...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_full\n",
    "    )\n",
    "\n",
    "    print(f\"   训练集: {len(X_train)} 样本\")\n",
    "    print(f\"   验证集: {len(X_val)} 样本\")\n",
    "    print(f\"   测试集: {len(X_test)} 样本\")\n",
    "\n",
    "    # 数据统计\n",
    "    print(f\"\\n4. 数据统计:\")\n",
    "    print(f\"   序列形状: {X_train.shape}\")\n",
    "    print(f\"   正面样本: {np.sum(y_train == 1)} ({np.mean(y_train == 1)*100:.1f}%)\")\n",
    "    print(f\"   负面样本: {np.sum(y_train == 0)} ({np.mean(y_train == 0)*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\n✓ 数据加载完成！\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb08eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_imdb_from_file(data_path, max_words=10000, max_len=200, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    从文件加载IMDB数据集（如果有本地文件）\n",
    "\n",
    "    Args:\n",
    "        data_path: 数据文件路径\n",
    "        max_words: 词汇表最大大小\n",
    "        max_len: 序列最大长度\n",
    "        test_size: 测试集比例\n",
    "        random_state: 随机种子\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"从文件加载IMDB数据集\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    set_seed(random_state)\n",
    "\n",
    "    # 读取数据\n",
    "    print(f\"\\n1. 读取数据文件: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    print(f\"   数据大小: {len(df)}\")\n",
    "    print(f\"   列名: {df.columns.tolist()}\")\n",
    "\n",
    "    # 假设数据格式: review, sentiment\n",
    "    texts = df['review'].values\n",
    "    labels = df['sentiment'].map({'positive': 1, 'negative': 0}).values\n",
    "\n",
    "    # 文本预处理\n",
    "    print(f\"\\n2. 文本预处理...\")\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    X = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    y = labels\n",
    "\n",
    "    print(f\"   词汇表大小: {len(tokenizer.word_index)}\")\n",
    "    print(f\"   使用词汇数: {max_words}\")\n",
    "\n",
    "    # 划分数据集\n",
    "    print(f\"\\n3. 划分数据集...\")\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_full\n",
    "    )\n",
    "\n",
    "    print(f\"   训练集: {len(X_train)} 样本\")\n",
    "    print(f\"   验证集: {len(X_val)} 样本\")\n",
    "    print(f\"   测试集: {len(X_test)} 样本\")\n",
    "\n",
    "    print(\"\\n✓ 数据加载完成！\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5ee4f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_word_index():\n",
    "    \"\"\"\n",
    "    获取IMDB数据集的词汇索引\n",
    "\n",
    "    Returns:\n",
    "        word_index: 词汇到索引的映射字典\n",
    "    \"\"\"\n",
    "    word_index = keras.datasets.imdb.get_word_index()\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f94797",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def decode_review(encoded_review, word_index=None):\n",
    "    \"\"\"\n",
    "    将编码的评论解码为文本\n",
    "\n",
    "    Args:\n",
    "        encoded_review: 编码的评论（整数序列）\n",
    "        word_index: 词汇索引字典\n",
    "\n",
    "    Returns:\n",
    "        decoded_text: 解码后的文本\n",
    "    \"\"\"\n",
    "    if word_index is None:\n",
    "        word_index = get_word_index()\n",
    "\n",
    "    # 反转词汇索引\n",
    "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "    # 解码（注意：索引偏移3，因为0,1,2是保留索引）\n",
    "    decoded_text = ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i > 0])\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804ee6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_sequence_lengths(sequences):\n",
    "    \"\"\"\n",
    "    分析序列长度分布\n",
    "\n",
    "    Args:\n",
    "        sequences: 序列列表\n",
    "\n",
    "    Returns:\n",
    "        stats: 统计信息字典\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    stats = {\n",
    "        'min': np.min(lengths),\n",
    "        'max': np.max(lengths),\n",
    "        'mean': np.mean(lengths),\n",
    "        'median': np.median(lengths),\n",
    "        'std': np.std(lengths),\n",
    "        'percentile_25': np.percentile(lengths, 25),\n",
    "        'percentile_50': np.percentile(lengths, 50),\n",
    "        'percentile_75': np.percentile(lengths, 75),\n",
    "        'percentile_90': np.percentile(lengths, 90),\n",
    "        'percentile_95': np.percentile(lengths, 95),\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d7aa3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试数据加载\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"数据加载模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 加载数据\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_imdb_data(\n",
    "        max_words=10000,\n",
    "        max_len=200,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 显示样本\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"样本示例\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    word_index = get_word_index()\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"\\n样本 {i+1}:\")\n",
    "        print(f\"标签: {'正面' if y_train[i] == 1 else '负面'}\")\n",
    "        print(f\"编码序列: {X_train[i][:20]}...\")\n",
    "        print(f\"解码文本: {decode_review(X_train[i], word_index)[:200]}...\")\n",
    "\n",
    "    print(\"\\n✓ 数据加载模块测试完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
