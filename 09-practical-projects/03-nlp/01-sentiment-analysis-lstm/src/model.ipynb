{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9be7883",
   "metadata": {},
   "source": [
    "# LSTM情感分析模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208107",
   "metadata": {},
   "source": [
    "本模块包含详细的LSTM模型实现，每一层都有详细的注释说明：\n",
    "1. 这一层是什么\n",
    "2. 这一层做什么\n",
    "3. 为什么使用这一层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39690c7d",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc3c81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/03_自然语言处理项目/01_情感分析_LSTM入门/src/model.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/03_自然语言处理项目/01_情感分析_LSTM入门/src/model.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29913890",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "project_root = Path(__file__).parent.parent.parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.common import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a3d9c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    LSTM情感分析器\n",
    "\n",
    "    支持三种模型架构：\n",
    "    1. simple_lstm: 单层LSTM（入门）\n",
    "    2. bilstm: 双向LSTM（中级）\n",
    "    3. stacked_lstm: 堆叠LSTM（高级）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type='simple_lstm', max_words=10000, max_len=200, random_state=42):\n",
    "        \"\"\"\n",
    "        初始化情感分析器\n",
    "\n",
    "        Args:\n",
    "            model_type: 模型类型 ['simple_lstm', 'bilstm', 'stacked_lstm']\n",
    "            max_words: 词汇表大小（最常用的词数量）\n",
    "            max_len: 序列最大长度（评论的最大词数）\n",
    "            random_state: 随机种子（保证结果可复现）\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.max_words = max_words\n",
    "        self.max_len = max_len\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "        # 设置随机种子，保证结果可复现\n",
    "        set_seed(random_state)\n",
    "\n",
    "    def create_simple_lstm(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        创建简单的单层LSTM模型（入门级）\n",
    "\n",
    "        架构：Embedding → LSTM → Dense → Dropout → Output\n",
    "\n",
    "        适用场景：\n",
    "        - 学习LSTM基础\n",
    "        - 数据量较小\n",
    "        - 快速原型验证\n",
    "\n",
    "        Args:\n",
    "            embedding_dim: 词嵌入维度\n",
    "\n",
    "        Returns:\n",
    "            Keras模型\n",
    "        \"\"\"\n",
    "        model = models.Sequential(name='simple_lstm')\n",
    "\n",
    "        # ============================================\n",
    "        # 第1层：Embedding（词嵌入层）\n",
    "        # ============================================\n",
    "        # 【是什么】：将整数索引转换为稠密向量\n",
    "        # 【做什么】：\n",
    "        #   输入：[5, 234, 12, ...]  (词的索引)\n",
    "        #   输出：[[0.2, 0.5, ...], [0.1, -0.3, ...], ...]  (每个词的向量表示)\n",
    "        # 【为什么】：\n",
    "        #   - 词需要用向量表示才能输入神经网络\n",
    "        #   - 稠密向量能捕获词之间的语义关系\n",
    "        #   - 例如：\"好\"和\"棒\"的向量会比较接近\n",
    "        # 【参数选择】：\n",
    "        #   - max_words=10000: 词汇表大小\n",
    "        #     * 太小(1000): 很多词会变成\"未知词\"\n",
    "        #     * 太大(50000): 稀有词太多，训练效果差\n",
    "        #     * 10000: 经验最佳值，覆盖常用词\n",
    "        #   - embedding_dim=128: 每个词的向量维度\n",
    "        #     * 太小(32): 表达能力不足\n",
    "        #     * 太大(512): 参数过多，容易过拟合\n",
    "        #     * 128: 平衡表达能力和效率\n",
    "        #   - input_length=max_len: 固定序列长度\n",
    "        #     * 短评论会填充0\n",
    "        #     * 长评论会截断\n",
    "        model.add(layers.Embedding(\n",
    "            input_dim=self.max_words,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_len,\n",
    "            name='embedding_word_vectors'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第2层：LSTM（长短期记忆层）- 核心层\n",
    "        # ============================================\n",
    "        # 【是什么】：能够记住长期依赖关系的循环神经网络\n",
    "        # 【做什么】：\n",
    "        #   - 顺序处理每个词的向量\n",
    "        #   - 通过三个门控制信息流动：\n",
    "        #     1. 遗忘门：决定丢弃哪些旧信息\n",
    "        #     2. 输入门：决定存储哪些新信息\n",
    "        #     3. 输出门：决定输出哪些信息\n",
    "        #   - 维护一个记忆单元，保存重要的上下文信息\n",
    "        # 【为什么】：\n",
    "        #   - 情感分析需要理解上下文关系\n",
    "        #   - 例如：\"不好\"需要理解\"不\"对\"好\"的否定作用\n",
    "        #   - LSTM能记住前面的词，理解整个句子的意思\n",
    "        # 【参数选择】：\n",
    "        #   - units=128: LSTM单元数（记忆容量）\n",
    "        #     * 太小(32): 记忆容量不足，无法捕获复杂模式\n",
    "        #     * 太大(512): 计算量大，容易过拟合\n",
    "        #     * 128: 平衡记忆能力和效率\n",
    "        #     * 与embedding_dim相同：信息流动更顺畅\n",
    "        #   - dropout=0.2: 输入dropout率\n",
    "        #     * 作用：随机丢弃20%的输入连接\n",
    "        #     * 为什么：防止过度依赖某些特定词\n",
    "        #     * 20%：不会太影响训练，又能防止过拟合\n",
    "        #   - recurrent_dropout=0.2: 循环dropout率\n",
    "        #     * 作用：在时间步之间应用dropout\n",
    "        #     * 为什么：防止在序列处理中过拟合\n",
    "        #     * 注意：不能太大，否则破坏时序信息\n",
    "        #   - return_sequences=False: 只返回最后一个输出\n",
    "        #     * False: 用于分类任务（返回整个句子的表示）\n",
    "        #     * True: 用于序列标注（返回每个词的表示）\n",
    "        model.add(layers.LSTM(\n",
    "            units=128,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            return_sequences=False,\n",
    "            name='lstm_sequence_processing'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第3层：Dense（全连接层）- 特征组合\n",
    "        # ============================================\n",
    "        # 【是什么】：全连接的神经网络层\n",
    "        # 【做什么】：\n",
    "        #   - 将LSTM的128维输出组合成64维特征\n",
    "        #   - 学习哪些特征组合对情感分析重要\n",
    "        # 【为什么】：\n",
    "        #   - 降维：去除冗余信息，减少过拟合\n",
    "        #   - 特征组合：学习更高层次的抽象特征\n",
    "        #   - 为最终分类做准备\n",
    "        # 【参数选择】：\n",
    "        #   - units=64: 神经元数量\n",
    "        #     * 为什么是64: LSTM输出的一半，适度压缩\n",
    "        #     * 压缩的好处：去除冗余，减少过拟合\n",
    "        #   - activation='relu': ReLU激活函数\n",
    "        #     * 作用：引入非线性，过滤负值\n",
    "        #     * 公式：f(x) = max(0, x)\n",
    "        #     * 为什么用ReLU：\n",
    "        #       1. 计算简单，训练快\n",
    "        #       2. 缓解梯度消失问题\n",
    "        #       3. 生物学上更合理（神经元激活模式）\n",
    "        model.add(layers.Dense(\n",
    "            units=64,\n",
    "            activation='relu',\n",
    "            name='dense_feature_combination'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第4层：Dropout（正则化层）\n",
    "        # ============================================\n",
    "        # 【是什么】：随机丢弃神经元的正则化技术\n",
    "        # 【做什么】：\n",
    "        #   - 训练时：随机关闭50%的神经元\n",
    "        #   - 测试时：使用所有神经元（输出乘以0.5）\n",
    "        # 【为什么】：\n",
    "        #   - 防止过拟合：强制网络不依赖特定神经元\n",
    "        #   - 集成效果：相当于训练多个子网络\n",
    "        #   - 提高泛化能力：学习更鲁棒的特征\n",
    "        # 【参数选择】：\n",
    "        #   - rate=0.5: 丢弃率50%\n",
    "        #     * 为什么这么高：全连接层容易过拟合\n",
    "        #     * 为什么不用在LSTM：LSTM已有dropout机制\n",
    "        #     * 经验值：全连接层常用0.5，卷积层常用0.25\n",
    "        model.add(layers.Dropout(\n",
    "            rate=0.5,\n",
    "            name='dropout_regularization'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第5层：Output（输出层）\n",
    "        # ============================================\n",
    "        # 【是什么】：二分类输出层\n",
    "        # 【做什么】：\n",
    "        #   - 输出一个0-1之间的概率值\n",
    "        #   - >0.5: 正面情感\n",
    "        #   - <0.5: 负面情感\n",
    "        # 【为什么】：\n",
    "        #   - 二分类问题只需要一个输出\n",
    "        #   - sigmoid将任意实数映射到概率\n",
    "        # 【参数选择】：\n",
    "        #   - units=1: 单个输出神经元\n",
    "        #     * 为什么是1：二分类只需一个概率值\n",
    "        #     * 多分类：units=类别数，用softmax\n",
    "        #   - activation='sigmoid': Sigmoid激活函数\n",
    "        #     * 公式：σ(x) = 1 / (1 + e^(-x))\n",
    "        #     * 作用：将实数映射到(0, 1)\n",
    "        #     * 为什么用sigmoid：\n",
    "        #       1. 输出可解释为概率\n",
    "        #       2. 配合binary_crossentropy损失\n",
    "        #       3. 二分类的标准选择\n",
    "        model.add(layers.Dense(\n",
    "            units=1,\n",
    "            activation='sigmoid',\n",
    "            name='output_probability'\n",
    "        ))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def create_bilstm(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        创建双向LSTM模型（中级）\n",
    "\n",
    "        架构：Embedding → BiLSTM → Dense → Dropout → Output\n",
    "\n",
    "        优势：\n",
    "        - 同时看前后文，理解更全面\n",
    "        - 例如：\"不好看\" - 双向能同时看到\"不\"和\"看\"\n",
    "\n",
    "        适用场景：\n",
    "        - 需要理解完整上下文\n",
    "        - 数据量充足\n",
    "        - 对准确率要求较高\n",
    "\n",
    "        Args:\n",
    "            embedding_dim: 词嵌入维度\n",
    "\n",
    "        Returns:\n",
    "            Keras模型\n",
    "        \"\"\"\n",
    "        model = models.Sequential(name='bilstm')\n",
    "\n",
    "        # 词嵌入层（同simple_lstm）\n",
    "        model.add(layers.Embedding(\n",
    "            input_dim=self.max_words,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_len,\n",
    "            name='embedding_word_vectors'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 双向LSTM层\n",
    "        # ============================================\n",
    "        # 【是什么】：同时从前向后和从后向前处理序列的LSTM\n",
    "        # 【做什么】：\n",
    "        #   前向LSTM: \"这\" → \"部\" → \"电影\" → \"不\" → \"好\"\n",
    "        #   后向LSTM: \"好\" → \"不\" → \"电影\" → \"部\" → \"这\"\n",
    "        #   最终输出: 拼接前向和后向的结果 (64+64=128维)\n",
    "        # 【为什么】：\n",
    "        #   - 单向LSTM的局限：\n",
    "        #     * 处理\"不好\"时，只看到了\"不\"\n",
    "        #     * 可能误判为负面\n",
    "        #   - 双向LSTM的优势：\n",
    "        #     * 同时看到\"不\"和\"好\"\n",
    "        #     * 理解\"不好\"是一个整体\n",
    "        #     * 判断更准确\n",
    "        # 【参数选择】：\n",
    "        #   - units=64: 每个方向的LSTM单元数\n",
    "        #     * 为什么是64而不是128：\n",
    "        #       1. 双向会拼接，总输出是128维\n",
    "        #       2. 保持与simple_lstm相同的输出维度\n",
    "        #       3. 减少参数量，防止过拟合\n",
    "        #   - 其他参数同simple_lstm\n",
    "        model.add(layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                units=64,\n",
    "                dropout=0.2,\n",
    "                recurrent_dropout=0.2,\n",
    "                return_sequences=False\n",
    "            ),\n",
    "            name='bilstm_bidirectional_processing'\n",
    "        ))\n",
    "\n",
    "        # 全连接层、Dropout、输出层（同simple_lstm）\n",
    "        model.add(layers.Dense(64, activation='relu', name='dense_feature_combination'))\n",
    "        model.add(layers.Dropout(0.5, name='dropout_regularization'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid', name='output_probability'))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def create_stacked_lstm(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        创建堆叠LSTM模型（高级）\n",
    "\n",
    "        架构：Embedding → LSTM → LSTM → Dense → Dropout → Output\n",
    "\n",
    "        优势：\n",
    "        - 多层LSTM能学习更复杂的模式\n",
    "        - 第一层：学习低级特征（词组）\n",
    "        - 第二层：学习高级特征（句子结构）\n",
    "\n",
    "        适用场景：\n",
    "        - 复杂的文本理解任务\n",
    "        - 数据量大\n",
    "        - 需要捕获深层语义\n",
    "\n",
    "        Args:\n",
    "            embedding_dim: 词嵌入维度\n",
    "\n",
    "        Returns:\n",
    "            Keras模型\n",
    "        \"\"\"\n",
    "        model = models.Sequential(name='stacked_lstm')\n",
    "\n",
    "        # 词嵌入层\n",
    "        model.add(layers.Embedding(\n",
    "            input_dim=self.max_words,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_len,\n",
    "            name='embedding_word_vectors'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第一层LSTM：学习低级特征\n",
    "        # ============================================\n",
    "        # 【是什么】：第一层LSTM，返回完整序列\n",
    "        # 【做什么】：\n",
    "        #   - 处理每个词，输出每个时间步的隐状态\n",
    "        #   - 学习词组级别的特征\n",
    "        #   - 例如：\"非常好\"、\"不太好\"等词组模式\n",
    "        # 【为什么】：\n",
    "        #   - return_sequences=True: 返回所有时间步\n",
    "        #     * 为什么：需要传递给下一层LSTM\n",
    "        #     * 输出形状：(batch, 200, 128)\n",
    "        #   - units=128: 较大的容量\n",
    "        #     * 为什么：第一层需要捕获丰富的低级特征\n",
    "        model.add(layers.LSTM(\n",
    "            units=128,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            return_sequences=True,  # 关键：返回序列给下一层\n",
    "            name='lstm_layer1_low_level_features'\n",
    "        ))\n",
    "\n",
    "        # ============================================\n",
    "        # 第二层LSTM：学习高级特征\n",
    "        # ============================================\n",
    "        # 【是什么】：第二层LSTM，只返回最后输出\n",
    "        # 【做什么】：\n",
    "        #   - 基于第一层的输出，学习更抽象的特征\n",
    "        #   - 学习句子级别的语义\n",
    "        #   - 例如：整体情感倾向、转折关系等\n",
    "        # 【为什么】：\n",
    "        #   - 多层的优势：\n",
    "        #     * 第一层：词组特征（\"非常好\"）\n",
    "        #     * 第二层：句子特征（\"虽然...但是...\"）\n",
    "        #   - units=64: 较小的容量\n",
    "        #     * 为什么：高级特征更抽象，不需要太大容量\n",
    "        #     * 防止过拟合\n",
    "        #   - return_sequences=False: 只返回最后输出\n",
    "        #     * 为什么：用于分类，只需要整体表示\n",
    "        model.add(layers.LSTM(\n",
    "            units=64,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            return_sequences=False,\n",
    "            name='lstm_layer2_high_level_features'\n",
    "        ))\n",
    "\n",
    "        # 全连接层、Dropout、输出层\n",
    "        model.add(layers.Dense(64, activation='relu', name='dense_feature_combination'))\n",
    "        model.add(layers.Dropout(0.5, name='dropout_regularization'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid', name='output_probability'))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        根据model_type创建对应的模型\n",
    "\n",
    "        Returns:\n",
    "            编译好的Keras模型\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"创建模型: {self.model_type}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        if self.model_type == 'simple_lstm':\n",
    "            model = self.create_simple_lstm()\n",
    "        elif self.model_type == 'bilstm':\n",
    "            model = self.create_bilstm()\n",
    "        elif self.model_type == 'stacked_lstm':\n",
    "            model = self.create_stacked_lstm()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的模型类型: {self.model_type}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 编译模型\n",
    "        # ============================================\n",
    "        # 【优化器】：Adam\n",
    "        #   - 为什么用Adam：\n",
    "        #     1. 自适应学习率，训练稳定\n",
    "        #     2. 对超参数不敏感\n",
    "        #     3. 是目前最常用的优化器\n",
    "        #   - learning_rate=0.001: 默认学习率\n",
    "        #     * 太大(0.01): 训练不稳定，可能不收敛\n",
    "        #     * 太小(0.0001): 训练太慢\n",
    "        #     * 0.001: 经验最佳值\n",
    "        # 【损失函数】：binary_crossentropy\n",
    "        #   - 为什么用这个：\n",
    "        #     1. 二分类的标准损失函数\n",
    "        #     2. 配合sigmoid输出\n",
    "        #     3. 公式：-[y*log(p) + (1-y)*log(1-p)]\n",
    "        # 【评估指标】：accuracy\n",
    "        #   - 为什么用准确率：\n",
    "        #     1. 直观易懂\n",
    "        #     2. 数据平衡时效果好\n",
    "        #     3. 如果数据不平衡，应该用F1-score\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None,\n",
    "              epochs=10, batch_size=32, callbacks=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练数据（已经过预处理的序列）\n",
    "            y_train: 训练标签（0或1）\n",
    "            X_val: 验证数据\n",
    "            y_val: 验证标签\n",
    "            epochs: 训练轮数\n",
    "            batch_size: 批大小\n",
    "            callbacks: 回调函数列表\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始训练模型...\")\n",
    "        print(f\"训练样本数: {len(X_train)}\")\n",
    "        print(f\"序列长度: {self.max_len}\")\n",
    "        print(f\"词汇表大小: {self.max_words}\")\n",
    "\n",
    "        # 创建模型\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # 打印模型结构\n",
    "        print(f\"\\n模型结构:\")\n",
    "        self.model.summary()\n",
    "\n",
    "        # 准备验证数据\n",
    "        validation_data = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            validation_data = (X_val, y_val)\n",
    "            print(f\"验证样本数: {len(X_val)}\")\n",
    "\n",
    "        # 训练模型\n",
    "        print(f\"\\n开始训练 (epochs={epochs}, batch_size={batch_size})...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"\\n✓ 模型训练完成！\")\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测类别\n",
    "\n",
    "        Args:\n",
    "            X: 输入数据\n",
    "\n",
    "        Returns:\n",
    "            预测类别（0或1）\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"模型未训练，请先调用train()方法\")\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        return (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        预测概率\n",
    "\n",
    "        Args:\n",
    "            X: 输入数据\n",
    "\n",
    "        Returns:\n",
    "            预测概率（0-1之间）\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"模型未训练，请先调用train()方法\")\n",
    "\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        评估模型\n",
    "\n",
    "        Args:\n",
    "            X: 测试数据\n",
    "            y: 测试标签\n",
    "\n",
    "        Returns:\n",
    "            评估指标字典\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"模型未训练，请先调用train()方法\")\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(X, y, verbose=0)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "\n",
    "        Args:\n",
    "            filepath: 保存路径\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"模型未训练，无法保存\")\n",
    "\n",
    "        filepath = Path(filepath)\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.model.save(filepath)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "\n",
    "        Args:\n",
    "            filepath: 模型路径\n",
    "        \"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"模型文件不存在: {filepath}\")\n",
    "\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36deb336",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_callbacks(model_path, patience=5):\n",
    "    \"\"\"\n",
    "    获取训练回调函数\n",
    "\n",
    "    Args:\n",
    "        model_path: 模型保存路径\n",
    "        patience: 早停耐心值\n",
    "\n",
    "    Returns:\n",
    "        回调函数列表\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        # 早停：验证集loss不再下降时停止训练\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # 保存最佳模型\n",
    "        ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # 学习率调度：验证集loss不下降时降低学习率\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c028f31",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试模型创建\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LSTM情感分析模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试三种模型\n",
    "    for model_type in ['simple_lstm', 'bilstm', 'stacked_lstm']:\n",
    "        print(f\"\\n\\n{'='*60}\")\n",
    "        print(f\"测试模型: {model_type}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        analyzer = LSTMSentimentAnalyzer(model_type=model_type)\n",
    "        model = analyzer.create_model()\n",
    "\n",
    "        print(f\"\\n✓ {model_type} 模型创建成功！\")\n",
    "        print(f\"总参数量: {model.count_params():,}\")\n",
    "\n",
    "    print(\"\\n\\n✓ 所有模型测试完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
