{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bd3cb4",
   "metadata": {},
   "source": [
    "# NER数据处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c8ffa",
   "metadata": {},
   "source": [
    "本模块负责：\n",
    "1. 加载CoNLL-2003数据集\n",
    "2. 构建词汇表和标签映射\n",
    "3. 序列填充和编码\n",
    "4. 数据集划分\n",
    "\n",
    "使用CoNLL-2003数据集（公开标准NER数据集）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c159c58",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fec8f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1250c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NERDataProcessor:\n",
    "    \"\"\"\n",
    "    NER数据处理器\n",
    "\n",
    "    【是什么】：处理命名实体识别数据的工具类\n",
    "    【做什么】：\n",
    "        - 加载CoNLL格式数据\n",
    "        - 构建词汇表和标签映射\n",
    "        - 序列编码和填充\n",
    "    【为什么】：\n",
    "        - NER是序列标注任务\n",
    "        - 需要对齐词和标签\n",
    "        - 需要特殊的数据处理\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len=128, max_vocab_size=10000):\n",
    "        \"\"\"\n",
    "        初始化数据处理器\n",
    "\n",
    "        Args:\n",
    "            max_len: 最大序列长度\n",
    "            max_vocab_size: 最大词汇表大小\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "        # 特殊token\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.CLS_TOKEN = '<CLS>'\n",
    "        self.SEP_TOKEN = '<SEP>'\n",
    "\n",
    "        self.special_tokens = [\n",
    "            self.PAD_TOKEN,\n",
    "            self.UNK_TOKEN,\n",
    "            self.CLS_TOKEN,\n",
    "            self.SEP_TOKEN\n",
    "        ]\n",
    "\n",
    "        # 词汇表和标签映射\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.tag2idx = {}\n",
    "        self.idx2tag = {}\n",
    "\n",
    "        # CoNLL-2003标签体系\n",
    "        # 【是什么】：BIO标注格式\n",
    "        # 【B】：Begin（实体开始）\n",
    "        # 【I】：Inside（实体内部）\n",
    "        # 【O】：Outside（非实体）\n",
    "        # 【实体类型】：PER（人名）、ORG（组织）、LOC（地点）、MISC（其他）\n",
    "        self.default_tags = [\n",
    "            'O',           # 非实体\n",
    "            'B-PER',       # 人名-开始\n",
    "            'I-PER',       # 人名-内部\n",
    "            'B-ORG',       # 组织-开始\n",
    "            'I-ORG',       # 组织-内部\n",
    "            'B-LOC',       # 地点-开始\n",
    "            'I-LOC',       # 地点-内部\n",
    "            'B-MISC',      # 其他-开始\n",
    "            'I-MISC'       # 其他-内部\n",
    "        ]\n",
    "\n",
    "    def load_conll_data(self, file_path):\n",
    "        \"\"\"\n",
    "        加载CoNLL格式数据\n",
    "\n",
    "        【CoNLL格式】：\n",
    "        每行一个词和标签，用空格或制表符分隔\n",
    "        句子之间用空行分隔\n",
    "\n",
    "        示例：\n",
    "        EU    B-ORG\n",
    "        rejects    O\n",
    "        German    B-MISC\n",
    "        call    O\n",
    "\n",
    "        British    B-MISC\n",
    "        ...\n",
    "\n",
    "        Args:\n",
    "            file_path: 数据文件路径\n",
    "\n",
    "        Returns:\n",
    "            sentences: 句子列表\n",
    "            tags: 标签列表\n",
    "        \"\"\"\n",
    "        print(f\"\\n加载CoNLL数据: {file_path}\")\n",
    "\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        current_sentence = []\n",
    "        current_tags = []\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "\n",
    "                    # 空行表示句子结束\n",
    "                    if not line:\n",
    "                        if current_sentence:\n",
    "                            sentences.append(current_sentence)\n",
    "                            tags.append(current_tags)\n",
    "                            current_sentence = []\n",
    "                            current_tags = []\n",
    "                        continue\n",
    "\n",
    "                    # 跳过注释行\n",
    "                    if line.startswith('-DOCSTART-'):\n",
    "                        continue\n",
    "\n",
    "                    # 解析词和标签\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        word = parts[0]\n",
    "                        tag = parts[-1]  # 最后一列是NER标签\n",
    "\n",
    "                        current_sentence.append(word)\n",
    "                        current_tags.append(tag)\n",
    "\n",
    "                # 处理最后一个句子\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ✗ 文件不存在: {file_path}\")\n",
    "            return [], []\n",
    "\n",
    "        print(f\"  加载句子数: {len(sentences)}\")\n",
    "        if sentences:\n",
    "            print(f\"  平均句子长度: {np.mean([len(s) for s in sentences]):.1f}\")\n",
    "            print(f\"  最长句子: {max([len(s) for s in sentences])}\")\n",
    "\n",
    "        return sentences, tags\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"\n",
    "        构建词汇表\n",
    "\n",
    "        Args:\n",
    "            sentences: 句子列表\n",
    "        \"\"\"\n",
    "        print(\"\\n构建词汇表...\")\n",
    "\n",
    "        # 统计词频\n",
    "        word_freq = Counter()\n",
    "        for sentence in sentences:\n",
    "            word_freq.update(sentence)\n",
    "\n",
    "        print(f\"  总词数: {len(word_freq)}\")\n",
    "\n",
    "        # 选择最常见的词\n",
    "        most_common = word_freq.most_common(self.max_vocab_size - len(self.special_tokens))\n",
    "\n",
    "        # 构建映射\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "\n",
    "        for idx, (word, _) in enumerate(most_common, start=len(self.special_tokens)):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "        print(f\"  词汇表大小: {len(self.word2idx)}\")\n",
    "\n",
    "        # 计算覆盖率\n",
    "        total_words = sum(word_freq.values())\n",
    "        covered_words = sum(freq for word, freq in word_freq.items() if word in self.word2idx)\n",
    "        coverage = covered_words / total_words\n",
    "        print(f\"  覆盖率: {coverage:.2%}\")\n",
    "\n",
    "    def build_tag_mapping(self, tags_list):\n",
    "        \"\"\"\n",
    "        构建标签映射\n",
    "\n",
    "        Args:\n",
    "            tags_list: 标签列表\n",
    "        \"\"\"\n",
    "        print(\"\\n构建标签映射...\")\n",
    "\n",
    "        # 收集所有标签\n",
    "        all_tags = set()\n",
    "        for tags in tags_list:\n",
    "            all_tags.update(tags)\n",
    "\n",
    "        # 使用默认标签顺序（如果存在）\n",
    "        unique_tags = []\n",
    "        for tag in self.default_tags:\n",
    "            if tag in all_tags:\n",
    "                unique_tags.append(tag)\n",
    "\n",
    "        # 添加其他标签\n",
    "        for tag in sorted(all_tags):\n",
    "            if tag not in unique_tags:\n",
    "                unique_tags.append(tag)\n",
    "\n",
    "        # 构建映射\n",
    "        for idx, tag in enumerate(unique_tags):\n",
    "            self.tag2idx[tag] = idx\n",
    "            self.idx2tag[idx] = tag\n",
    "\n",
    "        print(f\"  标签数量: {len(self.tag2idx)}\")\n",
    "        print(f\"  标签列表: {list(self.tag2idx.keys())}\")\n",
    "\n",
    "    def encode_sentences(self, sentences, tags_list=None):\n",
    "        \"\"\"\n",
    "        编码句子和标签\n",
    "\n",
    "        Args:\n",
    "            sentences: 句子列表\n",
    "            tags_list: 标签列表（可选）\n",
    "\n",
    "        Returns:\n",
    "            encoded_sentences: 编码后的句子\n",
    "            encoded_tags: 编码后的标签（如果提供）\n",
    "        \"\"\"\n",
    "        print(\"\\n编码句子...\")\n",
    "\n",
    "        encoded_sentences = []\n",
    "        encoded_tags = [] if tags_list is not None else None\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # 编码词\n",
    "            encoded_sentence = [\n",
    "                self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n",
    "                for word in sentence\n",
    "            ]\n",
    "            encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "            # 编码标签\n",
    "            if tags_list is not None:\n",
    "                tags = tags_list[i]\n",
    "                encoded_tag = [\n",
    "                    self.tag2idx.get(tag, 0)  # 默认为'O'\n",
    "                    for tag in tags\n",
    "                ]\n",
    "                encoded_tags.append(encoded_tag)\n",
    "\n",
    "        print(f\"  编码句子数: {len(encoded_sentences)}\")\n",
    "\n",
    "        return encoded_sentences, encoded_tags\n",
    "\n",
    "    def pad_sequences(self, sequences, tags=None):\n",
    "        \"\"\"\n",
    "        填充序列\n",
    "\n",
    "        【重要】：标签也需要填充，且填充值通常是0（对应'O'标签）\n",
    "\n",
    "        Args:\n",
    "            sequences: 序列列表\n",
    "            tags: 标签列表（可选）\n",
    "\n",
    "        Returns:\n",
    "            padded_sequences: 填充后的序列\n",
    "            padded_tags: 填充后的标签\n",
    "            mask: 填充掩码\n",
    "        \"\"\"\n",
    "        print(\"\\n填充序列...\")\n",
    "\n",
    "        # 填充词序列\n",
    "        padded_sequences = pad_sequences(\n",
    "            sequences,\n",
    "            maxlen=self.max_len,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=self.word2idx[self.PAD_TOKEN]\n",
    "        )\n",
    "\n",
    "        # 创建掩码（1表示真实词，0表示填充）\n",
    "        mask = (padded_sequences != self.word2idx[self.PAD_TOKEN]).astype(np.float32)\n",
    "\n",
    "        # 填充标签\n",
    "        padded_tags = None\n",
    "        if tags is not None:\n",
    "            padded_tags = pad_sequences(\n",
    "                tags,\n",
    "                maxlen=self.max_len,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0  # 填充标签为'O'\n",
    "            )\n",
    "\n",
    "        print(f\"  填充后形状: {padded_sequences.shape}\")\n",
    "\n",
    "        return padded_sequences, padded_tags, mask\n",
    "\n",
    "    def decode_predictions(self, predictions, mask=None):\n",
    "        \"\"\"\n",
    "        解码预测结果\n",
    "\n",
    "        Args:\n",
    "            predictions: 预测的标签ID\n",
    "            mask: 填充掩码\n",
    "\n",
    "        Returns:\n",
    "            decoded_tags: 解码后的标签\n",
    "        \"\"\"\n",
    "        decoded_tags = []\n",
    "\n",
    "        for i, pred_seq in enumerate(predictions):\n",
    "            decoded_seq = []\n",
    "            for j, tag_id in enumerate(pred_seq):\n",
    "                # 跳过填充位置\n",
    "                if mask is not None and mask[i][j] == 0:\n",
    "                    continue\n",
    "\n",
    "                tag = self.idx2tag.get(tag_id, 'O')\n",
    "                decoded_seq.append(tag)\n",
    "\n",
    "            decoded_tags.append(decoded_seq)\n",
    "\n",
    "        return decoded_tags\n",
    "\n",
    "    def save_processor(self, filepath):\n",
    "        \"\"\"保存数据处理器\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'word2idx': self.word2idx,\n",
    "                'idx2word': self.idx2word,\n",
    "                'tag2idx': self.tag2idx,\n",
    "                'idx2tag': self.idx2tag,\n",
    "                'max_len': self.max_len\n",
    "            }, f)\n",
    "        print(f\"✓ 数据处理器已保存: {filepath}\")\n",
    "\n",
    "    def load_processor(self, filepath):\n",
    "        \"\"\"加载数据处理器\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.word2idx = data['word2idx']\n",
    "            self.idx2word = data['idx2word']\n",
    "            self.tag2idx = data['tag2idx']\n",
    "            self.idx2tag = data['idx2tag']\n",
    "            self.max_len = data['max_len']\n",
    "        print(f\"✓ 数据处理器已加载: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82435b7c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_ner_data(train_path, test_path=None, max_len=128, max_vocab_size=10000):\n",
    "    \"\"\"\n",
    "    准备NER数据\n",
    "\n",
    "    Args:\n",
    "        train_path: 训练数据路径\n",
    "        test_path: 测试数据路径\n",
    "        max_len: 最大序列长度\n",
    "        max_vocab_size: 最大词汇表大小\n",
    "\n",
    "    Returns:\n",
    "        训练集、验证集、测试集、处理器\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"NER数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建处理器\n",
    "    processor = NERDataProcessor(max_len=max_len, max_vocab_size=max_vocab_size)\n",
    "\n",
    "    # 加载训练数据\n",
    "    train_sentences, train_tags = processor.load_conll_data(train_path)\n",
    "\n",
    "    if not train_sentences:\n",
    "        raise FileNotFoundError(f\"无法加载训练数据: {train_path}\")\n",
    "\n",
    "    # 构建词汇表和标签映射\n",
    "    processor.build_vocab(train_sentences)\n",
    "    processor.build_tag_mapping(train_tags)\n",
    "\n",
    "    # 编码训练数据\n",
    "    train_encoded_sentences, train_encoded_tags = processor.encode_sentences(\n",
    "        train_sentences, train_tags\n",
    "    )\n",
    "\n",
    "    # 填充训练数据\n",
    "    X_train, y_train, mask_train = processor.pad_sequences(\n",
    "        train_encoded_sentences, train_encoded_tags\n",
    "    )\n",
    "\n",
    "    # 划分验证集（从训练集中分出10%）\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n",
    "        X_train, y_train, mask_train,\n",
    "        test_size=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n训练集: {X_train.shape}\")\n",
    "    print(f\"验证集: {X_val.shape}\")\n",
    "\n",
    "    # 加载测试数据（如果提供）\n",
    "    X_test, y_test, mask_test = None, None, None\n",
    "    if test_path:\n",
    "        test_sentences, test_tags = processor.load_conll_data(test_path)\n",
    "        if test_sentences:\n",
    "            test_encoded_sentences, test_encoded_tags = processor.encode_sentences(\n",
    "                test_sentences, test_tags\n",
    "            )\n",
    "            X_test, y_test, mask_test = processor.pad_sequences(\n",
    "                test_encoded_sentences, test_encoded_tags\n",
    "            )\n",
    "            print(f\"测试集: {X_test.shape}\")\n",
    "\n",
    "    print(\"\\n数据准备完成！\")\n",
    "\n",
    "    return (X_train, y_train, mask_train), \\\n",
    "           (X_val, y_val, mask_val), \\\n",
    "           (X_test, y_test, mask_test), \\\n",
    "           processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0dbe4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"测试数据处理\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"NER数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建模拟CoNLL数据\n",
    "    print(\"\\n创建模拟数据...\")\n",
    "    mock_data = \"\"\"EU B-ORG\n",
    "rejects O\n",
    "German B-MISC\n",
    "call O\n",
    "to O\n",
    "boycott O\n",
    "British B-MISC\n",
    "lamb O\n",
    ".  O\n",
    "\n",
    "Peter B-PER\n",
    "Blackburn I-PER\n",
    "\"\"\"\n",
    "\n",
    "    # 保存临时文件\n",
    "    temp_path = 'temp_conll_data.txt'\n",
    "    with open(temp_path, 'w') as f:\n",
    "        f.write(mock_data)\n",
    "\n",
    "    try:\n",
    "        # 测试数据处理\n",
    "        (X_train, y_train, mask_train), \\\n",
    "        (X_val, y_val, mask_val), \\\n",
    "        (X_test, y_test, mask_test), \\\n",
    "        processor = prepare_ner_data(temp_path, max_len=20)\n",
    "\n",
    "        print(\"\\n✓ 数据处理测试通过！\")\n",
    "\n",
    "        # 测试解码\n",
    "        print(\"\\n测试解码...\")\n",
    "        decoded = processor.decode_predictions(y_val[:1], mask_val[:1])\n",
    "        print(f\"  解码标签: {decoded[0][:10]}\")\n",
    "\n",
    "    finally:\n",
    "        # 清理临时文件\n",
    "        import os\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
