{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d142bee4",
   "metadata": {},
   "source": [
    "# Transformer NER模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375820f",
   "metadata": {},
   "source": [
    "本模块实现：\n",
    "1. 基础Transformer NER模型\n",
    "2. Transformer + CRF模型\n",
    "3. BiLSTM + CRF模型（对比）\n",
    "\n",
    "每个模型都有详细的注释说明设计思路和参数选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37d818",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ab857",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c5aa0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerNERModel:\n",
    "    \"\"\"\n",
    "    Transformer NER模型\n",
    "\n",
    "    【是什么】：基于Transformer的命名实体识别模型\n",
    "    【做什么】：序列标注，为每个词预测实体标签\n",
    "    【为什么】：\n",
    "        - Transformer捕获长距离依赖\n",
    "        - 自注意力机制理解上下文\n",
    "        - 适合序列标注任务\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_tags,\n",
    "                 max_len=128,\n",
    "                 model_type='transformer',\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        初始化NER模型\n",
    "\n",
    "        Args:\n",
    "            vocab_size: 词汇表大小\n",
    "            num_tags: 标签数量\n",
    "            max_len: 最大序列长度\n",
    "            model_type: 模型类型\n",
    "                - 'transformer': Transformer编码器\n",
    "                - 'transformer_crf': Transformer + CRF\n",
    "                - 'bilstm_crf': BiLSTM + CRF\n",
    "            **kwargs: 其他参数\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_tags = num_tags\n",
    "        self.max_len = max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # 配置\n",
    "        self.config = self._get_model_config(model_type)\n",
    "        self.config.update(kwargs)\n",
    "\n",
    "        # 模型\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _get_model_config(self, model_type):\n",
    "        \"\"\"获取模型配置\"\"\"\n",
    "        configs = {\n",
    "            'transformer': {\n",
    "                # ============================================\n",
    "                # Transformer配置\n",
    "                # ============================================\n",
    "                'd_model': 128,\n",
    "                # 【为什么=128】：\n",
    "                #   - NER任务相对简单\n",
    "                #   - 128维足够捕获实体特征\n",
    "                #   - 训练速度快\n",
    "\n",
    "                'num_heads': 4,\n",
    "                # 【为什么=4】：\n",
    "                #   - d_model=128，每个头32维\n",
    "                #   - 4个头学习不同的上下文模式\n",
    "\n",
    "                'num_layers': 2,\n",
    "                # 【为什么=2】：\n",
    "                #   - 2层足够学习实体边界\n",
    "                #   - 避免过拟合\n",
    "\n",
    "                'd_ff': 512,\n",
    "                # 【为什么=512】：\n",
    "                #   - d_model的4倍\n",
    "                #   - 标准配置\n",
    "\n",
    "                'dropout': 0.1,\n",
    "                # 【为什么=0.1】：\n",
    "                #   - 轻度正则化\n",
    "                #   - NER数据通常不大\n",
    "\n",
    "                'use_crf': False,\n",
    "            },\n",
    "\n",
    "            'transformer_crf': {\n",
    "                # ============================================\n",
    "                # Transformer + CRF配置\n",
    "                # ============================================\n",
    "                'd_model': 128,\n",
    "                'num_heads': 4,\n",
    "                'num_layers': 3,\n",
    "                # 【为什么=3】：\n",
    "                #   - CRF会增加模型能力\n",
    "                #   - 可以用更深的编码器\n",
    "\n",
    "                'd_ff': 512,\n",
    "                'dropout': 0.2,\n",
    "                # 【为什么=0.2】：\n",
    "                #   - 更深的模型需要更强正则化\n",
    "\n",
    "                'use_crf': True,\n",
    "                # 【为什么使用CRF】：\n",
    "                #   - 考虑标签之间的依赖关系\n",
    "                #   - B-PER后面不能直接跟I-ORG\n",
    "                #   - 提升标注一致性\n",
    "            },\n",
    "\n",
    "            'bilstm_crf': {\n",
    "                # ============================================\n",
    "                # BiLSTM + CRF配置（对比基线）\n",
    "                # ============================================\n",
    "                'lstm_units': 128,\n",
    "                # 【为什么=128】：\n",
    "                #   - 与Transformer的d_model对齐\n",
    "                #   - 公平对比\n",
    "\n",
    "                'num_layers': 2,\n",
    "                'dropout': 0.2,\n",
    "                'use_crf': True,\n",
    "                # 【为什么BiLSTM+CRF】：\n",
    "                #   - 经典NER架构\n",
    "                #   - 作为对比基线\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return configs.get(model_type, configs['transformer'])\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"构建模型\"\"\"\n",
    "        # ============================================\n",
    "        # 输入层\n",
    "        # ============================================\n",
    "        input_ids = layers.Input(shape=(self.max_len,), dtype=tf.int32, name='input_ids')\n",
    "        mask = layers.Input(shape=(self.max_len,), dtype=tf.float32, name='mask')\n",
    "\n",
    "        # ============================================\n",
    "        # 词嵌入层\n",
    "        # ============================================\n",
    "        # 【是什么】：将词ID转换为稠密向量\n",
    "        # 【为什么】：\n",
    "        #   - 词嵌入捕获词的语义\n",
    "        #   - 可训练的表示\n",
    "        if self.model_type.startswith('transformer'):\n",
    "            d_model = self.config['d_model']\n",
    "            embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                d_model,\n",
    "                mask_zero=True,\n",
    "                name='embedding'\n",
    "            )(input_ids)\n",
    "\n",
    "            # 位置编码\n",
    "            # 【为什么需要】：\n",
    "            #   - Transformer没有位置信息\n",
    "            #   - 需要显式添加位置编码\n",
    "            positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "            position_embedding = layers.Embedding(\n",
    "                self.max_len,\n",
    "                d_model,\n",
    "                name='position_embedding'\n",
    "            )(positions)\n",
    "\n",
    "            x = embedding + position_embedding\n",
    "            x = layers.Dropout(self.config['dropout'])(x)\n",
    "\n",
    "            # ============================================\n",
    "            # Transformer编码器层\n",
    "            # ============================================\n",
    "            for i in range(self.config['num_layers']):\n",
    "                # Multi-Head Attention\n",
    "                attention_output = layers.MultiHeadAttention(\n",
    "                    num_heads=self.config['num_heads'],\n",
    "                    key_dim=d_model // self.config['num_heads'],\n",
    "                    dropout=self.config['dropout'],\n",
    "                    name=f'attention_{i}'\n",
    "                )(x, x, attention_mask=mask[:, tf.newaxis, tf.newaxis, :])\n",
    "\n",
    "                attention_output = layers.Dropout(self.config['dropout'])(attention_output)\n",
    "                x = layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "                # Feed Forward\n",
    "                ffn_output = layers.Dense(\n",
    "                    self.config['d_ff'],\n",
    "                    activation='relu',\n",
    "                    name=f'ffn_1_{i}'\n",
    "                )(x)\n",
    "                ffn_output = layers.Dropout(self.config['dropout'])(ffn_output)\n",
    "                ffn_output = layers.Dense(d_model, name=f'ffn_2_{i}')(ffn_output)\n",
    "                ffn_output = layers.Dropout(self.config['dropout'])(ffn_output)\n",
    "\n",
    "                x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "        else:\n",
    "            # ============================================\n",
    "            # BiLSTM编码器\n",
    "            # ============================================\n",
    "            embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.config['lstm_units'],\n",
    "                mask_zero=True,\n",
    "                name='embedding'\n",
    "            )(input_ids)\n",
    "\n",
    "            x = embedding\n",
    "\n",
    "            for i in range(self.config['num_layers']):\n",
    "                x = layers.Bidirectional(\n",
    "                    layers.LSTM(\n",
    "                        self.config['lstm_units'] // 2,\n",
    "                        return_sequences=True,\n",
    "                        dropout=self.config['dropout'],\n",
    "                        name=f'lstm_{i}'\n",
    "                    ),\n",
    "                    name=f'bilstm_{i}'\n",
    "                )(x)\n",
    "\n",
    "        # ============================================\n",
    "        # 输出层\n",
    "        # ============================================\n",
    "        if self.config.get('use_crf', False):\n",
    "            # ============================================\n",
    "            # CRF层\n",
    "            # ============================================\n",
    "            # 【是什么】：条件随机场\n",
    "            # 【做什么】：考虑标签之间的转移概率\n",
    "            # 【为什么】：\n",
    "            #   - 标签有依赖关系（如B-PER后只能跟I-PER或O）\n",
    "            #   - 提升标注一致性\n",
    "            #   - 避免非法标签序列\n",
    "\n",
    "            # Dense层输出logits\n",
    "            logits = layers.Dense(self.num_tags, name='logits')(x)\n",
    "\n",
    "            # CRF层\n",
    "            crf = tfa.layers.CRF(self.num_tags, name='crf')\n",
    "            outputs = crf(logits)\n",
    "\n",
    "            # 创建模型\n",
    "            model = keras.Model(\n",
    "                inputs=[input_ids, mask],\n",
    "                outputs=outputs,\n",
    "                name=f'ner_{self.model_type}'\n",
    "            )\n",
    "\n",
    "            # 保存CRF层（用于解码）\n",
    "            self.crf_layer = crf\n",
    "\n",
    "        else:\n",
    "            # ============================================\n",
    "            # Softmax输出\n",
    "            # ============================================\n",
    "            # 【是什么】：每个位置独立预测标签\n",
    "            # 【为什么】：\n",
    "            #   - 简单直接\n",
    "            #   - 不考虑标签依赖\n",
    "\n",
    "            outputs = layers.Dense(\n",
    "                self.num_tags,\n",
    "                activation='softmax',\n",
    "                name='output'\n",
    "            )(x)\n",
    "\n",
    "            model = keras.Model(\n",
    "                inputs=[input_ids, mask],\n",
    "                outputs=outputs,\n",
    "                name=f'ner_{self.model_type}'\n",
    "            )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"编译模型\"\"\"\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        if self.config.get('use_crf', False):\n",
    "            # CRF模型使用特殊的损失函数\n",
    "            # 【是什么】：负对数似然损失\n",
    "            # 【为什么】：\n",
    "            #   - CRF需要考虑整个序列\n",
    "            #   - 不能用普通的交叉熵\n",
    "            self.model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=self.crf_layer.loss,\n",
    "                metrics=[self.crf_layer.accuracy]\n",
    "            )\n",
    "        else:\n",
    "            # 普通模型使用交叉熵\n",
    "            self.model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "    def train(self, X_train, y_train, mask_train,\n",
    "              X_val, y_val, mask_val,\n",
    "              epochs=50, batch_size=32, learning_rate=0.001,\n",
    "              callbacks=None, verbose=1):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练输入\n",
    "            y_train: 训练标签\n",
    "            mask_train: 训练掩码\n",
    "            X_val: 验证输入\n",
    "            y_val: 验证标签\n",
    "            mask_val: 验证掩码\n",
    "            epochs: 训练轮数\n",
    "            batch_size: 批大小\n",
    "            learning_rate: 学习率\n",
    "            callbacks: 回调函数\n",
    "            verbose: 详细程度\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        # 编译模型\n",
    "        self.compile_model(learning_rate=learning_rate)\n",
    "\n",
    "        # 训练\n",
    "        history = self.model.fit(\n",
    "            [X_train, mask_train],\n",
    "            y_train,\n",
    "            validation_data=([X_val, mask_val], y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, mask):\n",
    "        \"\"\"\n",
    "        预测\n",
    "\n",
    "        Args:\n",
    "            X: 输入序列\n",
    "            mask: 掩码\n",
    "\n",
    "        Returns:\n",
    "            预测的标签\n",
    "        \"\"\"\n",
    "        if self.config.get('use_crf', False):\n",
    "            # CRF模型需要特殊的解码\n",
    "            # 【是什么】：Viterbi解码\n",
    "            # 【为什么】：\n",
    "            #   - 找到最优标签序列\n",
    "            #   - 考虑转移概率\n",
    "            predictions = self.model.predict([X, mask])\n",
    "        else:\n",
    "            # 普通模型直接argmax\n",
    "            predictions = self.model.predict([X, mask])\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, X, y, mask):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        results = self.model.evaluate([X, mask], y, verbose=0)\n",
    "\n",
    "        metrics = {}\n",
    "        for name, value in zip(self.model.metrics_names, results):\n",
    "            metrics[name] = value\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def calculate_f1_score(self, y_true, y_pred, mask, idx2tag):\n",
    "        \"\"\"\n",
    "        计算F1分数\n",
    "\n",
    "        【重要】：NER任务通常使用实体级别的F1分数\n",
    "\n",
    "        Args:\n",
    "            y_true: 真实标签\n",
    "            y_pred: 预测标签\n",
    "            mask: 掩码\n",
    "            idx2tag: 标签映射\n",
    "\n",
    "        Returns:\n",
    "            F1分数字典\n",
    "        \"\"\"\n",
    "        # 展平并过滤填充位置\n",
    "        y_true_flat = []\n",
    "        y_pred_flat = []\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            for j in range(len(y_true[i])):\n",
    "                if mask[i][j] == 1:  # 非填充位置\n",
    "                    y_true_flat.append(y_true[i][j])\n",
    "                    y_pred_flat.append(y_pred[i][j])\n",
    "\n",
    "        # 转换为标签名称\n",
    "        y_true_tags = [idx2tag.get(idx, 'O') for idx in y_true_flat]\n",
    "        y_pred_tags = [idx2tag.get(idx, 'O') for idx in y_pred_flat]\n",
    "\n",
    "        # 计算F1分数\n",
    "        f1_micro = f1_score(y_true_tags, y_pred_tags, average='micro')\n",
    "        f1_macro = f1_score(y_true_tags, y_pred_tags, average='macro')\n",
    "\n",
    "        return {\n",
    "            'f1_micro': f1_micro,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        self.model.save(filepath)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        if self.config.get('use_crf', False):\n",
    "            # CRF模型需要自定义对象\n",
    "            self.model = keras.models.load_model(\n",
    "                filepath,\n",
    "                custom_objects={'CRF': tfa.layers.CRF}\n",
    "            )\n",
    "        else:\n",
    "            self.model = keras.models.load_model(filepath)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"打印模型摘要\"\"\"\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665bf6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer NER模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试参数\n",
    "    vocab_size = 1000\n",
    "    num_tags = 9\n",
    "    max_len = 50\n",
    "    batch_size = 4\n",
    "\n",
    "    # 创建随机数据\n",
    "    X_train = np.random.randint(0, vocab_size, (100, max_len))\n",
    "    y_train = np.random.randint(0, num_tags, (100, max_len))\n",
    "    mask_train = np.ones((100, max_len), dtype=np.float32)\n",
    "\n",
    "    X_val = np.random.randint(0, vocab_size, (20, max_len))\n",
    "    y_val = np.random.randint(0, num_tags, (20, max_len))\n",
    "    mask_val = np.ones((20, max_len), dtype=np.float32)\n",
    "\n",
    "    # 测试三种模型\n",
    "    for model_type in ['transformer', 'transformer_crf', 'bilstm_crf']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"测试 {model_type} 模型\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # 创建模型\n",
    "        ner_model = TransformerNERModel(\n",
    "            vocab_size=vocab_size,\n",
    "            num_tags=num_tags,\n",
    "            max_len=max_len,\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        # 打印摘要\n",
    "        print(f\"\\n模型结构:\")\n",
    "        ner_model.summary()\n",
    "\n",
    "        # 训练\n",
    "        print(f\"\\n训练模型...\")\n",
    "        history = ner_model.train(\n",
    "            X_train, y_train, mask_train,\n",
    "            X_val, y_val, mask_val,\n",
    "            epochs=2,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # 评估\n",
    "        metrics = ner_model.evaluate(X_val, y_val, mask_val)\n",
    "        print(f\"\\n验证集性能:\")\n",
    "        for name, value in metrics.items():\n",
    "            print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "        # 预测\n",
    "        predictions = ner_model.predict(X_val[:2], mask_val[:2])\n",
    "        print(f\"\\n预测形状: {predictions.shape}\")\n",
    "        print(f\"预测示例: {predictions[0][:10]}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
