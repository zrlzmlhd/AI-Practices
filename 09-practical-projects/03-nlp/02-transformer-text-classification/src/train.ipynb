{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9801e0a",
   "metadata": {},
   "source": [
    "# Transformer文本分类模型训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39073a0",
   "metadata": {},
   "source": [
    "使用方法:\n",
    "    python src/train.py --model_type simple --epochs 10\n",
    "    python src/train.py --model_type improved --epochs 20\n",
    "    python src/train.py --model_type advanced --epochs 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd535ff",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a9bf9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook路径自适应处理\n",
    "import pathlib as _nb_pathlib\n",
    "def _nb_resolve_file_path():\n",
    "    if '__file__' not in globals():\n",
    "        _cwd = _nb_pathlib.Path.cwd().resolve()\n",
    "        for _candidate in (_cwd, *_cwd.parents):\n",
    "            _potential = _candidate / '09-practical-projects/03_自然语言处理项目/02_Transformer文本分类_入门/src/train.py'\n",
    "            if _potential.exists():\n",
    "                globals()['__file__'] = str(_potential)\n",
    "                return\n",
    "        globals()['__file__'] = str((_cwd / '09-practical-projects/03_自然语言处理项目/02_Transformer文本分类_入门/src/train.py').resolve())\n",
    "_nb_resolve_file_path()\n",
    "del _nb_pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fea2a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data import prepare_data\n",
    "from src.model import TransformerTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beeb6f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"解析命令行参数\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='训练Transformer文本分类模型')\n",
    "\n",
    "    # 模型参数\n",
    "    parser.add_argument('--model_type', type=str, default='simple',\n",
    "                       choices=['simple', 'improved', 'advanced'],\n",
    "                       help='模型类型')\n",
    "\n",
    "    # 数据参数\n",
    "    parser.add_argument('--data_dir', type=str, default='data/aclImdb',\n",
    "                       help='数据目录')\n",
    "    parser.add_argument('--max_vocab_size', type=int, default=10000,\n",
    "                       help='最大词汇表大小')\n",
    "    parser.add_argument('--max_len', type=int, default=256,\n",
    "                       help='最大序列长度')\n",
    "    parser.add_argument('--max_samples', type=int, default=None,\n",
    "                       help='最大样本数（用于快速测试）')\n",
    "\n",
    "    # 训练参数\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                       help='训练轮数')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                       help='批大小')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4,\n",
    "                       help='学习率')\n",
    "    parser.add_argument('--early_stopping_patience', type=int, default=3,\n",
    "                       help='早停耐心值')\n",
    "\n",
    "    # 其他参数\n",
    "    parser.add_argument('--random_state', type=int, default=42,\n",
    "                       help='随机种子')\n",
    "    parser.add_argument('--model_dir', type=str, default='models',\n",
    "                       help='模型保存目录')\n",
    "    parser.add_argument('--result_dir', type=str, default='results',\n",
    "                       help='结果保存目录')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc379178",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_callbacks(model_path, patience=3):\n",
    "    \"\"\"\n",
    "    创建训练回调函数\n",
    "\n",
    "    Args:\n",
    "        model_path: 模型保存路径\n",
    "        patience: 早停耐心值\n",
    "\n",
    "    Returns:\n",
    "        回调函数列表\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "\n",
    "    # ============================================\n",
    "    # ModelCheckpoint: 保存最佳模型\n",
    "    # ============================================\n",
    "    # 【是什么】：在验证集上表现最好时保存模型\n",
    "    # 【为什么】：\n",
    "    #   - 防止过拟合\n",
    "    #   - 保存最佳性能的模型\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(checkpoint)\n",
    "\n",
    "    # ============================================\n",
    "    # EarlyStopping: 早停\n",
    "    # ============================================\n",
    "    # 【是什么】：验证集性能不再提升时停止训练\n",
    "    # 【为什么】：\n",
    "    #   - 防止过拟合\n",
    "    #   - 节省训练时间\n",
    "    #   - 自动找到最佳训练轮数\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "\n",
    "    # ============================================\n",
    "    # ReduceLROnPlateau: 学习率衰减\n",
    "    # ============================================\n",
    "    # 【是什么】：验证集性能停滞时降低学习率\n",
    "    # 【为什么】：\n",
    "    #   - 帮助模型跳出局部最优\n",
    "    #   - 在训练后期进行精细调整\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934575ed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 解析参数\n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer文本分类 - 模型训练\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n配置:\")\n",
    "    for arg, value in vars(args).items():\n",
    "        print(f\"  {arg}: {value}\")\n",
    "\n",
    "    # 设置随机种子\n",
    "    # 【为什么】：保证结果可复现\n",
    "    np.random.seed(args.random_state)\n",
    "    tf.random.set_seed(args.random_state)\n",
    "\n",
    "    # 创建保存目录\n",
    "    project_dir = Path(__file__).parent.parent\n",
    "    model_dir = project_dir / args.model_dir\n",
    "    result_dir = project_dir / args.result_dir\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤1: 准备数据\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤1: 准备数据\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), vocab = prepare_data(\n",
    "            data_dir=args.data_dir,\n",
    "            max_vocab_size=args.max_vocab_size,\n",
    "            max_len=args.max_len,\n",
    "            random_state=args.random_state,\n",
    "            max_samples=args.max_samples\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n✗ 数据文件不存在: {e}\")\n",
    "        print(\"\\n请先下载数据:\")\n",
    "        print(\"  cd data\")\n",
    "        print(\"  python download_data.py\")\n",
    "        return\n",
    "\n",
    "    # 保存词汇表\n",
    "    vocab_path = model_dir / f'{args.model_type}_vocab.pkl'\n",
    "    vocab.save(vocab_path)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤2: 创建模型\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤2: 创建模型\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    classifier = TransformerTextClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        max_len=args.max_len,\n",
    "        num_classes=2,\n",
    "        model_type=args.model_type\n",
    "    )\n",
    "\n",
    "    # 打印模型摘要\n",
    "    print(f\"\\n模型结构:\")\n",
    "    classifier.summary()\n",
    "\n",
    "    # 计算参数量\n",
    "    total_params = classifier.model.count_params()\n",
    "    print(f\"\\n总参数量: {total_params:,}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤3: 训练模型\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤3: 训练模型\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 创建回调函数\n",
    "    model_path = model_dir / f'{args.model_type}_model.h5'\n",
    "    callbacks = create_callbacks(\n",
    "        model_path=model_path,\n",
    "        patience=args.early_stopping_patience\n",
    "    )\n",
    "\n",
    "    # 训练\n",
    "    print(f\"\\n开始训练...\")\n",
    "    history = classifier.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        learning_rate=args.learning_rate,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤4: 评估模型\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤4: 评估模型\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 训练集评估\n",
    "    train_metrics = classifier.evaluate(X_train, y_train)\n",
    "    print(f\"\\n训练集性能:\")\n",
    "    for name, value in train_metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "    # 验证集评估\n",
    "    val_metrics = classifier.evaluate(X_val, y_val)\n",
    "    print(f\"\\n验证集性能:\")\n",
    "    for name, value in val_metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "    # 测试集评估\n",
    "    test_metrics = classifier.evaluate(X_test, y_test)\n",
    "    print(f\"\\n测试集性能:\")\n",
    "    for name, value in test_metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤5: 保存结果\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤5: 保存结果\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 保存训练历史\n",
    "    history_path = result_dir / f'{args.model_type}_history.npz'\n",
    "    np.savez(\n",
    "        history_path,\n",
    "        **history.history\n",
    "    )\n",
    "    print(f\"✓ 训练历史已保存: {history_path}\")\n",
    "\n",
    "    # 保存评估结果\n",
    "    results = {\n",
    "        'model_type': args.model_type,\n",
    "        'total_params': total_params,\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_accuracy': train_metrics['accuracy'],\n",
    "        'train_auc': train_metrics.get('auc', 0),\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_accuracy': val_metrics['accuracy'],\n",
    "        'val_auc': val_metrics.get('auc', 0),\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_auc': test_metrics.get('auc', 0),\n",
    "    }\n",
    "\n",
    "    results_path = result_dir / f'{args.model_type}_results.txt'\n",
    "    with open(results_path, 'w') as f:\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"✓ 评估结果已保存: {results_path}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤6: 示例预测\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"步骤6: 示例预测\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 从测试集中随机选择几个样本\n",
    "    num_examples = 5\n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "\n",
    "    print(f\"\\n随机选择 {num_examples} 个测试样本:\")\n",
    "    for i, idx in enumerate(indices):\n",
    "        # 预测\n",
    "        x = X_test[idx:idx+1]\n",
    "        y_true = y_test[idx]\n",
    "        y_pred = classifier.predict(x)[0]\n",
    "        y_proba = classifier.predict_proba(x)[0][0]\n",
    "\n",
    "        # 解码文本\n",
    "        text = vocab.decode(X_test[idx])\n",
    "        # 去除padding\n",
    "        text = text.replace(vocab.PAD_TOKEN, '').strip()\n",
    "        # 截断显示\n",
    "        if len(text) > 100:\n",
    "            text = text[:100] + '...'\n",
    "\n",
    "        print(f\"\\n样本 {i+1}:\")\n",
    "        print(f\"  文本: {text}\")\n",
    "        print(f\"  真实标签: {'正面' if y_true == 1 else '负面'}\")\n",
    "        print(f\"  预测标签: {'正面' if y_pred == 1 else '负面'}\")\n",
    "        print(f\"  预测概率: {y_proba:.4f}\")\n",
    "        print(f\"  预测{'正确' if y_pred == y_true else '错误'}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 总结\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"训练完成！\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n模型保存路径: {model_path}\")\n",
    "    print(f\"词汇表保存路径: {vocab_path}\")\n",
    "    print(f\"\\n测试集性能:\")\n",
    "    print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {test_metrics.get('auc', 0):.4f}\")\n",
    "\n",
    "    # 给出建议\n",
    "    print(f\"\\n下一步:\")\n",
    "    print(f\"  1. 查看训练历史: {history_path}\")\n",
    "    print(f\"  2. 评估模型: python src/evaluate.py --model_path {model_path} --vocab_path {vocab_path}\")\n",
    "    print(f\"  3. 尝试其他模型类型:\")\n",
    "    print(f\"     python src/train.py --model_type improved\")\n",
    "    print(f\"     python src/train.py --model_type advanced\")\n",
    "\n",
    "    # 性能分析\n",
    "    print(f\"\\n性能分析:\")\n",
    "    if test_metrics['accuracy'] < 0.7:\n",
    "        print(\"  ⚠ 准确率较低，建议:\")\n",
    "        print(\"    - 增加训练轮数 (--epochs)\")\n",
    "        print(\"    - 使用更复杂的模型 (--model_type improved/advanced)\")\n",
    "        print(\"    - 增加词汇表大小 (--max_vocab_size)\")\n",
    "    elif test_metrics['accuracy'] < 0.85:\n",
    "        print(\"  ✓ 准确率中等，可以尝试:\")\n",
    "        print(\"    - 使用更复杂的模型\")\n",
    "        print(\"    - 调整学习率\")\n",
    "        print(\"    - 增加训练数据\")\n",
    "    else:\n",
    "        print(\"  ✓✓ 准确率优秀！\")\n",
    "\n",
    "    # 过拟合检查\n",
    "    if train_metrics['accuracy'] - test_metrics['accuracy'] > 0.1:\n",
    "        print(\"\\n  ⚠ 检测到过拟合，建议:\")\n",
    "        print(\"    - 增加Dropout\")\n",
    "        print(\"    - 减少模型复杂度\")\n",
    "        print(\"    - 增加训练数据\")\n",
    "        print(\"    - 使用数据增强\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504c18a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
