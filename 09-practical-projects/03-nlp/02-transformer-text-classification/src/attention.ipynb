{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27b3684",
   "metadata": {},
   "source": [
    "# Transformer注意力机制实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d02b81",
   "metadata": {},
   "source": [
    "本模块包含Transformer的核心组件：\n",
    "1. Scaled Dot-Product Attention（缩放点积注意力）\n",
    "2. Multi-Head Attention（多头注意力）\n",
    "3. Positional Encoding（位置编码）\n",
    "\n",
    "每个组件都有详细的注释说明原理和实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab544d1c",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204307d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5321ad3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    缩放点积注意力（Scaled Dot-Product Attention）\n",
    "\n",
    "    这是Transformer的核心机制，用于计算序列中每个位置对其他位置的注意力权重。\n",
    "\n",
    "    公式：Attention(Q, K, V) = softmax(Q·K^T / sqrt(d_k)) · V\n",
    "\n",
    "    参数说明：\n",
    "    - Q (Query): 查询向量，表示\"我要找什么\"\n",
    "    - K (Key): 键向量，表示\"我是什么\"\n",
    "    - V (Value): 值向量，表示\"我的内容是什么\"\n",
    "    - d_k: Key的维度，用于缩放\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        计算缩放点积注意力\n",
    "\n",
    "        Args:\n",
    "            query: Query矩阵，形状 (batch, seq_len_q, d_k)\n",
    "            key: Key矩阵，形状 (batch, seq_len_k, d_k)\n",
    "            value: Value矩阵，形状 (batch, seq_len_v, d_v)\n",
    "            mask: 掩码，用于遮蔽某些位置（如padding）\n",
    "\n",
    "        Returns:\n",
    "            output: 注意力输出，形状 (batch, seq_len_q, d_v)\n",
    "            attention_weights: 注意力权重，形状 (batch, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 计算注意力分数 (Q · K^T)\n",
    "        # ============================================\n",
    "        # 【是什么】：Query和Key的点积\n",
    "        # 【做什么】：计算Query和每个Key的相似度\n",
    "        # 【为什么】：相似度高的Key应该获得更多关注\n",
    "        #\n",
    "        # 例子：句子 \"我 爱 你\"\n",
    "        # Query=\"爱\" 与所有Key的相似度：\n",
    "        #   \"我\": 0.3 (中等相关)\n",
    "        #   \"爱\": 1.0 (自己最相关)\n",
    "        #   \"你\": 0.8 (高度相关，爱的对象)\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        # 形状: (batch, seq_len_q, seq_len_k)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 缩放 (除以 sqrt(d_k))\n",
    "        # ============================================\n",
    "        # 【是什么】：将分数除以sqrt(d_k)\n",
    "        # 【做什么】：缩放点积值到合理范围\n",
    "        # 【为什么】：\n",
    "        #   - 问题：d_k很大时，点积值会很大\n",
    "        #   - 后果：softmax后梯度很小（梯度消失）\n",
    "        #   - 解决：除以sqrt(d_k)进行缩放\n",
    "        #   - 效果：保持梯度在合理范围\n",
    "        #\n",
    "        # 数学原理：\n",
    "        #   假设Q和K的元素是独立的，均值0，方差1\n",
    "        #   则Q·K的方差是d_k\n",
    "        #   除以sqrt(d_k)后，方差变为1\n",
    "        d_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 应用掩码（可选）\n",
    "        # ============================================\n",
    "        # 【是什么】：将某些位置的分数设为-inf\n",
    "        # 【做什么】：遮蔽padding位置或未来位置\n",
    "        # 【为什么】：\n",
    "        #   - Padding位置：不应该被关注（没有实际内容）\n",
    "        #   - 未来位置：解码器不能看到未来（防止作弊）\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: Softmax归一化\n",
    "        # ============================================\n",
    "        # 【是什么】：将分数转换为概率分布\n",
    "        # 【做什么】：归一化，使所有权重和为1\n",
    "        # 【为什么】：\n",
    "        #   - 概率解释：每个位置的重要性\n",
    "        #   - 和为1：便于加权求和\n",
    "        #\n",
    "        # 例子：\n",
    "        #   分数: [2.0, 5.0, 3.0]\n",
    "        #   Softmax: [0.09, 0.67, 0.24]  # 和为1\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        # 形状: (batch, seq_len_q, seq_len_k)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤5: 加权求和 (Attention_Weights · V)\n",
    "        # ============================================\n",
    "        # 【是什么】：用注意力权重对Value加权求和\n",
    "        # 【做什么】：根据重要性组合信息\n",
    "        # 【为什么】：\n",
    "        #   - 重要的位置权重高，贡献大\n",
    "        #   - 不重要的位置权重低，贡献小\n",
    "        #   - 最终输出是所有位置的加权组合\n",
    "        #\n",
    "        # 例子：\n",
    "        #   权重: [0.1, 0.6, 0.3]\n",
    "        #   Value: [[1,2], [3,4], [5,6]]\n",
    "        #   输出: 0.1*[1,2] + 0.6*[3,4] + 0.3*[5,6]\n",
    "        #       = [3.4, 4.4]  # 重点关注第2个位置\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        # 形状: (batch, seq_len_q, d_v)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a070fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    多头注意力（Multi-Head Attention）\n",
    "\n",
    "    核心思想：使用多个注意力头，每个头学习不同的关注模式\n",
    "\n",
    "    为什么需要多头？\n",
    "    - 单头：只能学习一种关系（如语义相似）\n",
    "    - 多头：学习多种关系（语义、语法、位置等）\n",
    "\n",
    "    例如8个头可能学习：\n",
    "    - Head 1: 语义相似（\"好\"关注\"棒\"）\n",
    "    - Head 2: 语法关系（动词关注主语）\n",
    "    - Head 3: 位置关系（关注相邻词）\n",
    "    - Head 4: 情感词（\"好\"关注\"很\"）\n",
    "    - ... 等8个不同的关注模式\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化多头注意力\n",
    "\n",
    "        Args:\n",
    "            d_model: 模型维度（如512）\n",
    "            num_heads: 注意力头数（如8）\n",
    "\n",
    "        要求：d_model必须能被num_heads整除\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "        # ============================================\n",
    "        # 参数验证和初始化\n",
    "        # ============================================\n",
    "\n",
    "        # 【验证】：d_model必须能被num_heads整除\n",
    "        # 【为什么】：需要将d_model平均分配给每个头\n",
    "        # 例如：d_model=512, num_heads=8\n",
    "        #      每个头的维度 d_k = 512/8 = 64\n",
    "        assert d_model % num_heads == 0, \\\n",
    "            f\"d_model ({d_model}) 必须能被 num_heads ({num_heads}) 整除\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 每个头的维度\n",
    "        # 【是什么】：d_k = d_model / num_heads\n",
    "        # 【为什么】：将总维度平均分配给每个头\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # ============================================\n",
    "        # 权重矩阵\n",
    "        # ============================================\n",
    "\n",
    "        # 【是什么】：线性变换矩阵\n",
    "        # 【做什么】：将输入投影到Q、K、V空间\n",
    "        # 【为什么】：\n",
    "        #   - 学习如何生成Query、Key、Value\n",
    "        #   - 每个头有独立的投影空间\n",
    "        #   - 增加模型的表达能力\n",
    "\n",
    "        # WQ: Query权重矩阵\n",
    "        self.wq = layers.Dense(d_model, name='query_projection')\n",
    "\n",
    "        # WK: Key权重矩阵\n",
    "        self.wk = layers.Dense(d_model, name='key_projection')\n",
    "\n",
    "        # WV: Value权重矩阵\n",
    "        self.wv = layers.Dense(d_model, name='value_projection')\n",
    "\n",
    "        # WO: 输出权重矩阵\n",
    "        # 【作用】：将多个头的输出合并\n",
    "        self.dense = layers.Dense(d_model, name='output_projection')\n",
    "\n",
    "        # 注意力层\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        将输入分割成多个头\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状 (batch, seq_len, d_model)\n",
    "            batch_size: 批大小\n",
    "\n",
    "        Returns:\n",
    "            分割后的张量，形状 (batch, num_heads, seq_len, depth)\n",
    "\n",
    "        例子：\n",
    "            输入: (32, 100, 512)  # 32个样本，100个词，512维\n",
    "            输出: (32, 8, 100, 64)  # 32个样本，8个头，100个词，每头64维\n",
    "        \"\"\"\n",
    "        # 重塑为 (batch, seq_len, num_heads, depth)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        # 转置为 (batch, num_heads, seq_len, depth)\n",
    "        # 【为什么转置】：方便并行计算每个头的注意力\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        计算多头注意力\n",
    "\n",
    "        Args:\n",
    "            query: Query输入\n",
    "            key: Key输入\n",
    "            value: Value输入\n",
    "            mask: 掩码\n",
    "\n",
    "        Returns:\n",
    "            output: 多头注意力输出\n",
    "            attention_weights: 注意力权重\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 线性投影 (生成Q、K、V)\n",
    "        # ============================================\n",
    "        # 【做什么】：通过权重矩阵生成Q、K、V\n",
    "        # 【为什么】：\n",
    "        #   - 学习如何从输入生成查询、键、值\n",
    "        #   - 每个头有不同的投影，学习不同的模式\n",
    "        query = self.wq(query)  # (batch, seq_len, d_model)\n",
    "        key = self.wk(key)      # (batch, seq_len, d_model)\n",
    "        value = self.wv(value)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 分割成多个头\n",
    "        # ============================================\n",
    "        # 【做什么】：将d_model维度分成num_heads个头\n",
    "        # 【为什么】：让每个头独立学习不同的关注模式\n",
    "        #\n",
    "        # 例子：d_model=512, num_heads=8\n",
    "        #   输入: (batch, seq_len, 512)\n",
    "        #   分割: (batch, 8, seq_len, 64)\n",
    "        #   每个头处理64维\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 计算缩放点积注意力\n",
    "        # ============================================\n",
    "        # 【做什么】：每个头独立计算注意力\n",
    "        # 【为什么】：并行计算，每个头学习不同的模式\n",
    "        scaled_attention, attention_weights = self.attention(\n",
    "            query, key, value, mask\n",
    "        )\n",
    "        # scaled_attention: (batch, num_heads, seq_len, depth)\n",
    "        # attention_weights: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: 转置回原来的形状\n",
    "        # ============================================\n",
    "        # 【做什么】：从 (batch, num_heads, seq_len, depth)\n",
    "        #           转为 (batch, seq_len, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤5: 拼接所有头\n",
    "        # ============================================\n",
    "        # 【做什么】：将多个头的输出拼接成一个向量\n",
    "        # 【为什么】：\n",
    "        #   - 合并所有头学到的信息\n",
    "        #   - 恢复到原始维度d_model\n",
    "        #\n",
    "        # 例子：8个头，每个64维\n",
    "        #   拼接: [head1(64), head2(64), ..., head8(64)]\n",
    "        #   结果: 512维向量\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (batch_size, -1, self.d_model)\n",
    "        )\n",
    "        # 形状: (batch, seq_len, d_model)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤6: 最终线性变换\n",
    "        # ============================================\n",
    "        # 【做什么】：通过WO矩阵进行最终变换\n",
    "        # 【为什么】：\n",
    "        #   - 学习如何组合多个头的信息\n",
    "        #   - 增加模型的表达能力\n",
    "        #   - 类似于全连接层的作用\n",
    "        output = self.dense(concat_attention)\n",
    "        # 形状: (batch, seq_len, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fe172",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"\n",
    "    位置编码（Positional Encoding）\n",
    "\n",
    "    为什么需要位置编码？\n",
    "    - 问题：Self-Attention是无序的\n",
    "      \"我爱你\" 和 \"你爱我\" 的注意力计算结果相同\n",
    "    - 解决：添加位置信息\n",
    "      让模型知道每个词在句子中的位置\n",
    "\n",
    "    位置编码公式：\n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "    其中：\n",
    "    - pos: 词的位置 (0, 1, 2, ...)\n",
    "    - i: 维度索引 (0, 1, 2, ..., d_model/2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, d_model, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化位置编码\n",
    "\n",
    "        Args:\n",
    "            max_len: 最大序列长度\n",
    "            d_model: 模型维度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 预计算位置编码\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        \"\"\"\n",
    "        计算位置编码的角度\n",
    "\n",
    "        Args:\n",
    "            pos: 位置索引\n",
    "            i: 维度索引\n",
    "            d_model: 模型维度\n",
    "\n",
    "        Returns:\n",
    "            角度值\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 角度计算公式\n",
    "        # ============================================\n",
    "        # 【公式】：pos / 10000^(2i/d_model)\n",
    "        # 【为什么用这个公式】：\n",
    "        #   - 不同位置有不同的编码\n",
    "        #   - 不同维度有不同的频率\n",
    "        #   - 低维度：高频率（快速变化）\n",
    "        #   - 高维度：低频率（缓慢变化）\n",
    "        #\n",
    "        # 效果：\n",
    "        #   - 相邻位置的编码相似但不同\n",
    "        #   - 可以通过三角函数计算相对位置\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"\n",
    "        生成位置编码矩阵\n",
    "\n",
    "        Args:\n",
    "            max_len: 最大序列长度\n",
    "            d_model: 模型维度\n",
    "\n",
    "        Returns:\n",
    "            位置编码矩阵，形状 (1, max_len, d_model)\n",
    "        \"\"\"\n",
    "        # 生成位置和维度的索引\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(max_len)[:, np.newaxis],  # 位置: (max_len, 1)\n",
    "            np.arange(d_model)[np.newaxis, :],  # 维度: (1, d_model)\n",
    "            d_model\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 应用sin和cos函数\n",
    "        # ============================================\n",
    "        # 【为什么用sin/cos】：\n",
    "        #   1. 值域在[-1, 1]，不会太大\n",
    "        #   2. 周期性函数，相似位置有相似编码\n",
    "        #   3. 可以通过三角恒等式计算相对位置\n",
    "        #      PE(pos+k) 可以表示为 PE(pos) 的线性组合\n",
    "        #   4. 可以处理训练时未见过的长度\n",
    "\n",
    "        # 偶数维度用sin\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 奇数维度用cos\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        # 形状: (1, max_len, d_model)\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        添加位置编码\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状 (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            添加位置编码后的张量\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # ============================================\n",
    "        # 添加位置编码\n",
    "        # ============================================\n",
    "        # 【做什么】：将位置编码加到词嵌入上\n",
    "        # 【为什么是相加而不是拼接】：\n",
    "        #   - 相加：保持维度不变，位置信息融入词向量\n",
    "        #   - 拼接：维度翻倍，计算量大\n",
    "        #   - 相加效果更好（实验证明）\n",
    "        #\n",
    "        # 例子：\n",
    "        #   词嵌入: [0.2, 0.5, -0.3, ...]\n",
    "        #   位置编码: [0.0, 0.1, 0.05, ...]\n",
    "        #   结果: [0.2, 0.6, -0.25, ...]\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144db30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    创建padding掩码\n",
    "\n",
    "    Args:\n",
    "        seq: 输入序列，形状 (batch, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        掩码，形状 (batch, 1, 1, seq_len)\n",
    "\n",
    "    作用：遮蔽padding位置（值为0的位置）\n",
    "    \"\"\"\n",
    "    # 找出padding位置（值为0）\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # 添加额外维度以便广播\n",
    "    # (batch, seq_len) -> (batch, 1, 1, seq_len)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4cc73",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    创建前瞻掩码（用于解码器）\n",
    "\n",
    "    Args:\n",
    "        size: 序列长度\n",
    "\n",
    "    Returns:\n",
    "        掩码，形状 (size, size)\n",
    "\n",
    "    作用：防止解码器看到未来的词\n",
    "    \"\"\"\n",
    "    # 创建下三角矩阵\n",
    "    # 例如 size=4:\n",
    "    # [[0, 1, 1, 1],\n",
    "    #  [0, 0, 1, 1],\n",
    "    #  [0, 0, 0, 1],\n",
    "    #  [0, 0, 0, 0]]\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482c39e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试注意力机制\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer注意力机制测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试参数\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "\n",
    "    # 创建随机输入\n",
    "    x = tf.random.normal((batch_size, seq_len, d_model))\n",
    "\n",
    "    # 测试多头注意力\n",
    "    print(\"\\n测试多头注意力...\")\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "    print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "\n",
    "    # 测试位置编码\n",
    "    print(\"\\n测试位置编码...\")\n",
    "    pos_encoding = PositionalEncoding(max_len=100, d_model=d_model)\n",
    "    x_with_pos = pos_encoding(x)\n",
    "\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"添加位置编码后: {x_with_pos.shape}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
