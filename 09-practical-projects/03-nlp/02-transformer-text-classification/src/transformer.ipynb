{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf77926",
   "metadata": {},
   "source": [
    "# Transformer编码器实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106f47f",
   "metadata": {},
   "source": [
    "本模块实现完整的Transformer编码器，包括：\n",
    "1. Feed Forward Network（前馈神经网络）\n",
    "2. Encoder Layer（编码器层）\n",
    "3. Transformer Encoder（完整编码器）\n",
    "\n",
    "每个组件都有详细的注释说明原理和实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca128ab2",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973240f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from attention import MultiHeadAttention, PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf93f21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForwardNetwork(layers.Layer):\n",
    "    \"\"\"\n",
    "    前馈神经网络（Feed Forward Network）\n",
    "\n",
    "    【是什么】：两层全连接网络\n",
    "    【做什么】：对每个位置独立进行非线性变换\n",
    "    【为什么】：\n",
    "        - 增加模型的非线性表达能力\n",
    "        - 每个位置独立处理（Position-wise）\n",
    "        - 类似于1x1卷积的作用\n",
    "\n",
    "    结构：\n",
    "        Linear(d_model -> d_ff) -> ReLU -> Dropout -> Linear(d_ff -> d_model)\n",
    "\n",
    "    参数说明：\n",
    "        - d_model: 模型维度（如512）\n",
    "        - d_ff: 中间层维度（通常是d_model的4倍，如2048）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化前馈网络\n",
    "\n",
    "        Args:\n",
    "            d_model: 模型维度\n",
    "            d_ff: 中间层维度\n",
    "            dropout_rate: Dropout比率\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__(**kwargs)\n",
    "\n",
    "        # ============================================\n",
    "        # 第一层：扩展维度\n",
    "        # ============================================\n",
    "        # 【是什么】：全连接层，d_model -> d_ff\n",
    "        # 【做什么】：将维度扩展到更高维空间\n",
    "        # 【为什么】：\n",
    "        #   - 更高维度提供更强的表达能力\n",
    "        #   - 类似于\"瓶颈\"结构的反向\n",
    "        #   - 通常d_ff = 4 * d_model\n",
    "        self.dense1 = layers.Dense(d_ff, activation='relu', name='ffn_layer1')\n",
    "\n",
    "        # ============================================\n",
    "        # Dropout层\n",
    "        # ============================================\n",
    "        # 【是什么】：随机丢弃神经元\n",
    "        # 【做什么】：防止过拟合\n",
    "        # 【为什么】：\n",
    "        #   - 增加模型泛化能力\n",
    "        #   - 类似于集成学习的效果\n",
    "        self.dropout = layers.Dropout(dropout_rate, name='ffn_dropout')\n",
    "\n",
    "        # ============================================\n",
    "        # 第二层：恢复维度\n",
    "        # ============================================\n",
    "        # 【是什么】：全连接层，d_ff -> d_model\n",
    "        # 【做什么】：将维度恢复到原始大小\n",
    "        # 【为什么】：\n",
    "        #   - 保持输入输出维度一致\n",
    "        #   - 便于残差连接\n",
    "        self.dense2 = layers.Dense(d_model, name='ffn_layer2')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状 (batch, seq_len, d_model)\n",
    "            training: 是否训练模式\n",
    "\n",
    "        Returns:\n",
    "            输出张量，形状 (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 第一层：扩展 + ReLU\n",
    "        x = self.dense1(x)  # (batch, seq_len, d_ff)\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # 第二层：恢复维度\n",
    "        x = self.dense2(x)  # (batch, seq_len, d_model)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1509880",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer编码器层（Encoder Layer）\n",
    "\n",
    "    【是什么】：Transformer编码器的基本单元\n",
    "    【做什么】：包含多头自注意力和前馈网络\n",
    "    【为什么】：\n",
    "        - 自注意力：捕获序列内部的依赖关系\n",
    "        - 前馈网络：增加非线性表达能力\n",
    "        - 残差连接：帮助梯度传播\n",
    "        - Layer Normalization：稳定训练\n",
    "\n",
    "    结构：\n",
    "        Input\n",
    "          ↓\n",
    "        Multi-Head Attention\n",
    "          ↓\n",
    "        Add & Norm (残差连接 + 层归一化)\n",
    "          ↓\n",
    "        Feed Forward Network\n",
    "          ↓\n",
    "        Add & Norm\n",
    "          ↓\n",
    "        Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化编码器层\n",
    "\n",
    "        Args:\n",
    "            d_model: 模型维度\n",
    "            num_heads: 注意力头数\n",
    "            d_ff: 前馈网络中间层维度\n",
    "            dropout_rate: Dropout比率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "\n",
    "        # ============================================\n",
    "        # 子层1: 多头自注意力\n",
    "        # ============================================\n",
    "        # 【是什么】：Multi-Head Self-Attention\n",
    "        # 【做什么】：让每个位置关注序列中的所有位置\n",
    "        # 【为什么】：\n",
    "        #   - 捕获长距离依赖\n",
    "        #   - 并行处理整个序列\n",
    "        #   - 多头学习不同的关注模式\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, name='multi_head_attention')\n",
    "\n",
    "        # ============================================\n",
    "        # 子层2: 前馈网络\n",
    "        # ============================================\n",
    "        # 【是什么】：Position-wise Feed Forward Network\n",
    "        # 【做什么】：对每个位置独立进行非线性变换\n",
    "        # 【为什么】：\n",
    "        #   - 增加模型的非线性能力\n",
    "        #   - 每个位置独立处理\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout_rate, name='feed_forward')\n",
    "\n",
    "        # ============================================\n",
    "        # Layer Normalization\n",
    "        # ============================================\n",
    "        # 【是什么】：层归一化\n",
    "        # 【做什么】：归一化每个样本的特征\n",
    "        # 【为什么】：\n",
    "        #   - 稳定训练过程\n",
    "        #   - 加速收敛\n",
    "        #   - 减少对初始化的依赖\n",
    "        #\n",
    "        # 【Layer Norm vs Batch Norm】：\n",
    "        #   - Batch Norm: 对batch维度归一化（适合CNN）\n",
    "        #   - Layer Norm: 对特征维度归一化（适合RNN/Transformer）\n",
    "        #   - Transformer用Layer Norm因为序列长度可变\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6, name='layernorm1')\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6, name='layernorm2')\n",
    "\n",
    "        # ============================================\n",
    "        # Dropout\n",
    "        # ============================================\n",
    "        self.dropout1 = layers.Dropout(dropout_rate, name='dropout1')\n",
    "        self.dropout2 = layers.Dropout(dropout_rate, name='dropout2')\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 输入张量，形状 (batch, seq_len, d_model)\n",
    "            mask: 注意力掩码\n",
    "            training: 是否训练模式\n",
    "\n",
    "        Returns:\n",
    "            输出张量，形状 (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 子层1: 多头自注意力 + 残差连接 + Layer Norm\n",
    "        # ============================================\n",
    "        # 【残差连接】：x + SubLayer(x)\n",
    "        # 【为什么】：\n",
    "        #   - 帮助梯度传播（解决梯度消失）\n",
    "        #   - 允许网络学习恒等映射\n",
    "        #   - 使深层网络更容易训练\n",
    "\n",
    "        # 多头自注意力\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Dropout\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "\n",
    "        # 残差连接 + Layer Norm\n",
    "        # 【顺序】：Add -> Norm (Post-LN)\n",
    "        # 也可以用 Norm -> Add (Pre-LN)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # ============================================\n",
    "        # 子层2: 前馈网络 + 残差连接 + Layer Norm\n",
    "        # ============================================\n",
    "\n",
    "        # 前馈网络\n",
    "        ffn_output = self.ffn(out1, training=training)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # Dropout\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        # 残差连接 + Layer Norm\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch, seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813df4f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer编码器（Transformer Encoder）\n",
    "\n",
    "    【是什么】：多层编码器层的堆叠\n",
    "    【做什么】：将输入序列编码为上下文表示\n",
    "    【为什么】：\n",
    "        - 多层堆叠：逐层提取更抽象的特征\n",
    "        - 第1层：学习局部模式\n",
    "        - 第2-3层：学习中等距离依赖\n",
    "        - 第4-6层：学习长距离依赖和抽象语义\n",
    "\n",
    "    结构：\n",
    "        Input Embedding\n",
    "          ↓\n",
    "        Positional Encoding\n",
    "          ↓\n",
    "        Encoder Layer 1\n",
    "          ↓\n",
    "        Encoder Layer 2\n",
    "          ↓\n",
    "        ...\n",
    "          ↓\n",
    "        Encoder Layer N\n",
    "          ↓\n",
    "        Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff,\n",
    "                 vocab_size, max_len, dropout_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化Transformer编码器\n",
    "\n",
    "        Args:\n",
    "            num_layers: 编码器层数\n",
    "            d_model: 模型维度\n",
    "            num_heads: 注意力头数\n",
    "            d_ff: 前馈网络中间层维度\n",
    "            vocab_size: 词汇表大小\n",
    "            max_len: 最大序列长度\n",
    "            dropout_rate: Dropout比率\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # ============================================\n",
    "        # 词嵌入层\n",
    "        # ============================================\n",
    "        # 【是什么】：将词ID转换为稠密向量\n",
    "        # 【做什么】：查表操作，每个词对应一个向量\n",
    "        # 【为什么】：\n",
    "        #   - 将离散的词转换为连续表示\n",
    "        #   - 相似的词有相似的向量\n",
    "        #   - 可以学习词的语义信息\n",
    "        self.embedding = layers.Embedding(\n",
    "            vocab_size, d_model,\n",
    "            name='token_embedding'\n",
    "        )\n",
    "\n",
    "        # ============================================\n",
    "        # 位置编码\n",
    "        # ============================================\n",
    "        # 【是什么】：添加位置信息\n",
    "        # 【做什么】：让模型知道词的顺序\n",
    "        # 【为什么】：\n",
    "        #   - Self-Attention是无序的\n",
    "        #   - 需要显式添加位置信息\n",
    "        self.pos_encoding = PositionalEncoding(max_len, d_model, name='positional_encoding')\n",
    "\n",
    "        # ============================================\n",
    "        # 编码器层堆叠\n",
    "        # ============================================\n",
    "        # 【是什么】：多个编码器层\n",
    "        # 【做什么】：逐层提取特征\n",
    "        # 【为什么】：\n",
    "        #   - 浅层：学习局部模式\n",
    "        #   - 深层：学习全局语义\n",
    "        #   - 类似于CNN的层次特征提取\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout_rate,\n",
    "                        name=f'encoder_layer_{i}')\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = layers.Dropout(dropout_rate, name='encoder_dropout')\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            x: 输入序列（词ID），形状 (batch, seq_len)\n",
    "            mask: 注意力掩码\n",
    "            training: 是否训练模式\n",
    "\n",
    "        Returns:\n",
    "            编码后的表示，形状 (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 词嵌入\n",
    "        # ============================================\n",
    "        # 【做什么】：将词ID转换为向量\n",
    "        # 例如：[1, 234, 56] -> [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]\n",
    "        x = self.embedding(x)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 缩放\n",
    "        # ============================================\n",
    "        # 【是什么】：将嵌入向量乘以sqrt(d_model)\n",
    "        # 【为什么】：\n",
    "        #   - 嵌入向量的值通常较小（如[-1, 1]）\n",
    "        #   - 位置编码的值也在[-1, 1]\n",
    "        #   - 缩放后，嵌入向量的贡献更大\n",
    "        #   - 防止位置编码淹没词嵌入信息\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 添加位置编码\n",
    "        # ============================================\n",
    "        # 【做什么】：将位置信息加到词嵌入上\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: 通过所有编码器层\n",
    "        # ============================================\n",
    "        # 【做什么】：逐层提取特征\n",
    "        # 【效果】：\n",
    "        #   - 第1层：学习词级别的模式\n",
    "        #   - 第2-3层：学习短语级别的模式\n",
    "        #   - 第4-6层：学习句子级别的语义\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask, training)\n",
    "\n",
    "        return x  # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4741f03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试Transformer编码器\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer编码器测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试参数\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    vocab_size = 1000\n",
    "    max_len = 100\n",
    "\n",
    "    # 模型参数\n",
    "    num_layers = 2\n",
    "    d_model = 128\n",
    "    num_heads = 4\n",
    "    d_ff = 512\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    # 创建随机输入（词ID）\n",
    "    x = tf.random.uniform((batch_size, seq_len), maxval=vocab_size, dtype=tf.int32)\n",
    "\n",
    "    # 测试前馈网络\n",
    "    print(\"\\n测试前馈网络...\")\n",
    "    ffn = FeedForwardNetwork(d_model, d_ff, dropout_rate)\n",
    "    x_test = tf.random.normal((batch_size, seq_len, d_model))\n",
    "    ffn_output = ffn(x_test)\n",
    "    print(f\"输入形状: {x_test.shape}\")\n",
    "    print(f\"输出形状: {ffn_output.shape}\")\n",
    "\n",
    "    # 测试编码器层\n",
    "    print(\"\\n测试编码器层...\")\n",
    "    encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n",
    "    layer_output = encoder_layer(x_test)\n",
    "    print(f\"输入形状: {x_test.shape}\")\n",
    "    print(f\"输出形状: {layer_output.shape}\")\n",
    "\n",
    "    # 测试完整编码器\n",
    "    print(\"\\n测试完整编码器...\")\n",
    "    encoder = TransformerEncoder(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        vocab_size=vocab_size,\n",
    "        max_len=max_len,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    encoder_output = encoder(x)\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {encoder_output.shape}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
