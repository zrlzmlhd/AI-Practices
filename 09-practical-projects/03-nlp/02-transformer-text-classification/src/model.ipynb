{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12898465",
   "metadata": {},
   "source": [
    "# Transformer文本分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a8458",
   "metadata": {},
   "source": [
    "本模块实现基于Transformer的文本分类器，包括：\n",
    "1. 简单Transformer分类器（入门级）\n",
    "2. 改进的Transformer分类器（中级）\n",
    "3. 高级Transformer分类器（高级）\n",
    "\n",
    "每个模型都有详细的注释说明设计思路和参数选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd746d",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886652f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "from transformer import TransformerEncoder\n",
    "from attention import create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442839b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerTextClassifier:\n",
    "    \"\"\"\n",
    "    Transformer文本分类器\n",
    "\n",
    "    【是什么】：基于Transformer编码器的文本分类模型\n",
    "    【做什么】：将文本序列分类为不同类别（如情感分析）\n",
    "    【为什么】：\n",
    "        - Transformer能捕获长距离依赖\n",
    "        - 并行处理，训练速度快\n",
    "        - 在文本分类任务上效果优秀\n",
    "\n",
    "    模型结构：\n",
    "        Input (词ID序列)\n",
    "          ↓\n",
    "        Transformer Encoder\n",
    "          ↓\n",
    "        Global Average Pooling / [CLS] Token\n",
    "          ↓\n",
    "        Dense Layer(s)\n",
    "          ↓\n",
    "        Output (分类概率)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 max_len=512,\n",
    "                 num_classes=2,\n",
    "                 model_type='simple',\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        初始化分类器\n",
    "\n",
    "        Args:\n",
    "            vocab_size: 词汇表大小\n",
    "            max_len: 最大序列长度\n",
    "            num_classes: 分类类别数\n",
    "            model_type: 模型类型 ('simple', 'improved', 'advanced')\n",
    "            **kwargs: 其他参数\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # 根据模型类型设置参数\n",
    "        self.config = self._get_model_config(model_type)\n",
    "        self.config.update(kwargs)\n",
    "\n",
    "        # 创建模型\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _get_model_config(self, model_type):\n",
    "        \"\"\"\n",
    "        获取模型配置\n",
    "\n",
    "        Args:\n",
    "            model_type: 模型类型\n",
    "\n",
    "        Returns:\n",
    "            配置字典\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            'simple': {\n",
    "                # ============================================\n",
    "                # 简单配置（入门级）\n",
    "                # ============================================\n",
    "                # 【适用场景】：\n",
    "                #   - 小数据集（<10k样本）\n",
    "                #   - 快速实验\n",
    "                #   - 学习Transformer基础\n",
    "\n",
    "                'num_layers': 2,        # 编码器层数\n",
    "                # 【为什么=2】：\n",
    "                #   - 2层足够学习基本模式\n",
    "                #   - 训练速度快\n",
    "                #   - 不容易过拟合\n",
    "\n",
    "                'd_model': 128,         # 模型维度\n",
    "                # 【为什么=128】：\n",
    "                #   - 较小的维度，参数量少\n",
    "                #   - 适合小数据集\n",
    "                #   - 训练速度快\n",
    "\n",
    "                'num_heads': 4,         # 注意力头数\n",
    "                # 【为什么=4】：\n",
    "                #   - d_model=128，每个头32维\n",
    "                #   - 4个头学习不同模式\n",
    "                #   - 计算量适中\n",
    "\n",
    "                'd_ff': 512,            # 前馈网络维度\n",
    "                # 【为什么=512】：\n",
    "                #   - 通常是d_model的4倍\n",
    "                #   - 提供足够的非线性能力\n",
    "\n",
    "                'dropout_rate': 0.1,    # Dropout比率\n",
    "                # 【为什么=0.1】：\n",
    "                #   - 轻度正则化\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                'pooling': 'avg',       # 池化方式\n",
    "                # 【为什么='avg'】：\n",
    "                #   - 全局平均池化\n",
    "                #   - 简单有效\n",
    "                #   - 考虑所有位置\n",
    "            },\n",
    "\n",
    "            'improved': {\n",
    "                # ============================================\n",
    "                # 改进配置（中级）\n",
    "                # ============================================\n",
    "                # 【适用场景】：\n",
    "                #   - 中等数据集（10k-100k样本）\n",
    "                #   - 追求更好性能\n",
    "                #   - 有一定计算资源\n",
    "\n",
    "                'num_layers': 4,        # 编码器层数\n",
    "                # 【为什么=4】：\n",
    "                #   - 更深的网络学习更复杂模式\n",
    "                #   - 4层是性能和速度的平衡点\n",
    "\n",
    "                'd_model': 256,         # 模型维度\n",
    "                # 【为什么=256】：\n",
    "                #   - 更大的表示能力\n",
    "                #   - 适合中等数据集\n",
    "\n",
    "                'num_heads': 8,         # 注意力头数\n",
    "                # 【为什么=8】：\n",
    "                #   - d_model=256，每个头32维\n",
    "                #   - 8个头学习更多样的模式\n",
    "\n",
    "                'd_ff': 1024,           # 前馈网络维度\n",
    "                # 【为什么=1024】：\n",
    "                #   - d_model的4倍\n",
    "                #   - 更强的非线性能力\n",
    "\n",
    "                'dropout_rate': 0.2,    # Dropout比率\n",
    "                # 【为什么=0.2】：\n",
    "                #   - 中等强度正则化\n",
    "                #   - 防止过拟合\n",
    "\n",
    "                'pooling': 'cls',       # 池化方式\n",
    "                # 【为什么='cls'】：\n",
    "                #   - 使用[CLS] token\n",
    "                #   - 类似BERT的做法\n",
    "                #   - 学习全局表示\n",
    "            },\n",
    "\n",
    "            'advanced': {\n",
    "                # ============================================\n",
    "                # 高级配置（高级）\n",
    "                # ============================================\n",
    "                # 【适用场景】：\n",
    "                #   - 大数据集（>100k样本）\n",
    "                #   - 追求最佳性能\n",
    "                #   - 有充足计算资源\n",
    "\n",
    "                'num_layers': 6,        # 编码器层数\n",
    "                # 【为什么=6】：\n",
    "                #   - BERT-base的层数\n",
    "                #   - 学习深层语义\n",
    "\n",
    "                'd_model': 512,         # 模型维度\n",
    "                # 【为什么=512】：\n",
    "                #   - 标准Transformer维度\n",
    "                #   - 强大的表示能力\n",
    "\n",
    "                'num_heads': 8,         # 注意力头数\n",
    "                # 【为什么=8】：\n",
    "                #   - d_model=512，每个头64维\n",
    "                #   - 标准配置\n",
    "\n",
    "                'd_ff': 2048,           # 前馈网络维度\n",
    "                # 【为什么=2048】：\n",
    "                #   - d_model的4倍\n",
    "                #   - 标准Transformer配置\n",
    "\n",
    "                'dropout_rate': 0.3,    # Dropout比率\n",
    "                # 【为什么=0.3】：\n",
    "                #   - 较强正则化\n",
    "                #   - 大模型需要更强的正则化\n",
    "\n",
    "                'pooling': 'attention', # 池化方式\n",
    "                # 【为什么='attention'】：\n",
    "                #   - 注意力池化\n",
    "                #   - 自动学习重要位置\n",
    "                #   - 最灵活的方式\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return configs.get(model_type, configs['simple'])\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        构建模型\n",
    "\n",
    "        Returns:\n",
    "            Keras模型\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 输入层\n",
    "        # ============================================\n",
    "        # 【是什么】：词ID序列\n",
    "        # 【形状】：(batch, seq_len)\n",
    "        inputs = layers.Input(shape=(self.max_len,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "        # ============================================\n",
    "        # Transformer编码器\n",
    "        # ============================================\n",
    "        # 【是什么】：多层Transformer编码器\n",
    "        # 【做什么】：将词序列编码为上下文表示\n",
    "        encoder = TransformerEncoder(\n",
    "            num_layers=self.config['num_layers'],\n",
    "            d_model=self.config['d_model'],\n",
    "            num_heads=self.config['num_heads'],\n",
    "            d_ff=self.config['d_ff'],\n",
    "            vocab_size=self.vocab_size,\n",
    "            max_len=self.max_len,\n",
    "            dropout_rate=self.config['dropout_rate']\n",
    "        )\n",
    "\n",
    "        # 创建padding掩码\n",
    "        # 【是什么】：遮蔽padding位置\n",
    "        # 【为什么】：padding位置不应该被关注\n",
    "        mask = create_padding_mask(inputs)\n",
    "\n",
    "        # 编码\n",
    "        encoder_output = encoder(inputs, mask=mask)\n",
    "        # 形状: (batch, seq_len, d_model)\n",
    "\n",
    "        # ============================================\n",
    "        # 池化层\n",
    "        # ============================================\n",
    "        # 【是什么】：将序列表示转换为固定长度向量\n",
    "        # 【为什么】：分类需要固定长度的输入\n",
    "\n",
    "        pooling_type = self.config['pooling']\n",
    "\n",
    "        if pooling_type == 'avg':\n",
    "            # ============================================\n",
    "            # 全局平均池化\n",
    "            # ============================================\n",
    "            # 【是什么】：对序列维度求平均\n",
    "            # 【做什么】：将(batch, seq_len, d_model)变为(batch, d_model)\n",
    "            # 【为什么】：\n",
    "            #   - 简单有效\n",
    "            #   - 考虑所有位置\n",
    "            #   - 对序列长度不敏感\n",
    "            pooled = layers.GlobalAveragePooling1D(name='global_avg_pooling')(encoder_output)\n",
    "\n",
    "        elif pooling_type == 'max':\n",
    "            # ============================================\n",
    "            # 全局最大池化\n",
    "            # ============================================\n",
    "            # 【是什么】：对序列维度取最大值\n",
    "            # 【为什么】：\n",
    "            #   - 关注最显著的特征\n",
    "            #   - 适合关键词检测\n",
    "            pooled = layers.GlobalMaxPooling1D(name='global_max_pooling')(encoder_output)\n",
    "\n",
    "        elif pooling_type == 'cls':\n",
    "            # ============================================\n",
    "            # [CLS] Token池化\n",
    "            # ============================================\n",
    "            # 【是什么】：使用第一个位置的输出\n",
    "            # 【为什么】：\n",
    "            #   - 类似BERT的做法\n",
    "            #   - [CLS]位置学习全局表示\n",
    "            #   - 需要在输入时添加[CLS] token\n",
    "            pooled = encoder_output[:, 0, :]  # (batch, d_model)\n",
    "\n",
    "        elif pooling_type == 'attention':\n",
    "            # ============================================\n",
    "            # 注意力池化\n",
    "            # ============================================\n",
    "            # 【是什么】：学习每个位置的权重\n",
    "            # 【做什么】：加权求和\n",
    "            # 【为什么】：\n",
    "            #   - 自动学习重要位置\n",
    "            #   - 最灵活的方式\n",
    "            #   - 效果通常最好\n",
    "\n",
    "            # 注意力权重\n",
    "            attention = layers.Dense(1, activation='tanh', name='attention_weights')(encoder_output)\n",
    "            attention = layers.Flatten()(attention)\n",
    "            attention = layers.Activation('softmax', name='attention_softmax')(attention)\n",
    "            attention = layers.RepeatVector(self.config['d_model'])(attention)\n",
    "            attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "            # 加权求和\n",
    "            pooled = layers.Multiply()([encoder_output, attention])\n",
    "            pooled = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), name='attention_pooling')(pooled)\n",
    "\n",
    "        else:\n",
    "            # 默认使用平均池化\n",
    "            pooled = layers.GlobalAveragePooling1D()(encoder_output)\n",
    "\n",
    "        # ============================================\n",
    "        # 分类头\n",
    "        # ============================================\n",
    "        # 【是什么】：全连接层 + 输出层\n",
    "        # 【做什么】：将编码表示映射到类别概率\n",
    "\n",
    "        # Dropout\n",
    "        x = layers.Dropout(self.config['dropout_rate'], name='classifier_dropout')(pooled)\n",
    "\n",
    "        # 中间层（可选）\n",
    "        if self.model_type in ['improved', 'advanced']:\n",
    "            # ============================================\n",
    "            # 添加中间全连接层\n",
    "            # ============================================\n",
    "            # 【为什么】：\n",
    "            #   - 增加非线性能力\n",
    "            #   - 更好地适应分类任务\n",
    "            x = layers.Dense(\n",
    "                self.config['d_model'] // 2,\n",
    "                activation='relu',\n",
    "                name='classifier_hidden'\n",
    "            )(x)\n",
    "            x = layers.Dropout(self.config['dropout_rate'], name='classifier_dropout2')(x)\n",
    "\n",
    "        # 输出层\n",
    "        # 【是什么】：全连接层 + Softmax\n",
    "        # 【做什么】：输出每个类别的概率\n",
    "        if self.num_classes == 2:\n",
    "            # 二分类：使用sigmoid\n",
    "            outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "        else:\n",
    "            # 多分类：使用softmax\n",
    "            outputs = layers.Dense(self.num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "        # 创建模型\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=f'transformer_classifier_{self.model_type}')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        编译模型\n",
    "\n",
    "        Args:\n",
    "            learning_rate: 学习率\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 优化器\n",
    "        # ============================================\n",
    "        # 【是什么】：Adam优化器\n",
    "        # 【为什么】：\n",
    "        #   - 自适应学习率\n",
    "        #   - 对超参数不敏感\n",
    "        #   - Transformer标准选择\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # ============================================\n",
    "        # 损失函数\n",
    "        # ============================================\n",
    "        if self.num_classes == 2:\n",
    "            # 二分类：binary crossentropy\n",
    "            loss = 'binary_crossentropy'\n",
    "            metrics = ['accuracy', keras.metrics.AUC(name='auc')]\n",
    "        else:\n",
    "            # 多分类：categorical crossentropy\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val,\n",
    "              epochs=10, batch_size=32, learning_rate=1e-4,\n",
    "              callbacks=None, verbose=1):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "\n",
    "        Args:\n",
    "            X_train: 训练数据\n",
    "            y_train: 训练标签\n",
    "            X_val: 验证数据\n",
    "            y_val: 验证标签\n",
    "            epochs: 训练轮数\n",
    "            batch_size: 批大小\n",
    "            learning_rate: 学习率\n",
    "            callbacks: 回调函数列表\n",
    "            verbose: 详细程度\n",
    "\n",
    "        Returns:\n",
    "            训练历史\n",
    "        \"\"\"\n",
    "        # 编译模型\n",
    "        self.compile_model(learning_rate=learning_rate)\n",
    "\n",
    "        # 训练\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测\n",
    "\n",
    "        Args:\n",
    "            X: 输入数据\n",
    "\n",
    "        Returns:\n",
    "            预测结果\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(X)\n",
    "\n",
    "        if self.num_classes == 2:\n",
    "            # 二分类：返回0或1\n",
    "            return (predictions > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            # 多分类：返回类别索引\n",
    "            return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        预测概率\n",
    "\n",
    "        Args:\n",
    "            X: 输入数据\n",
    "\n",
    "        Returns:\n",
    "            预测概率\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        评估模型\n",
    "\n",
    "        Args:\n",
    "            X: 测试数据\n",
    "            y: 测试标签\n",
    "\n",
    "        Returns:\n",
    "            评估指标字典\n",
    "        \"\"\"\n",
    "        results = self.model.evaluate(X, y, verbose=0)\n",
    "\n",
    "        metrics = {}\n",
    "        for name, value in zip(self.model.metrics_names, results):\n",
    "            metrics[name] = value\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "\n",
    "        Args:\n",
    "            filepath: 保存路径\n",
    "        \"\"\"\n",
    "        self.model.save(filepath)\n",
    "        print(f\"✓ 模型已保存: {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "\n",
    "        Args:\n",
    "            filepath: 模型路径\n",
    "        \"\"\"\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        print(f\"✓ 模型已加载: {filepath}\")\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"打印模型摘要\"\"\"\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b6005",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer文本分类模型测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试参数\n",
    "    vocab_size = 10000\n",
    "    max_len = 128\n",
    "    num_classes = 2\n",
    "    batch_size = 4\n",
    "\n",
    "    # 创建随机数据\n",
    "    X_train = np.random.randint(0, vocab_size, (100, max_len))\n",
    "    y_train = np.random.randint(0, num_classes, (100,))\n",
    "    X_val = np.random.randint(0, vocab_size, (20, max_len))\n",
    "    y_val = np.random.randint(0, num_classes, (20,))\n",
    "\n",
    "    # 测试三种模型\n",
    "    for model_type in ['simple', 'improved', 'advanced']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"测试 {model_type} 模型\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # 创建模型\n",
    "        classifier = TransformerTextClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=max_len,\n",
    "            num_classes=num_classes,\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        # 打印摘要\n",
    "        print(f\"\\n模型结构:\")\n",
    "        classifier.summary()\n",
    "\n",
    "        # 训练\n",
    "        print(f\"\\n训练模型...\")\n",
    "        history = classifier.train(\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            epochs=2,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # 评估\n",
    "        metrics = classifier.evaluate(X_val, y_val)\n",
    "        print(f\"\\n验证集性能:\")\n",
    "        for name, value in metrics.items():\n",
    "            print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "        # 预测\n",
    "        predictions = classifier.predict(X_val[:5])\n",
    "        print(f\"\\n预测结果: {predictions}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
