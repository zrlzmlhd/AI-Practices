{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0dbc3ff",
   "metadata": {},
   "source": [
    "# 数据加载和预处理模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf6ca4",
   "metadata": {},
   "source": [
    "本模块负责：\n",
    "1. 加载IMDB数据集\n",
    "2. 文本预处理\n",
    "3. 构建词汇表\n",
    "4. 序列填充\n",
    "5. 数据集划分\n",
    "\n",
    "每个步骤都有详细的注释说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebaa2d5",
   "metadata": {},
   "source": [
    "## Notebook运行提示\n",
    "- 代码已拆分为多个小单元, 按顺序运行即可在每一步观察输出与中间变量。\n",
    "- 涉及 `Path(__file__)` 或相对路径的脚本会自动注入 `__file__` 解析逻辑, Notebook 环境下也能引用原项目资源。\n",
    "- 可在每个单元下追加说明或参数试验记录, 以跟踪核心算法和数据处理步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c014d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff046e53",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    文本预处理器\n",
    "\n",
    "    【是什么】：文本清洗和标准化工具\n",
    "    【做什么】：将原始文本转换为干净的文本\n",
    "    【为什么】：\n",
    "        - 去除噪声（HTML标签、特殊字符）\n",
    "        - 统一格式（小写、空格）\n",
    "        - 提高模型性能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lower=True, remove_html=True, remove_special_chars=True):\n",
    "        \"\"\"\n",
    "        初始化预处理器\n",
    "\n",
    "        Args:\n",
    "            lower: 是否转换为小写\n",
    "            remove_html: 是否去除HTML标签\n",
    "            remove_special_chars: 是否去除特殊字符\n",
    "        \"\"\"\n",
    "        self.lower = lower\n",
    "        self.remove_html = remove_html\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        清洗文本\n",
    "\n",
    "        Args:\n",
    "            text: 原始文本\n",
    "\n",
    "        Returns:\n",
    "            清洗后的文本\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # 步骤1: 去除HTML标签\n",
    "        # ============================================\n",
    "        # 【是什么】：<br />, <p>, </div> 等\n",
    "        # 【为什么】：IMDB数据包含HTML标签\n",
    "        if self.remove_html:\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 去除特殊字符\n",
    "        # ============================================\n",
    "        # 【是什么】：保留字母、数字、基本标点\n",
    "        # 【为什么】：\n",
    "        #   - 减少词汇表大小\n",
    "        #   - 去除噪声\n",
    "        if self.remove_special_chars:\n",
    "            text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\-]', ' ', text)\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 处理多余空格\n",
    "        # ============================================\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: 转换为小写\n",
    "        # ============================================\n",
    "        # 【为什么】：\n",
    "        #   - \"Good\"和\"good\"应该是同一个词\n",
    "        #   - 减少词汇表大小\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"使预处理器可调用\"\"\"\n",
    "        return self.clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62493a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    词汇表\n",
    "\n",
    "    【是什么】：词和ID的映射\n",
    "    【做什么】：\n",
    "        - 词 -> ID（编码）\n",
    "        - ID -> 词（解码）\n",
    "    【为什么】：\n",
    "        - 模型只能处理数字\n",
    "        - 需要将词转换为ID\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_vocab_size=10000, min_freq=2):\n",
    "        \"\"\"\n",
    "        初始化词汇表\n",
    "\n",
    "        Args:\n",
    "            max_vocab_size: 最大词汇表大小\n",
    "            min_freq: 最小词频（低于此频率的词被忽略）\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        # 特殊token\n",
    "        # 【是什么】：特殊用途的token\n",
    "        # 【为什么】：\n",
    "        #   - <PAD>: 填充短序列\n",
    "        #   - <UNK>: 未知词（不在词汇表中的词）\n",
    "        #   - <CLS>: 分类token（可选，用于BERT风格）\n",
    "        #   - <SEP>: 分隔token（可选）\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.CLS_TOKEN = '<CLS>'\n",
    "        self.SEP_TOKEN = '<SEP>'\n",
    "\n",
    "        self.special_tokens = [\n",
    "            self.PAD_TOKEN,\n",
    "            self.UNK_TOKEN,\n",
    "            self.CLS_TOKEN,\n",
    "            self.SEP_TOKEN\n",
    "        ]\n",
    "\n",
    "        # 词汇表\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = Counter()\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        构建词汇表\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "        \"\"\"\n",
    "        print(\"\\n构建词汇表...\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤1: 统计词频\n",
    "        # ============================================\n",
    "        # 【做什么】：计算每个词出现的次数\n",
    "        # 【为什么】：\n",
    "        #   - 保留高频词\n",
    "        #   - 过滤低频词（可能是噪声）\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            self.word_freq.update(words)\n",
    "\n",
    "        print(f\"  总词数: {len(self.word_freq)}\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤2: 过滤低频词\n",
    "        # ============================================\n",
    "        # 【是什么】：去除出现次数 < min_freq 的词\n",
    "        # 【为什么】：\n",
    "        #   - 低频词可能是拼写错误\n",
    "        #   - 减少词汇表大小\n",
    "        #   - 提高泛化能力\n",
    "        filtered_words = [\n",
    "            word for word, freq in self.word_freq.items()\n",
    "            if freq >= self.min_freq\n",
    "        ]\n",
    "        print(f\"  过滤后: {len(filtered_words)} (min_freq={self.min_freq})\")\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤3: 选择最常见的词\n",
    "        # ============================================\n",
    "        # 【是什么】：按频率排序，取前max_vocab_size个\n",
    "        # 【为什么】：\n",
    "        #   - 限制词汇表大小\n",
    "        #   - 高频词包含更多信息\n",
    "        most_common = self.word_freq.most_common(self.max_vocab_size - len(self.special_tokens))\n",
    "        vocab_words = [word for word, _ in most_common]\n",
    "\n",
    "        # ============================================\n",
    "        # 步骤4: 构建映射\n",
    "        # ============================================\n",
    "        # 【做什么】：创建词<->ID的双向映射\n",
    "\n",
    "        # 添加特殊token\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "\n",
    "        # 添加普通词\n",
    "        for idx, word in enumerate(vocab_words, start=len(self.special_tokens)):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "        print(f\"  最终词汇表大小: {len(self.word2idx)}\")\n",
    "        print(f\"  覆盖率: {self._calculate_coverage(texts):.2%}\")\n",
    "\n",
    "    def _calculate_coverage(self, texts):\n",
    "        \"\"\"\n",
    "        计算词汇表覆盖率\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "\n",
    "        Returns:\n",
    "            覆盖率（0-1之间）\n",
    "        \"\"\"\n",
    "        total_words = 0\n",
    "        covered_words = 0\n",
    "\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            total_words += len(words)\n",
    "            covered_words += sum(1 for word in words if word in self.word2idx)\n",
    "\n",
    "        return covered_words / total_words if total_words > 0 else 0\n",
    "\n",
    "    def encode(self, text, add_cls=False, add_sep=False):\n",
    "        \"\"\"\n",
    "        将文本编码为ID序列\n",
    "\n",
    "        Args:\n",
    "            text: 文本字符串\n",
    "            add_cls: 是否添加[CLS] token\n",
    "            add_sep: 是否添加[SEP] token\n",
    "\n",
    "        Returns:\n",
    "            ID列表\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "\n",
    "        # 转换为ID\n",
    "        # 【是什么】：查表操作\n",
    "        # 【为什么】：不在词汇表中的词用<UNK>代替\n",
    "        ids = [\n",
    "            self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n",
    "            for word in words\n",
    "        ]\n",
    "\n",
    "        # 添加特殊token\n",
    "        if add_cls:\n",
    "            ids = [self.word2idx[self.CLS_TOKEN]] + ids\n",
    "        if add_sep:\n",
    "            ids = ids + [self.word2idx[self.SEP_TOKEN]]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        将ID序列解码为文本\n",
    "\n",
    "        Args:\n",
    "            ids: ID列表\n",
    "\n",
    "        Returns:\n",
    "            文本字符串\n",
    "        \"\"\"\n",
    "        words = [\n",
    "            self.idx2word.get(idx, self.UNK_TOKEN)\n",
    "            for idx in ids\n",
    "        ]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"保存词汇表\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'word2idx': self.word2idx,\n",
    "                'idx2word': self.idx2word,\n",
    "                'word_freq': self.word_freq,\n",
    "                'max_vocab_size': self.max_vocab_size,\n",
    "                'min_freq': self.min_freq\n",
    "            }, f)\n",
    "        print(f\"✓ 词汇表已保存: {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        \"\"\"加载词汇表\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.word2idx = data['word2idx']\n",
    "            self.idx2word = data['idx2word']\n",
    "            self.word_freq = data['word_freq']\n",
    "            self.max_vocab_size = data['max_vocab_size']\n",
    "            self.min_freq = data['min_freq']\n",
    "        print(f\"✓ 词汇表已加载: {filepath}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回词汇表大小\"\"\"\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbfe1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_imdb_data(data_dir, max_samples=None):\n",
    "    \"\"\"\n",
    "    加载IMDB数据集\n",
    "\n",
    "    Args:\n",
    "        data_dir: 数据目录\n",
    "        max_samples: 最大样本数（用于快速测试）\n",
    "\n",
    "    Returns:\n",
    "        (texts, labels) 元组\n",
    "    \"\"\"\n",
    "    print(\"\\n加载IMDB数据集...\")\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # ============================================\n",
    "    # 加载正面评论\n",
    "    # ============================================\n",
    "    pos_dir = data_dir / 'pos'\n",
    "    if pos_dir.exists():\n",
    "        pos_files = list(pos_dir.glob('*.txt'))\n",
    "        if max_samples:\n",
    "            pos_files = pos_files[:max_samples // 2]\n",
    "\n",
    "        for file_path in pos_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1)  # 正面=1\n",
    "\n",
    "    # ============================================\n",
    "    # 加载负面评论\n",
    "    # ============================================\n",
    "    neg_dir = data_dir / 'neg'\n",
    "    if neg_dir.exists():\n",
    "        neg_files = list(neg_dir.glob('*.txt'))\n",
    "        if max_samples:\n",
    "            neg_files = neg_files[:max_samples // 2]\n",
    "\n",
    "        for file_path in neg_files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(0)  # 负面=0\n",
    "\n",
    "    print(f\"  加载样本数: {len(texts)}\")\n",
    "    print(f\"  正面样本: {sum(labels)}\")\n",
    "    print(f\"  负面样本: {len(labels) - sum(labels)}\")\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b5e76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data(data_dir='data/aclImdb',\n",
    "                 max_vocab_size=10000,\n",
    "                 max_len=256,\n",
    "                 test_size=0.2,\n",
    "                 val_size=0.1,\n",
    "                 random_state=42,\n",
    "                 max_samples=None):\n",
    "    \"\"\"\n",
    "    准备数据\n",
    "\n",
    "    Args:\n",
    "        data_dir: 数据目录\n",
    "        max_vocab_size: 最大词汇表大小\n",
    "        max_len: 最大序列长度\n",
    "        test_size: 测试集比例\n",
    "        val_size: 验证集比例\n",
    "        random_state: 随机种子\n",
    "        max_samples: 最大样本数\n",
    "\n",
    "    Returns:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test), vocab\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"数据准备\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤1: 加载数据\n",
    "    # ============================================\n",
    "    train_dir = Path(data_dir) / 'train'\n",
    "    test_dir = Path(data_dir) / 'test'\n",
    "\n",
    "    # 加载训练集\n",
    "    train_texts, train_labels = load_imdb_data(train_dir, max_samples)\n",
    "\n",
    "    # 加载测试集\n",
    "    test_texts, test_labels = load_imdb_data(test_dir, max_samples)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤2: 文本预处理\n",
    "    # ============================================\n",
    "    print(\"\\n文本预处理...\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "\n",
    "    train_texts = [preprocessor(text) for text in train_texts]\n",
    "    test_texts = [preprocessor(text) for text in test_texts]\n",
    "\n",
    "    print(f\"  训练集样本: {len(train_texts)}\")\n",
    "    print(f\"  测试集样本: {len(test_texts)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤3: 构建词汇表\n",
    "    # ============================================\n",
    "    vocab = Vocabulary(max_vocab_size=max_vocab_size)\n",
    "    vocab.build_vocab(train_texts)\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤4: 文本编码\n",
    "    # ============================================\n",
    "    print(\"\\n文本编码...\")\n",
    "\n",
    "    # 【是什么】：将文本转换为ID序列\n",
    "    # 【为什么】：模型只能处理数字\n",
    "    train_sequences = [vocab.encode(text) for text in train_texts]\n",
    "    test_sequences = [vocab.encode(text) for text in test_texts]\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤5: 序列填充\n",
    "    # ============================================\n",
    "    print(\"\\n序列填充...\")\n",
    "\n",
    "    # 【是什么】：将序列填充到相同长度\n",
    "    # 【为什么】：\n",
    "    #   - 批处理需要相同长度\n",
    "    #   - 短序列用<PAD>填充\n",
    "    #   - 长序列被截断\n",
    "\n",
    "    # 【padding='post'】：在后面填充\n",
    "    # 【truncating='post'】：从后面截断\n",
    "    X_train = pad_sequences(\n",
    "        train_sequences,\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=vocab.word2idx[vocab.PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    X_test = pad_sequences(\n",
    "        test_sequences,\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=vocab.word2idx[vocab.PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    y_train = np.array(train_labels)\n",
    "    y_test = np.array(test_labels)\n",
    "\n",
    "    print(f\"  训练集形状: {X_train.shape}\")\n",
    "    print(f\"  测试集形状: {X_test.shape}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 步骤6: 划分验证集\n",
    "    # ============================================\n",
    "    print(\"\\n划分验证集...\")\n",
    "\n",
    "    # 【是什么】：从训练集中分出验证集\n",
    "    # 【为什么】：\n",
    "    #   - 用于调参和早停\n",
    "    #   - 避免在测试集上调参\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train  # 保持类别比例\n",
    "    )\n",
    "\n",
    "    print(f\"  训练集: {X_train.shape}\")\n",
    "    print(f\"  验证集: {X_val.shape}\")\n",
    "    print(f\"  测试集: {X_test.shape}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 数据统计\n",
    "    # ============================================\n",
    "    print(\"\\n数据统计:\")\n",
    "    print(f\"  词汇表大小: {len(vocab)}\")\n",
    "    print(f\"  最大序列长度: {max_len}\")\n",
    "    print(f\"  训练集正负比: {y_train.sum()}/{len(y_train)-y_train.sum()}\")\n",
    "    print(f\"  验证集正负比: {y_val.sum()}/{len(y_val)-y_val.sum()}\")\n",
    "    print(f\"  测试集正负比: {y_test.sum()}/{len(y_test)-y_test.sum()}\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8c03f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    测试数据处理\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"数据处理模块测试\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 测试文本预处理\n",
    "    print(\"\\n测试文本预处理...\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "    text = \"This is a <b>GREAT</b> movie!!! I really enjoyed it.\"\n",
    "    cleaned = preprocessor(text)\n",
    "    print(f\"原始文本: {text}\")\n",
    "    print(f\"清洗后: {cleaned}\")\n",
    "\n",
    "    # 测试词汇表\n",
    "    print(\"\\n测试词汇表...\")\n",
    "    texts = [\n",
    "        \"this is a good movie\",\n",
    "        \"this is a bad movie\",\n",
    "        \"i love this movie\",\n",
    "        \"i hate this movie\"\n",
    "    ]\n",
    "    vocab = Vocabulary(max_vocab_size=20, min_freq=1)\n",
    "    vocab.build_vocab(texts)\n",
    "\n",
    "    print(f\"\\n词汇表大小: {len(vocab)}\")\n",
    "    print(f\"词汇表: {list(vocab.word2idx.keys())[:10]}\")\n",
    "\n",
    "    # 测试编码\n",
    "    text = \"this is a good movie\"\n",
    "    ids = vocab.encode(text)\n",
    "    decoded = vocab.decode(ids)\n",
    "    print(f\"\\n原始文本: {text}\")\n",
    "    print(f\"编码: {ids}\")\n",
    "    print(f\"解码: {decoded}\")\n",
    "\n",
    "    print(\"\\n✓ 所有测试通过！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
