# Deep Q-Network 变体算法知识点总结

## 目录

1. [背景与动机](#1-背景与动机)
2. [Double DQN](#2-double-dqn)
3. [Dueling DQN](#3-dueling-dqn)
4. [Noisy Networks](#4-noisy-networks)
5. [Categorical DQN (C51)](#5-categorical-dqn-c51)
6. [Prioritized Experience Replay](#6-prioritized-experience-replay)
7. [N-step Learning](#7-n-step-learning)
8. [Rainbow](#8-rainbow)
9. [实现要点](#9-实现要点)
10. [算法对比](#10-算法对比)

---

## 1. 背景与动机

### 1.1 原始DQN回顾

**核心组件**：
- 神经网络近似Q函数: $Q(s, a; \theta)$
- 经验回放缓冲区 (Experience Replay)
- 目标网络 (Target Network)

**TD目标**：
$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

### 1.2 原始DQN的四大局限

| 问题 | 原因 | 后果 |
|------|------|------|
| **过估计偏差** | max操作同时用于选择和评估 | 不稳定训练、次优策略 |
| **样本效率低** | 均匀随机采样 | 学习慢、数据浪费 |
| **探索能力弱** | ε-greedy与状态无关 | 局部最优、难以逃离 |
| **标量值局限** | 只建模期望值 | 丢失分布信息、风险中立 |

---

## 2. Double DQN

### 2.1 核心思想

**问题**：标准DQN的max操作导致系统性过估计
$$\mathbb{E}[\max_a Q(s,a)] \geq \max_a \mathbb{E}[Q(s,a)]$$

**解决方案**：解耦动作选择与动作评估

### 2.2 数学公式

**标准DQN目标**：
$$y^{\text{DQN}} = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

**Double DQN目标**：
$$y^{\text{Double}} = r + \gamma Q\left(s', \underbrace{\arg\max_{a'} Q(s', a'; \theta)}_{\text{online选择}}; \theta^-\right)$$

### 2.3 关键点

1. **Online网络** ($\theta$) 选择最优动作
2. **Target网络** ($\theta^-$) 评估该动作的价值
3. 两个网络的噪声独立，防止同一噪声同时放大选择和评估

### 2.4 实现代码要点

```python
# 标准DQN
next_q = target_net(next_states).max(dim=1)[0]

# Double DQN
best_actions = online_net(next_states).argmax(dim=1)
next_q = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)
```

---

## 3. Dueling DQN

### 3.1 核心思想

将Q函数分解为**状态价值**和**动作优势**：
$$Q(s, a) = V(s) + A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a')$$

### 3.2 直觉理解

- **V(s)**：这个状态有多好？（与动作无关）
- **A(s,a)**：动作a比平均动作好多少？

### 3.3 可辨识性

**问题**：$Q = V + A$ 有无穷多分解方式

**解决方案**：强制 $\sum_a A(s,a) = 0$，通过减去均值实现

### 3.4 优势

1. 更好的泛化：可以在不评估每个动作的情况下学习状态价值
2. 对于动作选择不重要的状态，更快学习
3. 提高样本效率

### 3.5 网络架构

```
Input → Shared Layers → Value Stream  → V(s) [1维]
                     ↘
                       Advantage Stream → A(s,a) [|A|维]
                     ↘
                       聚合 → Q(s,a) = V + (A - mean(A))
```

---

## 4. Noisy Networks

### 4.1 核心思想

用**可学习的参数化噪声**替代ε-greedy探索

### 4.2 Noisy Linear层

$$y = (\mu^w + \sigma^w \odot \varepsilon^w) x + (\mu^b + \sigma^b \odot \varepsilon^b)$$

- $\mu$: 可学习的均值
- $\sigma$: 可学习的噪声尺度
- $\varepsilon$: 随机噪声

### 4.3 因式分解噪声

减少参数量的技巧：
$$\varepsilon_{ij} = f(\varepsilon_i) \cdot f(\varepsilon_j), \quad f(x) = \text{sign}(x)\sqrt{|x|}$$

参数量从 $O(pq)$ 降到 $O(p+q)$

### 4.4 优势

1. **状态依赖探索**：不同状态有不同的探索程度
2. **自动退火**：随着学习进行，σ自然减小
3. **端到端学习**：不需要手动调整ε衰减

---

## 5. Categorical DQN (C51)

### 5.1 核心思想

从建模**期望值**转向建模**完整回报分布**

### 5.2 分布表示

使用N个固定支撑点的离散分布：
$$Z(s, a) \sim \text{Categorical}(z_1, ..., z_N; p_1, ..., p_N)$$

**支撑点**：
$$z_i = V_{\min} + i \cdot \Delta z, \quad \Delta z = \frac{V_{\max} - V_{\min}}{N - 1}$$

### 5.3 分布式Bellman算子

$$\mathcal{T} Z(s, a) \stackrel{D}{=} R + \gamma Z(S', A')$$

### 5.4 投影操作

由于 $r + \gamma z_j$ 可能落在支撑点之间，需要投影：
$$(\Phi \mathcal{T} Z)_i = \sum_j \left[1 - \frac{|[\mathcal{T}z_j]_{V_{\min}}^{V_{\max}} - z_i|}{\Delta z}\right]_0^1 p_j$$

### 5.5 损失函数

KL散度：
$$L = D_{KL}(\Phi \mathcal{T} Z(s, a) \| Z(s, a; \theta))$$

### 5.6 超参数

| 参数 | 典型值 | 说明 |
|------|--------|------|
| N (atoms) | 51 | 支撑点数量 |
| $V_{\min}$ | -10 | 最小回报 |
| $V_{\max}$ | 10 | 最大回报 |

---

## 6. Prioritized Experience Replay

### 6.1 核心思想

根据**TD误差大小**分配采样优先级，更频繁地采样"惊讶"样本

### 6.2 优先级定义

$$p_i = |\delta_i| + \epsilon$$

其中 $\delta_i$ 是TD误差，$\epsilon$ 防止零优先级

### 6.3 采样概率

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

- $\alpha = 0$: 均匀采样
- $\alpha = 1$: 完全优先级

### 6.4 重要性采样校正

优先级采样引入偏差，需要IS权重校正：
$$w_i = \left( \frac{1}{N \cdot P(i)} \right)^\beta / \max_j w_j$$

$\beta$ 从 $\beta_0$ 退火到 1

### 6.5 SumTree数据结构

- 叶节点存储优先级
- 内部节点存储子节点和
- 操作复杂度: $O(\log N)$

### 6.6 超参数

| 参数 | 典型值 | 说明 |
|------|--------|------|
| $\alpha$ | 0.6 | 优先级指数 |
| $\beta_0$ | 0.4 | 初始IS指数 |
| $\epsilon$ | 1e-6 | 防止零优先级 |

---

## 7. N-step Learning

### 7.1 核心思想

用多步回报替代单步bootstrap，实现偏差-方差权衡

### 7.2 N步回报

$$G_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k+1} + \gamma^n V(s_{t+n})$$

### 7.3 偏差-方差权衡

| n | 偏差 | 方差 | 特点 |
|---|------|------|------|
| 1 | 高 | 低 | TD(0) |
| 3-5 | 中 | 中 | Sweet spot |
| ∞ | 零 | 高 | Monte Carlo |

### 7.4 典型值

- Rainbow使用 $n = 3$
- 通常 $n \in [3, 5]$ 效果最好

---

## 8. Rainbow

### 8.1 组合策略

Rainbow = Double + Dueling + Noisy + Categorical + PER + N-step

### 8.2 性能 (Atari中位数人类标准化分数)

| 算法 | 分数 |
|------|------|
| DQN | 79% |
| Double DQN | 117% |
| Dueling DQN | 151% |
| Prioritized DQN | 141% |
| Categorical DQN | 235% |
| **Rainbow** | **441%** |

### 8.3 消融研究发现

移除组件的影响（从大到小）：
1. Multi-step (-101点)
2. PER (-83点)
3. Distributional (-126点)
4. Noisy (-129点)

---

## 9. 实现要点

### 9.1 经验回放

```python
class ReplayBuffer:
    def push(self, transition):
        # O(1) 添加
        self.buffer.append(transition)

    def sample(self, batch_size):
        # O(B) 采样
        return random.sample(self.buffer, batch_size)
```

### 9.2 目标网络更新

**硬更新**：
```python
if step % target_update_freq == 0:
    target_net.load_state_dict(online_net.state_dict())
```

**软更新**：
```python
for target_param, online_param in zip(target_net.parameters(), online_net.parameters()):
    target_param.data.copy_(tau * online_param.data + (1 - tau) * target_param.data)
```

### 9.3 梯度裁剪

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
```

---

## 10. 算法对比

### 10.1 功能对比

| 变体 | 过估计 | 样本效率 | 探索 | 分布建模 |
|------|--------|----------|------|----------|
| Vanilla DQN | ✗ | ✗ | ε-greedy | ✗ |
| Double DQN | ✓ | ✗ | ε-greedy | ✗ |
| Dueling DQN | - | ✓ | ε-greedy | ✗ |
| Noisy DQN | ✗ | ✗ | ✓ 参数化 | ✗ |
| C51 | ✓ | ✓ | ε-greedy | ✓ |
| PER | - | ✓ | ε-greedy | ✗ |
| Rainbow | ✓ | ✓ | ✓ | ✓ |

### 10.2 计算开销

| 变体 | 额外计算 | 额外存储 |
|------|----------|----------|
| Double DQN | ~0% | 0 |
| Dueling DQN | ~20% | ~50% |
| Noisy DQN | ~50% | 2x |
| C51 | ~100% | N atoms |
| PER | O(log N) per sample | 2N-1 tree nodes |

### 10.3 使用建议

1. **入门/简单任务**: Double DQN
2. **中等难度**: Double + Dueling + PER
3. **困难任务/追求性能**: Rainbow
4. **计算受限**: Double + N-step

---

## 快速复习卡片

### Double DQN
> 用online网络选动作，用target网络评估价值，消除过估计

### Dueling DQN
> Q = V + A - mean(A)，分离状态价值与动作优势

### Noisy Networks
> y = (μ + σ⊙ε)x + b，可学习的参数化探索

### C51
> 建模51个支撑点上的回报分布，KL散度损失

### PER
> P(i) ∝ |δ_i|^α，TD误差大的样本采样更多

### N-step
> G = Σγ^k r + γ^n V(s_n)，权衡偏差方差

### Rainbow
> 六合一：Double + Dueling + Noisy + C51 + PER + N-step

---

## 参考文献

1. Mnih et al. (2015). Human-level control through deep reinforcement learning. *Nature*.
2. van Hasselt et al. (2016). Deep Reinforcement Learning with Double Q-learning. *AAAI*.
3. Wang et al. (2016). Dueling Network Architectures for Deep Reinforcement Learning. *ICML*.
4. Fortunato et al. (2017). Noisy Networks for Exploration. *ICLR*.
5. Bellemare et al. (2017). A Distributional Perspective on Reinforcement Learning. *ICML*.
6. Schaul et al. (2016). Prioritized Experience Replay. *ICLR*.
7. Hessel et al. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. *AAAI*.
