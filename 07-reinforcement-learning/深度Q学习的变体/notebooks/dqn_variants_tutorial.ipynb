{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Variants: 深度Q网络变体详解\n",
    "\n",
    "---\n",
    "\n",
    "## 模块概述\n",
    "\n",
    "本模块系统地介绍Deep Q-Network (DQN)的主要变体算法，包括理论推导、核心创新点、实现细节和对比分析。\n",
    "\n",
    "### 学习目标\n",
    "\n",
    "完成本模块后，您将能够：\n",
    "\n",
    "1. **理解原始DQN的局限性**及其在实际应用中的失效模式\n",
    "2. **掌握各DQN变体的数学原理**：Double DQN、Dueling DQN、Noisy Networks、Categorical DQN等\n",
    "3. **实现各种replay buffer策略**：Uniform、Prioritized、N-step\n",
    "4. **分析Rainbow算法**如何组合所有改进达到SOTA性能\n",
    "5. **进行算法对比实验**并解读结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [环境配置](#1-环境配置)\n",
    "2. [背景知识与DQN局限性](#2-背景知识与DQN局限性)\n",
    "3. [Double DQN: 消除过估计偏差](#3-double-dqn-消除过估计偏差)\n",
    "4. [Dueling DQN: 价值-优势分解](#4-dueling-dqn-价值-优势分解)\n",
    "5. [Noisy Networks: 参数化探索](#5-noisy-networks-参数化探索)\n",
    "6. [Categorical DQN (C51): 分布式强化学习](#6-categorical-dqn-c51-分布式强化学习)\n",
    "7. [Prioritized Experience Replay](#7-prioritized-experience-replay)\n",
    "8. [N-step Learning: 多步学习](#8-n-step-learning-多步学习)\n",
    "9. [Rainbow: 集大成者](#9-rainbow-集大成者)\n",
    "10. [实验对比与分析](#10-实验对比与分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地模块导入\n",
    "from core.config import DQNVariantConfig\n",
    "from core.enums import DQNVariant\n",
    "from buffers import ReplayBuffer, PrioritizedReplayBuffer, NStepReplayBuffer, SumTree\n",
    "from networks import DQNNetwork, DuelingNetwork, NoisyLinear, NoisyNetwork, CategoricalNetwork, RainbowNetwork\n",
    "from agents import DQNVariantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机种子与设备配置\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化配置\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 背景知识与DQN局限性\n",
    "\n",
    "### 2.1 原始DQN回顾\n",
    "\n",
    "Deep Q-Network (Mnih et al., 2015) 是深度强化学习的里程碑式工作。\n",
    "\n",
    "**核心组件**:\n",
    "- 神经网络近似Q函数: $Q(s, a; \\theta)$\n",
    "- 经验回放缓冲区 (Experience Replay)\n",
    "- 目标网络 (Target Network)\n",
    "\n",
    "**TD目标**:\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 原始DQN的四大局限性\n",
    "\n",
    "| 问题 | 原因 | 后果 | 解决方案 |\n",
    "|------|------|------|----------|\n",
    "| **过估计偏差** | max操作同时用于选择和评估 | 不稳定、次优策略 | Double DQN |\n",
    "| **样本效率低** | 均匀随机采样 | 学习慢、数据浪费 | Prioritized Replay |\n",
    "| **探索能力弱** | ε-greedy与状态无关 | 难以逃离局部最优 | Noisy Networks |\n",
    "| **标量值局限** | 只建模期望值 | 丢失分布信息、风险中立 | Categorical DQN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化过估计偏差\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 过估计原理\n",
    "np.random.seed(42)\n",
    "true_q = np.array([1.0, 1.5, 0.8, 1.2])\n",
    "noise = np.random.randn(4, 1000) * 0.5\n",
    "estimated_q = true_q[:, np.newaxis] + noise\n",
    "max_estimated = np.max(estimated_q, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制过估计分布\n",
    "ax1 = axes[0]\n",
    "ax1.hist(max_estimated, bins=30, alpha=0.7, density=True, label='E[max Q]')\n",
    "ax1.axvline(np.max(true_q), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'max Q* = {np.max(true_q)}')\n",
    "ax1.axvline(np.mean(max_estimated), color='blue', linestyle='-', linewidth=2, \n",
    "            label=f'E[max Q] = {np.mean(max_estimated):.2f}')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Overestimation Bias')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索策略对比\n",
    "ax2 = axes[1]\n",
    "steps = np.arange(50000)\n",
    "epsilon_greedy = np.maximum(1.0 - steps / 25000, 0.1)\n",
    "noisy_exploration = 0.5 * np.exp(-steps / 15000) + 0.1 * np.sin(steps / 2500)\n",
    "noisy_exploration = np.maximum(noisy_exploration, 0.05)\n",
    "\n",
    "ax2.plot(steps, epsilon_greedy, 'r-', linewidth=2, label='ε-greedy')\n",
    "ax2.plot(steps, noisy_exploration, 'g-', linewidth=2, alpha=0.8, label='Noisy Networks')\n",
    "ax2.set_xlabel('Training steps')\n",
    "ax2.set_ylabel('Exploration amount')\n",
    "ax2.set_title('Exploration Strategies')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Double DQN: 消除过估计偏差\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "**问题**: 标准DQN的max操作导致系统性过估计：\n",
    "$$\\mathbb{E}[\\max_a Q(s,a)] \\geq \\max_a \\mathbb{E}[Q(s,a)]$$\n",
    "\n",
    "**解决方案**: 解耦动作选择与动作评估\n",
    "\n",
    "$$y^{\\text{Double}} = r + \\gamma Q\\left(s', \\underbrace{\\arg\\max_{a'} Q(s', a'; \\theta)}_{\\text{online选择}}; \\theta^-\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_overestimation(n_actions=10, n_samples=1000, noise_std=1.0):\n",
    "    \"\"\"模拟过估计偏差实验\"\"\"\n",
    "    true_q = np.zeros(n_actions)\n",
    "    \n",
    "    # 标准DQN: 同一噪声源\n",
    "    dqn_estimates = []\n",
    "    for _ in range(n_samples):\n",
    "        noise = np.random.randn(n_actions) * noise_std\n",
    "        estimated_q = true_q + noise\n",
    "        dqn_estimates.append(np.max(estimated_q))\n",
    "    \n",
    "    # Double DQN: 不同噪声源\n",
    "    double_dqn_estimates = []\n",
    "    for _ in range(n_samples):\n",
    "        noise_online = np.random.randn(n_actions) * noise_std\n",
    "        noise_target = np.random.randn(n_actions) * noise_std\n",
    "        estimated_online = true_q + noise_online\n",
    "        estimated_target = true_q + noise_target\n",
    "        best_action = np.argmax(estimated_online)\n",
    "        double_dqn_estimates.append(estimated_target[best_action])\n",
    "    \n",
    "    return np.mean(dqn_estimates), np.mean(double_dqn_estimates), np.max(true_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同动作数量下的过估计\n",
    "action_counts = [2, 4, 8, 16, 32, 64]\n",
    "dqn_bias = []\n",
    "ddqn_bias = []\n",
    "\n",
    "for n_actions in action_counts:\n",
    "    dqn_est, ddqn_est, true_max = simulate_overestimation(n_actions=n_actions)\n",
    "    dqn_bias.append(dqn_est - true_max)\n",
    "    ddqn_bias.append(ddqn_est - true_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制结果\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(action_counts, dqn_bias, 'ro-', linewidth=2, markersize=10, label='DQN (overestimation)')\n",
    "plt.plot(action_counts, ddqn_bias, 'g^-', linewidth=2, markersize=10, label='Double DQN (corrected)')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, label='True value')\n",
    "plt.xlabel('Number of Actions')\n",
    "plt.ylabel('Estimation Bias')\n",
    "plt.title('Overestimation Bias: DQN vs Double DQN')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dueling DQN: 价值-优势分解\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "将Q函数分解为状态价值V(s)和动作优势A(s,a):\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a')$$\n",
    "\n",
    "- **V(s)**: 这个状态有多好？（与动作无关）\n",
    "- **A(s,a)**: 动作a比平均动作好多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示Dueling网络的前向传播\n",
    "state_dim, action_dim, hidden_dim = 4, 2, 64\n",
    "dueling_net = DuelingNetwork(state_dim, action_dim, hidden_dim)\n",
    "print(\"Dueling Network Architecture:\")\n",
    "print(dueling_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播查看中间结果\n",
    "sample_state = torch.randn(1, state_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = F.relu(dueling_net.feature[0](sample_state))\n",
    "    value = dueling_net.value_stream(features)\n",
    "    advantage = dueling_net.advantage_stream(features)\n",
    "    q_values = dueling_net(sample_state)\n",
    "\n",
    "print(f\"\\nV(s) = {value.item():.4f}\")\n",
    "print(f\"A(s,a) = {advantage.numpy().flatten()}\")\n",
    "print(f\"Q(s,a) = {q_values.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Noisy Networks: 参数化探索\n",
    "\n",
    "### 5.1 核心思想\n",
    "\n",
    "用可学习的参数化噪声替代ε-greedy探索:\n",
    "\n",
    "$$y = (\\mu^w + \\sigma^w \\odot \\varepsilon^w) x + (\\mu^b + \\sigma^b \\odot \\varepsilon^b)$$\n",
    "\n",
    "**优势**:\n",
    "1. 状态依赖探索\n",
    "2. 自动退火（σ随学习减小）\n",
    "3. 端到端学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示NoisyLinear层\n",
    "in_features, out_features = 64, 32\n",
    "noisy_layer = NoisyLinear(in_features, out_features)\n",
    "\n",
    "print(\"NoisyLinear Layer:\")\n",
    "print(f\"  mu_weight shape: {noisy_layer.weight_mu.shape}\")\n",
    "print(f\"  sigma_weight shape: {noisy_layer.weight_sigma.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示噪声采样对输出的影响\n",
    "sample_input = torch.randn(1, in_features)\n",
    "\n",
    "outputs = []\n",
    "for _ in range(100):\n",
    "    noisy_layer.reset_noise()\n",
    "    with torch.no_grad():\n",
    "        output = noisy_layer(sample_input)\n",
    "    outputs.append(output.numpy().flatten())\n",
    "\n",
    "outputs = np.array(outputs)\n",
    "print(f\"\\nOutput variance (due to noise): {outputs.std(axis=0)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Categorical DQN (C51): 分布式强化学习\n",
    "\n",
    "### 6.1 核心思想\n",
    "\n",
    "从建模期望值转向建模完整的回报分布:\n",
    "\n",
    "$$Z(s, a) \\sim \\text{Categorical}(z_1, ..., z_N; p_1, ..., p_N)$$\n",
    "\n",
    "**支撑点**:\n",
    "$$z_i = V_{\\min} + i \\cdot \\Delta z, \\quad \\Delta z = \\frac{V_{\\max} - V_{\\min}}{N - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化C51分布表示\n",
    "n_atoms = 51\n",
    "v_min, v_max = -10, 10\n",
    "support = np.linspace(v_min, v_max, n_atoms)\n",
    "delta_z = (v_max - v_min) / (n_atoms - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟两个动作的回报分布\n",
    "def create_distribution(mean, std, support):\n",
    "    probs = np.exp(-(support - mean)**2 / (2 * std**2))\n",
    "    return probs / probs.sum()\n",
    "\n",
    "dist_action1 = create_distribution(mean=5, std=2, support=support)\n",
    "dist_action2 = create_distribution(mean=5, std=5, support=support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制分布对比\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(support, dist_action1, width=delta_z*0.8, alpha=0.7, label='Action 1 (low var)')\n",
    "plt.bar(support, dist_action2, width=delta_z*0.4, alpha=0.5, label='Action 2 (high var)')\n",
    "plt.xlabel('Return value')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('C51: Return Distributions (same mean, different variance)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Prioritized Experience Replay\n",
    "\n",
    "### 7.1 核心思想\n",
    "\n",
    "根据TD误差大小分配采样优先级:\n",
    "\n",
    "$$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}, \\quad p_i = |\\delta_i| + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示SumTree数据结构\n",
    "tree = SumTree(capacity=8)\n",
    "priorities = [1.0, 3.0, 2.0, 4.0, 1.5, 2.5, 3.5, 0.5]\n",
    "\n",
    "for i, p in enumerate(priorities):\n",
    "    tree.add(p, f\"data_{i}\")\n",
    "\n",
    "print(f\"Total priority: {tree.total_priority}\")\n",
    "print(f\"Expected: {sum(priorities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示PER采样\n",
    "per_buffer = PrioritizedReplayBuffer(capacity=100, alpha=0.6, beta_start=0.4)\n",
    "\n",
    "for _ in range(50):\n",
    "    state = np.random.randn(4).astype(np.float32)\n",
    "    per_buffer.push(state, 0, 1.0, state, False)\n",
    "\n",
    "# 采样并查看权重\n",
    "batch = per_buffer.sample(8)\n",
    "weights = batch[-1]\n",
    "print(f\"IS weights: {weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. N-step Learning: 多步学习\n",
    "\n",
    "### 8.1 核心思想\n",
    "\n",
    "N-step Return在偏差与方差之间提供权衡:\n",
    "\n",
    "$$G_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k R_{t+k+1} + \\gamma^n V(S_{t+n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示N-step return计算\n",
    "n_step_buffer = NStepReplayBuffer(capacity=100, n_steps=3, gamma=0.99)\n",
    "\n",
    "rewards = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "state = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "for i, r in enumerate(rewards):\n",
    "    result = n_step_buffer.push(state, 0, r, state, i == len(rewards) - 1)\n",
    "    if result:\n",
    "        print(f\"Step {i+1}: n_step_return = {result.n_step_return:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Rainbow: 集大成者\n",
    "\n",
    "### 9.1 组合策略\n",
    "\n",
    "Rainbow = Double + Dueling + Noisy + Categorical + PER + N-step\n",
    "\n",
    "| 算法 | Atari中位数得分 | 相对DQN提升 |\n",
    "|------|-----------------|-------------|\n",
    "| DQN | 79% | baseline |\n",
    "| Double DQN | 117% | +48% |\n",
    "| Dueling DQN | 151% | +91% |\n",
    "| Categorical DQN | 235% | +197% |\n",
    "| **Rainbow** | **441%** | **+458%** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示Rainbow网络\n",
    "rainbow_net = RainbowNetwork(\n",
    "    state_dim=4, action_dim=2, hidden_dim=64,\n",
    "    num_atoms=51, v_min=-10, v_max=10\n",
    ")\n",
    "print(\"Rainbow Network:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in rainbow_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试前向传播\n",
    "sample_state = torch.randn(1, 4)\n",
    "rainbow_net.reset_noise()\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_probs = rainbow_net(sample_state)\n",
    "    q_values = rainbow_net.get_q_values(sample_state)\n",
    "\n",
    "print(f\"Log probs shape: {log_probs.shape}\")\n",
    "print(f\"Q-values: {q_values.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 实验对比与分析\n",
    "\n",
    "使用DQNVariantAgent对比不同变体的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建配置 (使用小参数快速演示)\n",
    "config = DQNVariantConfig(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    hidden_dim=64,\n",
    "    batch_size=32,\n",
    "    buffer_size=1000,\n",
    "    min_buffer_size=100,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试各变体初始化\n",
    "variants = [\n",
    "    DQNVariant.VANILLA,\n",
    "    DQNVariant.DOUBLE,\n",
    "    DQNVariant.DUELING,\n",
    "    DQNVariant.RAINBOW,\n",
    "]\n",
    "\n",
    "for variant in variants:\n",
    "    agent = DQNVariantAgent(config, variant)\n",
    "    print(f\"{variant}: initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速训练测试\n",
    "agent = DQNVariantAgent(config, DQNVariant.DOUBLE)\n",
    "mock_state = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "# 模拟训练步骤\n",
    "losses = []\n",
    "for i in range(200):\n",
    "    action = agent.select_action(mock_state, training=True)\n",
    "    loss = agent.train_step(mock_state, action, 1.0, mock_state, i % 50 == 49)\n",
    "    if loss is not None:\n",
    "        losses.append(loss)\n",
    "\n",
    "print(f\"Training steps: {len(losses)}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\" if losses else \"No loss yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "| 变体 | 核心创新 | 解决的问题 |\n",
    "|------|----------|------------|\n",
    "| **Double DQN** | 解耦动作选择与评估 | 过估计偏差 |\n",
    "| **Dueling DQN** | V/A分解架构 | 泛化能力 |\n",
    "| **Noisy Networks** | 参数化噪声 | 状态无关探索 |\n",
    "| **Categorical DQN** | 分布建模 | 标量值局限 |\n",
    "| **PER** | TD误差优先级 | 样本效率 |\n",
    "| **N-step** | 多步bootstrap | 信用分配 |\n",
    "| **Rainbow** | 全部组合 | 最优性能 |\n",
    "\n",
    "### 实践建议\n",
    "\n",
    "1. **简单任务**: Vanilla DQN或Double DQN\n",
    "2. **中等难度**: Double + Dueling + PER\n",
    "3. **困难任务**: 使用Rainbow获得最佳性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Mnih, V. et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "2. van Hasselt, H. et al. (2016). Deep Reinforcement Learning with Double Q-learning. *AAAI*.\n",
    "3. Wang, Z. et al. (2016). Dueling Network Architectures for Deep Reinforcement Learning. *ICML*.\n",
    "4. Fortunato, M. et al. (2017). Noisy Networks for Exploration. *ICLR*.\n",
    "5. Bellemare, M. et al. (2017). A Distributional Perspective on Reinforcement Learning. *ICML*.\n",
    "6. Schaul, T. et al. (2016). Prioritized Experience Replay. *ICLR*.\n",
    "7. Hessel, M. et al. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. *AAAI*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
