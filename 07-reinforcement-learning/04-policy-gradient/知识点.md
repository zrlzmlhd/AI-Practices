# 策略梯度方法 - 知识点总结

> 本文档整理策略梯度算法的核心知识点，涵盖理论基础、算法实现、工程实践三个层面。

---

## 目录

1. [核心概念](#1-核心概念)
2. [策略梯度定理](#2-策略梯度定理)
3. [REINFORCE算法](#3-reinforce算法)
4. [方差缩减技术](#4-方差缩减技术)
5. [优势函数与GAE](#5-优势函数与gae)
6. [Actor-Critic架构](#6-actor-critic架构)
7. [连续动作空间](#7-连续动作空间)
8. [工程实践要点](#8-工程实践要点)
9. [算法对比总结](#9-算法对比总结)
10. [常见问题与解决方案](#10-常见问题与解决方案)

---

## 1. 核心概念

### 1.1 值方法 vs 策略方法

| 维度 | 值方法 (Value-Based) | 策略方法 (Policy-Based) |
|------|---------------------|------------------------|
| **学习目标** | 动作价值函数 $Q(s,a)$ | 策略函数 $\pi_\theta(a\|s)$ |
| **策略类型** | 隐式（argmax派生） | 显式参数化 |
| **动作空间** | 主要用于离散 | 离散/连续均适用 |
| **探索机制** | $\epsilon$-greedy | 随机策略自带探索 |
| **收敛性** | 可能振荡 | 更平滑的收敛 |
| **代表算法** | DQN, Double DQN | REINFORCE, A2C, PPO |

### 1.2 策略参数化

**离散动作空间 - Softmax策略：**
$$\pi_\theta(a|s) = \frac{\exp(h(s, a; \theta))}{\sum_{a'} \exp(h(s, a'; \theta))}$$

**连续动作空间 - 高斯策略：**
$$\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)$$

### 1.3 关键术语

- **轨迹 (Trajectory)**: $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$
- **回报 (Return)**: $G_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k}$
- **优势 (Advantage)**: $A(s,a) = Q(s,a) - V(s)$
- **熵 (Entropy)**: $H(\pi) = -\sum_a \pi(a|s) \log \pi(a|s)$

---

## 2. 策略梯度定理

### 2.1 目标函数

最大化期望累积回报：
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \gamma^t r_t\right]$$

### 2.2 策略梯度定理 (Sutton et al., 1999)

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi}(s, a)\right]$$

**证明思路：**
1. 对策略轨迹概率求导
2. 利用似然比技巧 (Likelihood Ratio Trick): $\nabla_\theta \pi_\theta = \pi_\theta \nabla_\theta \log \pi_\theta$
3. 消除状态分布梯度依赖

### 2.3 直观理解

- $\nabla_\theta \log \pi_\theta(a|s)$：增加动作概率的方向
- $Q^{\pi}(s, a)$：动作的好坏程度
- **核心思想**：好的动作增加概率，差的动作减少概率

### 2.4 为什么用 log π 而不是 π？

1. **数学原因**：似然比技巧简化梯度计算
2. **数值稳定性**：避免概率连乘导致的数值问题
3. **梯度特性**：$\nabla \log \pi = \nabla \pi / \pi$，自动归一化

---

## 3. REINFORCE算法

### 3.1 算法原理

使用蒙特卡洛回报 $G_t$ 估计 $Q(s,a)$：

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}$$

### 3.2 算法流程

```
Algorithm: REINFORCE
────────────────────────────────────────
1. 初始化策略网络参数 θ
2. for each episode:
   a. 采样轨迹 τ = (s₀,a₀,r₀,...,sₜ)
   b. 计算每步回报 Gₜ = Σₖ γᵏ rₜ₊ₖ
   c. 计算策略梯度 g = Σₜ ∇log π(aₜ|sₜ) · Gₜ
   d. 更新参数 θ ← θ + α · g
```

### 3.3 回报计算实现

```python
def compute_returns(rewards: List[float], gamma: float) -> torch.Tensor:
    """
    从后向前计算折扣回报，时间复杂度 O(T)。

    G_T = r_T
    G_t = r_t + γ · G_{t+1}
    """
    returns = []
    G = 0.0
    for reward in reversed(rewards):
        G = reward + gamma * G
        returns.insert(0, G)
    return torch.tensor(returns, dtype=torch.float32)
```

### 3.4 特性分析

| 特性 | 描述 |
|------|------|
| **无偏性** | 使用真实回报，梯度估计无偏 |
| **高方差** | 回报受整条轨迹随机性影响 |
| **样本效率** | 低（需要完整episode） |
| **更新频率** | Episode结束后更新 |

---

## 4. 方差缩减技术

### 4.1 高方差问题

REINFORCE的梯度方差来源：
1. 策略随机性
2. 环境随机性
3. 回报的累积噪声

### 4.2 基线方法 (Baseline)

引入状态相关基线 $b(s)$：
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot (Q(s,a) - b(s))\right]$$

**关键定理**：任意状态相关基线不改变梯度期望：
$$\mathbb{E}_{a \sim \pi}[\nabla_\theta \log \pi_\theta(a|s) \cdot b(s)] = 0$$

**证明**：
$$\mathbb{E}_{a}[\nabla_\theta \log \pi \cdot b(s)] = b(s) \sum_a \pi \cdot \frac{\nabla_\theta \pi}{\pi} = b(s) \nabla_\theta \sum_a \pi = b(s) \nabla_\theta 1 = 0$$

### 4.3 最优基线

理论最优基线（最小化方差）：
$$b^*(s) = \frac{\mathbb{E}[(\nabla_\theta \log \pi)^2 \cdot Q]}{\mathbb{E}[(\nabla_\theta \log \pi)^2]}$$

**实践中**：使用状态价值函数 $V(s)$ 作为基线，得到优势函数。

### 4.4 回报标准化

简单有效的方差缩减：
```python
returns = (returns - returns.mean()) / (returns.std() + 1e-8)
```

### 4.5 方差缩减效果对比

```
无基线:     方差 ≈ 10000
带基线:     方差 ≈ 100
带标准化:   方差 ≈ 1
```

---

## 5. 优势函数与GAE

### 5.1 优势函数定义

$$A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$$

**直观含义**：
- $A > 0$：动作优于该状态下的平均水平
- $A < 0$：动作劣于平均水平
- $A = 0$：动作表现等于平均

### 5.2 TD误差

单步时序差分误差：
$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

**性质**：$\delta_t$ 是优势函数的无偏估计
$$\mathbb{E}[\delta_t | s_t, a_t] = A^{\pi}(s_t, a_t)$$

### 5.3 N步回报

$$G_t^{(n)} = r_t + \gamma r_{t+1} + \ldots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$

| N值 | 偏差 | 方差 | 特点 |
|-----|------|------|------|
| N=1 | 高 | 低 | TD(0)，依赖V估计 |
| N=∞ | 0 | 高 | MC，使用真实回报 |
| N=5 | 中 | 中 | 常用折中 |

### 5.4 GAE (Generalized Advantage Estimation)

Schulman et al., 2016 提出的优势估计方法：

$$A_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$

**展开形式**：
$$A_t^{GAE} = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \ldots$$

### 5.5 λ参数的作用

| λ值 | 等价形式 | 偏差 | 方差 | 适用场景 |
|-----|----------|------|------|----------|
| λ=0 | TD(0) | 高 | 低 | V估计准确时 |
| λ=1 | MC | 无 | 高 | 短episode |
| λ=0.95 | 加权混合 | 低 | 中 | 通用推荐 |

### 5.6 GAE实现

```python
def compute_gae(rewards, values, next_value, dones, gamma, gae_lambda):
    """
    计算GAE优势估计。

    时间复杂度: O(T)
    空间复杂度: O(T)
    """
    advantages = []
    gae = 0.0
    values = list(values) + [next_value]

    for t in reversed(range(len(rewards))):
        next_val = 0.0 if dones[t] else values[t + 1]
        delta = rewards[t] + gamma * next_val - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae
        advantages.insert(0, gae)

    return torch.tensor(advantages)
```

---

## 6. Actor-Critic架构

### 6.1 架构概述

```
             ┌─────────────────────────────────────┐
             │              State                  │
             └─────────────────────────────────────┘
                             │
                             ▼
             ┌─────────────────────────────────────┐
             │        Shared Feature Net           │
             │        (可选共享层)                   │
             └─────────────────────────────────────┘
                    │                    │
                    ▼                    ▼
        ┌──────────────────┐   ┌──────────────────┐
        │    Actor Head    │   │   Critic Head    │
        │    π(a|s; θ)     │   │    V(s; φ)       │
        └──────────────────┘   └──────────────────┘
                │                       │
                ▼                       ▼
          Policy Loss            Value Loss
         -log π · A              (V - G)²
```

### 6.2 A2C算法

**Advantage Actor-Critic** 同步版本：

```
Algorithm: A2C
────────────────────────────────────────
1. 初始化 Actor θ, Critic φ
2. for each update:
   a. 采集n步经验
   b. 计算GAE优势 Aₜ
   c. 计算目标回报 Gₜ = Aₜ + V(sₜ)
   d. Actor损失: L_π = -E[log π(a|s) · A]
   e. Critic损失: L_V = E[(V(s) - G)²]
   f. 熵奖励: H = E[-log π]
   g. 总损失: L = L_π + c_v·L_V - c_ent·H
   h. 梯度更新 θ, φ
```

### 6.3 损失函数详解

**总损失函数**：
$$\mathcal{L} = \underbrace{-\mathbb{E}[\log \pi(a|s) \cdot A]}_{\text{策略损失}} + \underbrace{c_v \mathbb{E}[(V(s) - G)^2]}_{\text{价值损失}} - \underbrace{c_{ent} H(\pi)}_{\text{熵奖励}}$$

| 组件 | 作用 | 典型系数 |
|------|------|----------|
| 策略损失 | 最大化优势加权对数概率 | 1.0 |
| 价值损失 | 准确估计状态价值 | 0.5 |
| 熵奖励 | 鼓励探索，防止早熟收敛 | 0.01 |

### 6.4 共享网络 vs 分离网络

**共享网络优势**：
- 参数量少
- 特征复用
- 训练更稳定

**分离网络优势**：
- 避免梯度干扰
- 独立学习率
- 更灵活

**实践建议**：简单任务用共享网络，复杂任务考虑分离。

---

## 7. 连续动作空间

### 7.1 高斯策略

$$\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)$$

**网络输出**：
- 均值 $\mu$：线性层直接输出
- 标准差 $\sigma$：可学习参数或网络输出（需正数约束）

### 7.2 重参数化技巧 (Reparameterization Trick)

$$a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

**优势**：
- 允许梯度通过采样操作传播
- 降低方差
- 适用于确定性策略梯度

### 7.3 Tanh压缩

将无界高斯动作映射到 $[-1, 1]$：
$$a = \tanh(u), \quad u \sim \mathcal{N}(\mu, \sigma^2)$$

**对数概率修正（雅可比行列式）**：
$$\log \pi(a|s) = \log \mathcal{N}(u|\mu,\sigma^2) - \sum_i \log(1 - \tanh^2(u_i))$$

### 7.4 实现细节

```python
class ContinuousPolicy(nn.Module):
    LOG_STD_MIN = -20.0  # 防止std过小
    LOG_STD_MAX = 2.0    # 防止std过大

    def sample(self, state):
        mean, std = self.forward(state)
        dist = Normal(mean, std)

        # 重参数化采样
        u = dist.rsample()
        action = torch.tanh(u)

        # 修正对数概率
        log_prob = dist.log_prob(u).sum(-1)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6).sum(-1)

        return action, log_prob
```

---

## 8. 工程实践要点

### 8.1 网络初始化

**正交初始化 (Orthogonal Initialization)**：

```python
def init_weights(module, gain=np.sqrt(2)):
    if isinstance(module, nn.Linear):
        nn.init.orthogonal_(module.weight, gain=gain)
        nn.init.zeros_(module.bias)
```

**输出层特殊处理**：
- Actor最后一层：gain=0.01（小初始化，初始策略接近均匀）
- Critic最后一层：gain=1.0

### 8.2 梯度裁剪

```python
nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
```

**作用**：防止梯度爆炸，稳定训练。

### 8.3 优势标准化

```python
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
```

**注意**：优势标准化应在detach之后进行。

### 8.4 超参数推荐

| 参数 | 典型范围 | 说明 |
|------|----------|------|
| 学习率(Actor) | 1e-4 ~ 3e-4 | 策略应平滑变化 |
| 学习率(Critic) | 1e-3 ~ 3e-3 | 可比Actor大 |
| γ (折扣因子) | 0.99 | 短任务可用0.95 |
| λ (GAE) | 0.95 | 平衡偏差-方差 |
| 熵系数 | 0.01 | 太大导致随机策略 |
| 梯度裁剪 | 0.5 ~ 1.0 | 防止不稳定 |
| 隐藏层 | 64/128/256 | 任务复杂度决定 |

### 8.5 常见技巧

1. **学习率调度**：余弦退火或线性衰减
2. **奖励裁剪**：限制在[-10, 10]
3. **观测标准化**：Running mean/std
4. **多环境并行**：提高样本效率
5. **早停**：基于验证性能

---

## 9. 算法对比总结

### 9.1 算法演进

```
REINFORCE (1992)
    │
    ├─ 加入基线 ──────────────▶ REINFORCE + Baseline
    │
    ├─ TD替代MC ──────────────▶ Actor-Critic
    │
    ├─ 优势函数 + GAE ────────▶ A2C (2016)
    │
    └─ 加入约束 ──────────────▶ PPO / TRPO
```

### 9.2 详细对比

| 特性 | REINFORCE | +Baseline | A2C |
|------|-----------|-----------|-----|
| 优势估计 | $G_t$ | $G_t - V(s)$ | GAE |
| 更新时机 | Episode结束 | Episode结束 | N步 |
| 方差 | 高 | 中 | 低 |
| 偏差 | 无 | 无 | 有（可控） |
| 样本效率 | 低 | 中 | 高 |
| 实现复杂度 | 简单 | 中等 | 中等 |
| 适用场景 | 教学/简单任务 | 中等任务 | 生产环境 |

### 9.3 选择建议

- **学习/原型验证**：REINFORCE
- **中等复杂任务**：REINFORCE + Baseline
- **生产部署**：A2C 或 PPO
- **连续控制**：A2C/PPO + GAE

---

## 10. 常见问题与解决方案

### 10.1 训练不收敛

**症状**：奖励不增长或剧烈波动

**排查步骤**：
1. 检查奖励尺度是否合理
2. 降低学习率
3. 增大熵系数
4. 检查网络初始化
5. 检查梯度是否爆炸/消失

### 10.2 策略崩溃

**症状**：策略变得确定性，熵降为0

**解决方案**：
1. 增大熵系数
2. 减小学习率
3. 使用PPO的clip机制
4. 检查优势计算是否正确

### 10.3 价值估计不准

**症状**：Value loss居高不下

**解决方案**：
1. 增大Critic学习率
2. 增加Critic网络容量
3. 检查回报计算
4. 使用独立的Critic网络

### 10.4 探索不足

**症状**：陷入局部最优

**解决方案**：
1. 增大熵系数
2. 使用更大的初始标准差（连续动作）
3. 添加探索噪声
4. 使用curiosity-driven探索

### 10.5 数值不稳定

**症状**：出现NaN或Inf

**解决方案**：
1. 添加数值稳定性epsilon
2. 使用log_softmax代替softmax+log
3. 裁剪log_std范围
4. 检查除零情况

---

## 附录A：数学符号表

| 符号 | 含义 |
|------|------|
| $s, a, r$ | 状态、动作、奖励 |
| $\pi_\theta(a\|s)$ | 参数化策略 |
| $V^\pi(s)$ | 状态价值函数 |
| $Q^\pi(s,a)$ | 动作价值函数 |
| $A^\pi(s,a)$ | 优势函数 |
| $\gamma$ | 折扣因子 |
| $\lambda$ | GAE参数 |
| $G_t$ | 从t时刻的回报 |
| $\delta_t$ | TD误差 |
| $H(\pi)$ | 策略熵 |
| $\theta$ | 策略网络参数 |
| $\phi$ | 价值网络参数 |

---

## 附录B：模块架构

本项目采用模块化设计：

```
04-policy-gradient/
├── core/              # 核心数据结构
│   ├── config.py      # 训练配置
│   └── buffers.py     # 经验缓冲区
├── networks/          # 神经网络
│   ├── base.py        # 基础网络组件
│   ├── policy.py      # 策略网络
│   └── value.py       # 价值网络
├── algorithms/        # 算法实现
│   ├── base.py        # 算法基类
│   ├── reinforce.py   # REINFORCE
│   ├── reinforce_baseline.py
│   └── a2c.py         # A2C
├── utils/             # 工具函数
│   ├── returns.py     # 回报计算
│   ├── training.py    # 训练循环
│   └── visualization.py
└── tests/             # 单元测试
```

---

## 附录C：参考文献

1. Williams, R.J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine Learning*, 8(3-4), 229-256.

2. Sutton, R.S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. *NeurIPS*.

3. Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. *ICLR*.

4. Mnih, V., Badia, A.P., et al. (2016). Asynchronous methods for deep reinforcement learning. *ICML*.

5. Sutton, R.S., & Barto, A.G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.

---

*最后更新：2025年12月*
