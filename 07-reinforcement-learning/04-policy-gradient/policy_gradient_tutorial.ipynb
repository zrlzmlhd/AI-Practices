{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods Tutorial\n",
    "\n",
    "## From REINFORCE to Advantage Actor-Critic\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial provides a comprehensive introduction to policy gradient methods in reinforcement learning, covering:\n",
    "\n",
    "1. **Policy Gradient Theorem** - The theoretical foundation\n",
    "2. **REINFORCE Algorithm** - Monte Carlo policy gradient\n",
    "3. **Variance Reduction** - Baselines and advantage functions\n",
    "4. **Actor-Critic Methods** - Combining policy and value learning\n",
    "5. **GAE** - Generalized Advantage Estimation\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Williams (1992). Simple statistical gradient-following algorithms\n",
    "- Sutton et al. (1999). Policy gradient methods for RL with function approximation\n",
    "- Schulman et al. (2016). High-dimensional continuous control using GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Device selection\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-header",
   "metadata": {},
   "source": [
    "## 2. Policy Gradient Theorem\n",
    "\n",
    "### 2.1 Value Methods vs Policy Methods\n",
    "\n",
    "| Aspect | Value Methods (DQN) | Policy Methods |\n",
    "|--------|---------------------|----------------|\n",
    "| Learning Target | Q(s,a) → implicit policy | Direct policy π(a\\|s) |\n",
    "| Action Space | Primarily discrete | Natural for continuous |\n",
    "| Policy Type | Deterministic (argmax) | Stochastic (distribution) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective",
   "metadata": {},
   "source": [
    "### 2.2 Objective Function\n",
    "\n",
    "Maximize expected cumulative return:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n",
    "\n",
    "### 2.3 Policy Gradient Theorem (Sutton et al., 1999)\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi}(s, a)\\right]$$\n",
    "\n",
    "**Intuition:**\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$: Direction to increase action probability\n",
    "- $Q^{\\pi}(s, a)$: How good the action is\n",
    "- Good actions → increase probability, bad actions → decrease probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intuition-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy_gradient_intuition():\n",
    "    \"\"\"Visualize the intuition behind policy gradient updates.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    actions = ['Left', 'Right', 'Jump']\n",
    "    initial_probs = [0.33, 0.33, 0.34]\n",
    "    rewards = [0.1, 0.9, 0.0]\n",
    "    updated_probs = [0.15, 0.70, 0.15]\n",
    "    \n",
    "    # Initial policy\n",
    "    axes[0].bar(actions, initial_probs, color='steelblue', alpha=0.7)\n",
    "    axes[0].set_ylim(0, 0.8)\n",
    "    axes[0].set_title('Initial Policy π(a|s)')\n",
    "    axes[0].set_ylabel('Probability')\n",
    "    \n",
    "    # Action returns\n",
    "    axes[1].bar(actions, rewards, color='green', alpha=0.7)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_title('Action Returns Q(s,a)')\n",
    "    axes[1].set_ylabel('Return')\n",
    "    \n",
    "    # Updated policy\n",
    "    axes[2].bar(actions, updated_probs, color='orange', alpha=0.7)\n",
    "    axes[2].set_ylim(0, 0.8)\n",
    "    axes[2].set_title(\"Updated Policy π'(a|s)\")\n",
    "    axes[2].set_ylabel('Probability')\n",
    "    \n",
    "    plt.suptitle('Policy Gradient: Increase probability of high-return actions', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy_gradient_intuition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "networks-header",
   "metadata": {},
   "source": [
    "## 3. Policy Networks\n",
    "\n",
    "### 3.1 Discrete Action Space: Softmax Policy\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\frac{\\exp(h(s, a; \\theta))}{\\sum_{a'} \\exp(h(s, a'; \\theta))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for discrete action spaces.\n",
    "    \n",
    "    Uses Softmax to output a probability distribution over actions.\n",
    "    Network outputs logits for numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Apply orthogonal initialization for stable training.\"\"\"\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Output action logits.\"\"\"\n",
    "        return self.net(state)\n",
    "    \n",
    "    def get_distribution(self, state: torch.Tensor) -> Categorical:\n",
    "        \"\"\"Get categorical distribution over actions.\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample action, returning (action, log_prob, entropy).\"\"\"\n",
    "        dist = self.get_distribution(state)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-discrete-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test discrete policy\n",
    "policy = DiscretePolicy(state_dim=4, action_dim=2)\n",
    "state = torch.randn(1, 4)\n",
    "\n",
    "action, log_prob, entropy = policy.sample(state)\n",
    "probs = policy.get_distribution(state).probs\n",
    "\n",
    "print(f\"State: {state.squeeze().numpy().round(3)}\")\n",
    "print(f\"Action Probabilities: {probs.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"Sampled Action: {action.item()}\")\n",
    "print(f\"Log π(a|s): {log_prob.item():.4f}\")\n",
    "print(f\"Entropy H(π): {entropy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-header",
   "metadata": {},
   "source": [
    "### 3.2 Continuous Action Space: Gaussian Policy\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)^2)$$\n",
    "\n",
    "For bounded actions, apply **tanh squashing** and correct log-probability:\n",
    "\n",
    "$$a = \\tanh(u), \\quad u \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "$$\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu,\\sigma^2) - \\sum_i \\log(1 - \\tanh^2(u_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian policy for continuous action spaces.\n",
    "    \n",
    "    Outputs mean and log-std of Gaussian distribution.\n",
    "    Uses tanh to bound actions to [-1, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    LOG_STD_MIN = -20.0\n",
    "    LOG_STD_MAX = 2.0\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction network\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Mean output layer\n",
    "        self.mean_layer = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Learnable log standard deviation\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Output (mean, std) of Gaussian distribution.\"\"\"\n",
    "        features = self.feature_net(state)\n",
    "        mean = self.mean_layer(features)\n",
    "        log_std = torch.clamp(self.log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample action using reparameterization trick.\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        # Reparameterization: u = mean + std * epsilon\n",
    "        u = dist.rsample()\n",
    "        action = torch.tanh(u)  # Bound to [-1, 1]\n",
    "        \n",
    "        # Correct log_prob for tanh transformation (Jacobian)\n",
    "        log_prob = dist.log_prob(u).sum(dim=-1)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6).sum(dim=-1)\n",
    "        \n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        return action, log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-continuous-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test continuous policy\n",
    "cont_policy = ContinuousPolicy(state_dim=3, action_dim=2)\n",
    "state = torch.randn(1, 3)\n",
    "\n",
    "action, log_prob, entropy = cont_policy.sample(state)\n",
    "mean, std = cont_policy.forward(state)\n",
    "\n",
    "print(f\"Mean μ: {mean.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"Std σ: {std.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"Sampled Action (after tanh): {action.squeeze().detach().numpy().round(3)}\")\n",
    "print(f\"Action bounds: [{action.min().item():.3f}, {action.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reinforce-header",
   "metadata": {},
   "source": [
    "## 4. REINFORCE Algorithm\n",
    "\n",
    "### 4.1 Monte Carlo Policy Gradient (Williams, 1992)\n",
    "\n",
    "Use complete episode return $G_t$ to estimate $Q(s,a)$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}$$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the discounted return from time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-returns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards: List[float], gamma: float, normalize: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Monte Carlo returns.\n",
    "    \n",
    "    G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...\n",
    "    \n",
    "    Computed efficiently via backward iteration: O(T) time.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards [r_0, r_1, ..., r_{T-1}]\n",
    "        gamma: Discount factor\n",
    "        normalize: Whether to standardize returns\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of returns [G_0, G_1, ..., G_{T-1}]\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    \n",
    "    # Backward iteration\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    if normalize and len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-returns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate return computation\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "gamma = 0.99\n",
    "\n",
    "returns = compute_returns(rewards, gamma, normalize=False)\n",
    "\n",
    "print(\"Return Computation Example\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Discount γ = {gamma}\")\n",
    "print(f\"\\nReturns (backward computation):\")\n",
    "for t, (r, G) in enumerate(zip(rewards, returns)):\n",
    "    print(f\"  t={t}: r_t={r:.1f}, G_t={G.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reinforce-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    REINFORCE algorithm implementation.\n",
    "    \n",
    "    Properties:\n",
    "    - Uses Monte Carlo returns (complete episodes)\n",
    "    - Unbiased gradient estimate\n",
    "    - High variance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 lr: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = DiscretePolicy(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action according to policy.\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob, _ = self.policy.sample(state_t)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"Update policy at episode end.\"\"\"\n",
    "        # Compute returns\n",
    "        returns = compute_returns(self.rewards, self.gamma, normalize=True)\n",
    "        \n",
    "        # Policy gradient loss: -E[log π(a|s) * G]\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "print(\"REINFORCE class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## 5. Variance Reduction with Baselines\n",
    "\n",
    "### 5.1 The High Variance Problem\n",
    "\n",
    "REINFORCE suffers from high variance because returns $G_t$ include many random factors.\n",
    "\n",
    "### 5.2 Introducing a Baseline\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (Q(s,a) - b(s))\\right]$$\n",
    "\n",
    "**Key insight:** Any state-dependent baseline $b(s)$ doesn't change the expected gradient:\n",
    "\n",
    "$$\\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot b(s)] = 0$$\n",
    "\n",
    "### 5.3 Advantage Function\n",
    "\n",
    "Using $V(s)$ as baseline gives us the **advantage function**:\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "\n",
    "- $A > 0$: Action better than average\n",
    "- $A < 0$: Action worse than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_baseline_effect():\n",
    "    \"\"\"Demonstrate how baselines reduce variance.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_samples = 1000\n",
    "    base_return = 100  # High average return\n",
    "    returns = np.random.normal(base_return, 20, n_samples)\n",
    "    \n",
    "    # Without baseline\n",
    "    gradient_no_baseline = returns\n",
    "    \n",
    "    # With baseline (subtract mean)\n",
    "    baseline = returns.mean()\n",
    "    gradient_with_baseline = returns - baseline\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(gradient_no_baseline, bins=50, alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', label='Zero')\n",
    "    axes[0].set_title(f'No Baseline\\nMean={np.mean(gradient_no_baseline):.1f}, Var={np.var(gradient_no_baseline):.1f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].hist(gradient_with_baseline, bins=50, alpha=0.7, color='orange')\n",
    "    axes[1].axvline(x=0, color='red', linestyle='--', label='Zero')\n",
    "    axes[1].set_title(f'With Baseline\\nMean={np.mean(gradient_with_baseline):.1f}, Var={np.var(gradient_with_baseline):.1f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.suptitle('Baseline Reduces Variance While Preserving Expected Gradient', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Variance reduction: {np.var(gradient_no_baseline) / np.var(gradient_with_baseline):.2f}x\")\n",
    "\n",
    "visualize_baseline_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "value-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"State value function V(s) for use as baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reinforce-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaseline:\n",
    "    \"\"\"\n",
    "    REINFORCE with learned value baseline.\n",
    "    \n",
    "    Uses advantage A = G - V(s) instead of raw returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr_policy: float = 1e-3, lr_value: float = 1e-3,\n",
    "                 gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = DiscretePolicy(state_dim, action_dim)\n",
    "        self.value_net = ValueNetwork(state_dim)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        action, log_prob, _ = self.policy.sample(state_t)\n",
    "        value = self.value_net(state_t)\n",
    "        \n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> Tuple[float, float]:\n",
    "        # Compute returns (not normalized, for value target)\n",
    "        returns = compute_returns(self.rewards, self.gamma, normalize=False)\n",
    "        \n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.cat(self.values).squeeze()\n",
    "        \n",
    "        # Compute advantages: A = G - V\n",
    "        # IMPORTANT: detach values to prevent gradient flow through baseline\n",
    "        advantages = returns - values.detach()\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Policy loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Value loss (MSE)\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Update policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Update value network\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "\n",
    "print(\"REINFORCEBaseline class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gae-header",
   "metadata": {},
   "source": [
    "## 6. Generalized Advantage Estimation (GAE)\n",
    "\n",
    "### 6.1 TD Error\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### 6.2 GAE Formula (Schulman et al., 2016)\n",
    "\n",
    "$$A_t^{GAE}(\\gamma,\\lambda) = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "### 6.3 λ Parameter Controls Bias-Variance Trade-off\n",
    "\n",
    "| λ Value | Effect |\n",
    "|---------|--------|\n",
    "| λ=0 | TD(0), $A_t = \\delta_t$, high bias, low variance |\n",
    "| λ=1 | Monte Carlo, $A_t = G_t - V(s_t)$, low bias, high variance |\n",
    "| λ=0.95 | Good balance for most tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-gae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards: List[float],\n",
    "    values: List[float],\n",
    "    next_value: float,\n",
    "    dones: List[bool],\n",
    "    gamma: float,\n",
    "    gae_lambda: float\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "    A_t^GAE = Σ (γλ)^l δ_{t+l}\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards\n",
    "        values: List of value estimates\n",
    "        next_value: Bootstrap value for final state\n",
    "        dones: Episode termination flags\n",
    "        gamma: Discount factor\n",
    "        gae_lambda: GAE λ parameter\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantage estimates\n",
    "        returns: Value targets (A + V)\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0.0\n",
    "    \n",
    "    values = list(values) + [next_value]\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        # If done, next value is 0\n",
    "        next_val = 0.0 if dones[t] else values[t + 1]\n",
    "        \n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * next_val - values[t]\n",
    "        \n",
    "        # GAE accumulation (reset on done)\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-gae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate GAE\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "values = [0.8, 0.9, 1.0, 0.95, 0.85]\n",
    "dones = [False, False, False, False, True]\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "\n",
    "advantages, returns = compute_gae(rewards, values, 0.0, dones, gamma, gae_lambda)\n",
    "\n",
    "print(\"GAE Computation Example\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Rewards: {rewards}\")\n",
    "print(f\"Values: {values}\")\n",
    "print(f\"\\nAdvantages (λ={gae_lambda}): {advantages.numpy().round(3)}\")\n",
    "print(f\"Returns: {returns.numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gae_lambda_effect():\n",
    "    \"\"\"Visualize how λ affects advantage variance.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    T = 50\n",
    "    rewards = np.random.normal(1.0, 0.5, T)\n",
    "    true_values = np.cumsum(rewards[::-1])[::-1] * 0.99 ** np.arange(T)\n",
    "    noisy_values = true_values + np.random.normal(0, 0.3, T)\n",
    "    dones = [False] * (T-1) + [True]\n",
    "    gamma = 0.99\n",
    "    \n",
    "    lambdas = [0.0, 0.5, 0.9, 1.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    for ax, lam in zip(axes.flat, lambdas):\n",
    "        advantages, _ = compute_gae(\n",
    "            rewards.tolist(), noisy_values.tolist(),\n",
    "            0.0, dones, gamma, lam\n",
    "        )\n",
    "        \n",
    "        ax.plot(advantages.numpy(), linewidth=2)\n",
    "        ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Advantage')\n",
    "        ax.set_title(f'λ={lam}: Variance={advantages.std().item():.3f}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('GAE: Effect of λ on Advantage Variance', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gae_lambda_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c-header",
   "metadata": {},
   "source": [
    "## 7. Advantage Actor-Critic (A2C)\n",
    "\n",
    "### 7.1 Architecture\n",
    "\n",
    "```\n",
    "state → [shared_net] → features\n",
    "                         ├→ [actor_head] → policy π(a|s)\n",
    "                         └→ [critic_head] → value V(s)\n",
    "```\n",
    "\n",
    "### 7.2 Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{-\\log \\pi(a|s) \\cdot A}_{\\text{Actor}} + \\underbrace{c_v (V(s) - G)^2}_{\\text{Critic}} - \\underbrace{c_{ent} H(\\pi)}_{\\text{Entropy}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actor-critic-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared-feature Actor-Critic network.\n",
    "    \n",
    "    Actor and Critic share feature extraction layers,\n",
    "    then branch into separate heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic head (value)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        features = self.shared(state)\n",
    "        logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return Categorical(logits=logits), value\n",
    "    \n",
    "    def get_action_and_value(self, state: torch.Tensor):\n",
    "        \"\"\"Get action, log_prob, entropy, and value in one pass.\"\"\"\n",
    "        dist, value = self.forward(state)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy(), value.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic with GAE.\n",
    "    \n",
    "    Features:\n",
    "    - Shared actor-critic network\n",
    "    - GAE for advantage estimation\n",
    "    - Entropy regularization for exploration\n",
    "    - Gradient clipping for stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.model = ActorCriticNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob, entropy, value = self.model.get_action_and_value(state_t)\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.item())\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value.item())\n",
    "        self.entropies.append(entropy)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store(self, reward: float, done: bool):\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def update(self, next_state: np.ndarray, done: bool) -> Dict[str, float]:\n",
    "        # Get bootstrap value\n",
    "        if done:\n",
    "            next_value = 0.0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_state_t = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                _, next_value = self.model(next_state_t)\n",
    "                next_value = next_value.item()\n",
    "        \n",
    "        # Compute GAE\n",
    "        advantages, returns = compute_gae(\n",
    "            self.rewards, self.values, next_value,\n",
    "            self.dones, self.gamma, self.gae_lambda\n",
    "        )\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.tensor(self.values, dtype=torch.float32)\n",
    "        entropies = torch.stack(self.entropies)\n",
    "        \n",
    "        # Compute losses\n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        entropy_bonus = entropies.mean()\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (\n",
    "            policy_loss\n",
    "            + self.value_coef * value_loss\n",
    "            - self.entropy_coef * entropy_bonus\n",
    "        )\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy_bonus.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "print(\"A2C class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 8. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gymnasium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(\"Gymnasium imported successfully\")\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"Warning: gymnasium not installed\")\n",
    "    print(\"Install with: pip install gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env_name: str, num_episodes: int = 300, log_interval: int = 50):\n",
    "    \"\"\"Train an agent on a gymnasium environment.\"\"\"\n",
    "    if not HAS_GYM:\n",
    "        print(\"Gymnasium required for training\")\n",
    "        return []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rewards_history = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {agent.__class__.__name__} on {env_name}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if hasattr(agent, 'store'):\n",
    "                agent.store(reward, done)\n",
    "            else:\n",
    "                agent.store_reward(reward)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update\n",
    "        if isinstance(agent, A2C):\n",
    "            agent.update(next_state, done)\n",
    "        else:\n",
    "            agent.update()\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare algorithms\n",
    "if HAS_GYM:\n",
    "    env_name = \"CartPole-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    env.close()\n",
    "    \n",
    "    num_episodes = 300\n",
    "    results = {}\n",
    "    \n",
    "    # Train REINFORCE\n",
    "    print(\"[1/3] Training REINFORCE...\")\n",
    "    agent_rf = REINFORCE(state_dim, action_dim, lr=1e-3, gamma=0.99)\n",
    "    results['REINFORCE'] = train_agent(agent_rf, env_name, num_episodes)\n",
    "    \n",
    "    # Train REINFORCE + Baseline\n",
    "    print(\"\\n[2/3] Training REINFORCE + Baseline...\")\n",
    "    agent_rfb = REINFORCEBaseline(state_dim, action_dim, lr_policy=1e-3, lr_value=1e-3)\n",
    "    results['REINFORCE+Baseline'] = train_agent(agent_rfb, env_name, num_episodes)\n",
    "    \n",
    "    # Train A2C\n",
    "    print(\"\\n[3/3] Training A2C...\")\n",
    "    agent_a2c = A2C(state_dim, action_dim, lr=3e-4, gamma=0.99, gae_lambda=0.95)\n",
    "    results['A2C (GAE)'] = train_agent(agent_a2c, env_name, num_episodes)\n",
    "else:\n",
    "    print(\"Skipping training (gymnasium not installed)\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(results: Dict[str, List[float]], window: int = 50):\n",
    "    \"\"\"Plot learning curves with smoothing.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No training data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    colors = ['steelblue', 'orange', 'green']\n",
    "    \n",
    "    for (name, rewards), color in zip(results.items(), colors):\n",
    "        # Raw data (transparent)\n",
    "        ax.plot(rewards, alpha=0.2, color=color)\n",
    "        \n",
    "        # Smoothed curve\n",
    "        if len(rewards) > window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax.plot(range(window-1, len(rewards)), smoothed,\n",
    "                   label=name, color=color, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('Policy Gradient Methods Comparison')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "| Algorithm | Advantage Estimate | Update | Variance | Bias | Best For |\n",
    "|-----------|-------------------|--------|----------|------|----------|\n",
    "| REINFORCE | $G_t$ (MC) | Episode | High | None | Simple tasks |\n",
    "| +Baseline | $G_t - V(s)$ | Episode | Medium | None | Medium tasks |\n",
    "| A2C | GAE($\\delta_t$) | n-step | Low | Some | Complex tasks |\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "| Parameter | Typical Range | Notes |\n",
    "|-----------|---------------|-------|\n",
    "| Learning Rate (Actor) | 1e-4 to 3e-4 | Policy should change smoothly |\n",
    "| Learning Rate (Critic) | 1e-3 to 3e-3 | Can be larger than actor |\n",
    "| γ (Discount) | 0.99 | Use 0.95 for shorter tasks |\n",
    "| λ (GAE) | 0.95 | Balance bias-variance |\n",
    "| Entropy Coefficient | 0.01 | Too high = random policy |\n",
    "| Gradient Clipping | 0.5 to 1.0 | Prevents instability |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-header",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Understanding Policy Gradients\n",
    "Why does the policy gradient formula use $\\log \\pi$ instead of $\\pi$? Explain from both mathematical and intuitive perspectives.\n",
    "\n",
    "### Exercise 2: Implement Improvements\n",
    "Add the following improvements to REINFORCE and observe effects:\n",
    "1. Causality: Only use future rewards (not the entire episode)\n",
    "2. Reward normalization\n",
    "\n",
    "### Exercise 3: Continuous Control\n",
    "Use `ContinuousPolicy` to train A2C on `Pendulum-v1`.\n",
    "\n",
    "### Exercise 4: Hyperparameter Tuning\n",
    "Experiment with different GAE λ values (0.9, 0.95, 0.99, 1.0) and compare learning curve stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise code space\n",
    "# Write your solutions here\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Sutton & Barto, \"Reinforcement Learning: An Introduction\", Chapter 13\n",
    "2. Williams (1992). Simple statistical gradient-following algorithms\n",
    "3. Schulman et al. (2016). High-dimensional continuous control using GAE\n",
    "4. Mnih et al. (2016). Asynchronous methods for deep RL (A3C)\n",
    "5. OpenAI Spinning Up: https://spinningup.openai.com/\n",
    "\n",
    "---\n",
    "\n",
    "[Back to Index](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
