{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) 深度解析\n",
    "\n",
    "本 Notebook 深入剖析 DQN 算法的核心组件和数学原理，通过交互式实验帮助理解算法细节。\n",
    "\n",
    "## 目录\n",
    "1. [DQN 算法概述](#1-dqn-算法概述)\n",
    "2. [Q 函数与贝尔曼方程](#2-q-函数与贝尔曼方程)\n",
    "3. [经验回放机制](#3-经验回放机制)\n",
    "4. [目标网络](#4-目标网络)\n",
    "5. [ε-贪婪探索](#5-ε-贪婪探索)\n",
    "6. [DQN 变体对比](#6-dqn-变体对比)\n",
    "7. [实验与分析](#7-实验与分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TF-Agents\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.utils import common\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN 算法概述\n",
    "\n",
    "### 1.1 核心思想\n",
    "\n",
    "DQN (Deep Q-Network) 使用深度神经网络逼近 Q 函数，解决了传统 Q-Learning 无法处理高维状态空间的问题。\n",
    "\n",
    "**关键创新**:\n",
    "1. **经验回放 (Experience Replay)**: 打破样本相关性\n",
    "2. **目标网络 (Target Network)**: 稳定训练目标\n",
    "\n",
    "### 1.2 算法流程\n",
    "\n",
    "```\n",
    "初始化 Q 网络 θ 和目标网络 θ⁻\n",
    "初始化经验回放缓冲区 D\n",
    "\n",
    "for episode = 1 to M:\n",
    "    初始化状态 s₁\n",
    "    for t = 1 to T:\n",
    "        # ε-贪婪动作选择\n",
    "        以概率 ε 选择随机动作 aₜ\n",
    "        否则选择 aₜ = argmax_a Q(sₜ, a; θ)\n",
    "        \n",
    "        执行动作 aₜ，观测奖励 rₜ 和下一状态 sₜ₊₁\n",
    "        存储 (sₜ, aₜ, rₜ, sₜ₊₁) 到 D\n",
    "        \n",
    "        # 从 D 采样 mini-batch\n",
    "        计算目标: yⱼ = rⱼ + γ max_a' Q(s'ⱼ, a'; θ⁻)\n",
    "        最小化损失: L = (yⱼ - Q(sⱼ, aⱼ; θ))²\n",
    "        \n",
    "        # 周期性更新目标网络\n",
    "        每 C 步: θ⁻ ← θ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q 函数与贝尔曼方程\n",
    "\n",
    "### 2.1 Q 函数定义\n",
    "\n",
    "状态-动作价值函数 $Q^\\pi(s, a)$ 表示从状态 $s$ 执行动作 $a$，然后遵循策略 $\\pi$ 的期望累积奖励：\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_0 = a\\right]$$\n",
    "\n",
    "### 2.2 最优贝尔曼方程\n",
    "\n",
    "最优 Q 函数满足贝尔曼最优方程：\n",
    "\n",
    "$$Q^*(s, a) = \\mathbb{E}_{s'}\\left[r + \\gamma \\max_{a'} Q^*(s', a') | s, a\\right]$$\n",
    "\n",
    "这是一个递归定义：当前状态-动作的价值 = 即时奖励 + 折扣后的最优未来价值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示贝尔曼方程的迭代求解（简单网格世界）\n",
    "\n",
    "def bellman_iteration_demo():\n",
    "    \"\"\"\n",
    "    在简单的 3x3 网格世界中演示贝尔曼迭代\n",
    "    \n",
    "    网格布局:\n",
    "    [0][1][2]\n",
    "    [3][4][5]\n",
    "    [6][7][G]  G=目标状态(+10奖励)\n",
    "    \"\"\"\n",
    "    num_states = 9\n",
    "    num_actions = 4  # 上下左右\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # Q 值表初始化\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    # 奖励：到达目标状态(8)获得+10\n",
    "    rewards = np.zeros(num_states)\n",
    "    rewards[8] = 10.0\n",
    "    \n",
    "    # 简单的转移函数（确定性）\n",
    "    # transitions[s][a] = next_state\n",
    "    transitions = np.array([\n",
    "        [0, 3, 0, 1],  # 状态0: 上→0, 下→3, 左→0, 右→1\n",
    "        [1, 4, 0, 2],\n",
    "        [2, 5, 1, 2],\n",
    "        [0, 6, 3, 4],\n",
    "        [1, 7, 3, 5],\n",
    "        [2, 8, 4, 5],\n",
    "        [3, 6, 6, 7],\n",
    "        [4, 8, 6, 8],\n",
    "        [8, 8, 8, 8],  # 目标状态（终止）\n",
    "    ])\n",
    "    \n",
    "    # 贝尔曼迭代\n",
    "    Q_history = [Q.copy()]\n",
    "    \n",
    "    for iteration in range(20):\n",
    "        Q_new = np.zeros_like(Q)\n",
    "        \n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                s_next = transitions[s, a]\n",
    "                # Q(s,a) = r + γ * max_a' Q(s', a')\n",
    "                Q_new[s, a] = rewards[s_next] + gamma * np.max(Q[s_next])\n",
    "        \n",
    "        Q = Q_new\n",
    "        Q_history.append(Q.copy())\n",
    "    \n",
    "    return Q, Q_history\n",
    "\n",
    "Q_final, Q_history = bellman_iteration_demo()\n",
    "\n",
    "# 可视化收敛过程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 左图：Q 值收敛过程\n",
    "ax1 = axes[0]\n",
    "for s in [4, 5, 7]:  # 选择几个状态可视化\n",
    "    max_q_over_time = [np.max(Q_history[i][s]) for i in range(len(Q_history))]\n",
    "    ax1.plot(max_q_over_time, label=f'State {s}')\n",
    "\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('max Q(s, a)')\n",
    "ax1.set_title('Bellman Iteration Convergence')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 右图：最终 Q 值热力图\n",
    "ax2 = axes[1]\n",
    "V = np.max(Q_final, axis=1).reshape(3, 3)\n",
    "im = ax2.imshow(V, cmap='YlOrRd')\n",
    "ax2.set_title('State Values V(s) = max_a Q(s,a)')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax2.text(j, i, f'{V[i,j]:.1f}', ha='center', va='center')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"最终 Q 值表:\")\n",
    "print(\"动作: [上, 下, 左, 右]\")\n",
    "print(Q_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 经验回放机制\n",
    "\n",
    "### 3.1 为什么需要经验回放？\n",
    "\n",
    "直接使用在线数据训练神经网络存在两个问题：\n",
    "\n",
    "1. **样本相关性**: 连续的状态高度相关，违反 i.i.d. 假设\n",
    "2. **数据效率**: 每个经验只使用一次，浪费了有价值的数据\n",
    "\n",
    "### 3.2 经验回放的作用\n",
    "\n",
    "- 存储历史经验 $(s, a, r, s')$\n",
    "- 随机采样打破时间相关性\n",
    "- 允许多次重用数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示经验回放的采样分布\n",
    "\n",
    "class SimpleReplayBuffer:\n",
    "    \"\"\"简单的经验回放缓冲区实现\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in indices], indices\n",
    "\n",
    "# 模拟数据收集过程\n",
    "buffer = SimpleReplayBuffer(capacity=1000)\n",
    "\n",
    "# 添加模拟经验（带时间戳）\n",
    "for t in range(1000):\n",
    "    experience = {'time': t, 'data': np.random.randn()}\n",
    "    buffer.add(experience)\n",
    "\n",
    "# 采样并分析时间分布\n",
    "sampled_times = []\n",
    "for _ in range(100):  # 100 次采样\n",
    "    batch, indices = buffer.sample(batch_size=64)\n",
    "    times = [exp['time'] for exp in batch]\n",
    "    sampled_times.extend(times)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(sampled_times, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Experience Timestamp')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.title('Uniform Sampling from Replay Buffer\\n(Breaks temporal correlation)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：采样分布近似均匀，时间上不相邻的经验被混合在一起\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 目标网络\n",
    "\n",
    "### 4.1 训练不稳定问题\n",
    "\n",
    "在标准 Q-Learning 中，TD 目标是：\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta)$$\n",
    "\n",
    "问题：$\\theta$ 每步都在更新，导致目标 $y$ 不断变化（追逐移动目标）。\n",
    "\n",
    "### 4.2 目标网络解决方案\n",
    "\n",
    "使用独立的目标网络 $\\theta^-$ 计算 TD 目标：\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "$\\theta^-$ 周期性地从 $\\theta$ 复制（硬更新）或软更新：\n",
    "\n",
    "$$\\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示目标网络的稳定性作用\n",
    "\n",
    "def simulate_training_stability(use_target_network=True, update_period=10):\n",
    "    \"\"\"\n",
    "    模拟训练过程中的目标稳定性\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 模拟网络参数\n",
    "    theta = 1.0\n",
    "    theta_target = 1.0\n",
    "    \n",
    "    # 记录历史\n",
    "    theta_history = []\n",
    "    target_history = []\n",
    "    \n",
    "    for step in range(100):\n",
    "        # 模拟梯度更新（带噪声）\n",
    "        gradient = 0.1 * (np.random.randn() - 0.1 * theta)\n",
    "        theta += gradient\n",
    "        \n",
    "        if use_target_network:\n",
    "            # 周期性更新目标网络\n",
    "            if step % update_period == 0:\n",
    "                theta_target = theta\n",
    "            target = theta_target\n",
    "        else:\n",
    "            target = theta\n",
    "        \n",
    "        theta_history.append(theta)\n",
    "        target_history.append(target)\n",
    "    \n",
    "    return theta_history, target_history\n",
    "\n",
    "# 对比有无目标网络\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 无目标网络\n",
    "theta_no_target, target_no_target = simulate_training_stability(use_target_network=False)\n",
    "axes[0].plot(theta_no_target, label='θ (online)', alpha=0.8)\n",
    "axes[0].plot(target_no_target, label='target', alpha=0.8, linestyle='--')\n",
    "axes[0].set_title('Without Target Network\\n(Target changes every step)')\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('Parameter Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 有目标网络\n",
    "theta_with_target, target_with_target = simulate_training_stability(use_target_network=True, update_period=10)\n",
    "axes[1].plot(theta_with_target, label='θ (online)', alpha=0.8)\n",
    "axes[1].plot(target_with_target, label='θ⁻ (target)', alpha=0.8, linestyle='--')\n",
    "axes[1].set_title('With Target Network (update every 10 steps)\\n(Target is stable)')\n",
    "axes[1].set_xlabel('Training Step')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：目标网络提供了稳定的训练目标（阶梯状变化）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ε-贪婪探索\n",
    "\n",
    "### 5.1 探索-利用权衡\n",
    "\n",
    "强化学习面临经典的探索-利用困境：\n",
    "- **利用 (Exploitation)**: 选择当前已知最优动作\n",
    "- **探索 (Exploration)**: 尝试新动作以发现更好的策略\n",
    "\n",
    "### 5.2 ε-贪婪策略\n",
    "\n",
    "$$a = \\begin{cases}\n",
    "\\text{随机动作} & \\text{概率 } \\epsilon \\\\\n",
    "\\arg\\max_a Q(s, a) & \\text{概率 } 1 - \\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "通常 $\\epsilon$ 随训练衰减：从高探索逐渐转向高利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化不同的 ε 衰减策略\n",
    "\n",
    "def linear_decay(step, start=1.0, end=0.01, decay_steps=10000):\n",
    "    \"\"\"线性衰减\"\"\"\n",
    "    return max(end, start - (start - end) * step / decay_steps)\n",
    "\n",
    "def exponential_decay(step, start=1.0, end=0.01, decay_rate=0.9995):\n",
    "    \"\"\"指数衰减\"\"\"\n",
    "    return max(end, start * (decay_rate ** step))\n",
    "\n",
    "def step_decay(step, start=1.0, end=0.01, decay_steps=5000):\n",
    "    \"\"\"阶梯衰减\"\"\"\n",
    "    levels = [1.0, 0.5, 0.2, 0.1, 0.01]\n",
    "    idx = min(step // decay_steps, len(levels) - 1)\n",
    "    return levels[idx]\n",
    "\n",
    "# 绘制衰减曲线\n",
    "steps = np.arange(20000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, [linear_decay(s) for s in steps], label='Linear Decay')\n",
    "plt.plot(steps, [exponential_decay(s) for s in steps], label='Exponential Decay')\n",
    "plt.plot(steps, [step_decay(s) for s in steps], label='Step Decay')\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('ε (Exploration Rate)')\n",
    "plt.title('ε-Greedy Exploration Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"不同衰减策略的特点:\")\n",
    "print(\"- 线性衰减: 简单直观，衰减速度恒定\")\n",
    "print(\"- 指数衰减: 初期快速衰减，后期缓慢\")\n",
    "print(\"- 阶梯衰减: 阶段性调整，便于控制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DQN 变体对比\n",
    "\n",
    "### 6.1 Double DQN\n",
    "\n",
    "**问题**: 标准 DQN 存在 Q 值过估计问题\n",
    "\n",
    "标准 DQN 的 max 操作同时用于选择和评估动作：\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "**Double DQN 解决方案**:\n",
    "$$y = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-)$$\n",
    "\n",
    "- 在线网络选择动作\n",
    "- 目标网络评估动作\n",
    "\n",
    "### 6.2 Dueling DQN\n",
    "\n",
    "将 Q 值分解为状态价值和优势函数：\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s, a')$$\n",
    "\n",
    "### 6.3 算法对比表\n",
    "\n",
    "| 变体 | 核心改进 | 优点 |\n",
    "|------|---------|------|\n",
    "| DQN | 经验回放 + 目标网络 | 基础稳定方法 |\n",
    "| Double DQN | 分离选择与评估 | 缓解过估计 |\n",
    "| Dueling DQN | V + A 分解 | 更好的状态表示 |\n",
    "| PER | 优先级采样 | 提高样本效率 |\n",
    "| Noisy DQN | 参数化探索 | 更好的探索 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 Q 值过估计问题\n",
    "\n",
    "def demonstrate_overestimation():\n",
    "    \"\"\"\n",
    "    演示 max 操作导致的过估计\n",
    "    \n",
    "    假设真实 Q 值为 0，但估计值有噪声\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    num_actions = 10\n",
    "    num_trials = 1000\n",
    "    \n",
    "    # 不同噪声水平\n",
    "    noise_levels = np.linspace(0.1, 2.0, 20)\n",
    "    \n",
    "    # 真实值为 0\n",
    "    true_q = 0.0\n",
    "    \n",
    "    max_estimates = []\n",
    "    mean_estimates = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        max_vals = []\n",
    "        mean_vals = []\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            # Q 估计 = 真实值 + 噪声\n",
    "            q_estimates = true_q + noise * np.random.randn(num_actions)\n",
    "            \n",
    "            max_vals.append(np.max(q_estimates))\n",
    "            mean_vals.append(np.mean(q_estimates))\n",
    "        \n",
    "        max_estimates.append(np.mean(max_vals))\n",
    "        mean_estimates.append(np.mean(mean_vals))\n",
    "    \n",
    "    return noise_levels, max_estimates, mean_estimates\n",
    "\n",
    "noise_levels, max_est, mean_est = demonstrate_overestimation()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(noise_levels, max_est, 'b-o', label='max Q (overestimated)', markersize=4)\n",
    "plt.plot(noise_levels, mean_est, 'g-s', label='mean Q', markersize=4)\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='True Q = 0')\n",
    "\n",
    "plt.xlabel('Estimation Noise (σ)')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.title('Q-Value Overestimation due to max Operation\\n(True Q = 0 for all actions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：噪声越大，max 操作导致的过估计越严重\")\n",
    "print(\"Double DQN 通过分离选择和评估来缓解这个问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 实验与分析\n",
    "\n",
    "### 7.1 在 CartPole 上训练 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "env_name = 'CartPole-v1'\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "print(f\"环境: {env_name}\")\n",
    "print(f\"观测空间: {train_env.observation_spec()}\")\n",
    "print(f\"动作空间: {train_env.action_spec()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置超参数\n",
    "num_iterations = 5000\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_capacity = 100000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "target_update_period = 200\n",
    "epsilon_greedy = 0.1\n",
    "log_interval = 500\n",
    "eval_interval = 1000\n",
    "num_eval_episodes = 10\n",
    "\n",
    "# 创建 Q 网络\n",
    "fc_layer_params = (100, 50)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params\n",
    ")\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 创建 DQN Agent\n",
    "train_step_counter = tf.Variable(0, dtype=tf.int64)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    gamma=gamma,\n",
    "    epsilon_greedy=epsilon_greedy,\n",
    "    target_update_period=target_update_period\n",
    ")\n",
    "\n",
    "agent.initialize()\n",
    "print(\"DQN Agent 创建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建回放缓冲区\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity\n",
    ")\n",
    "\n",
    "# 数据收集函数\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def evaluate_policy(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward.numpy()[0]\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes\n",
    "\n",
    "# 预填充缓冲区\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec()\n",
    ")\n",
    "\n",
    "train_env.reset()\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy, replay_buffer)\n",
    "\n",
    "print(f\"缓冲区初始大小: {replay_buffer.num_frames().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# 记录历史\n",
    "returns = []\n",
    "losses = []\n",
    "\n",
    "# 初始评估\n",
    "avg_return = evaluate_policy(eval_env, agent.policy, num_eval_episodes)\n",
    "returns.append(avg_return)\n",
    "print(f\"Step 0: Avg Return = {avg_return:.2f}\")\n",
    "\n",
    "# 训练\n",
    "train_env.reset()\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    # 收集数据\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "    \n",
    "    # 训练\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    losses.append(train_loss.numpy())\n",
    "    \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: Loss = {train_loss:.4f}\")\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = evaluate_policy(eval_env, agent.policy, num_eval_episodes)\n",
    "        returns.append(avg_return)\n",
    "        print(f\"Step {step}: Avg Return = {avg_return:.2f}\")\n",
    "\n",
    "print(\"\\n训练完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练结果\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 回报曲线\n",
    "ax1 = axes[0]\n",
    "eval_steps = [0] + list(range(eval_interval, num_iterations + 1, eval_interval))\n",
    "ax1.plot(eval_steps, returns, 'b-o', linewidth=2, markersize=4)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Average Return')\n",
    "ax1.set_title('DQN Training: Episode Returns')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=500, color='g', linestyle='--', label='Max Score')\n",
    "ax1.legend()\n",
    "\n",
    "# 损失曲线\n",
    "ax2 = axes[1]\n",
    "window = 50\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(smoothed, 'r-', linewidth=1, alpha=0.8)\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Loss (smoothed)')\n",
    "ax2.set_title('DQN Training: TD Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### DQN 关键要点\n",
    "\n",
    "1. **Q 函数逼近**: 使用神经网络替代 Q 表，处理高维状态\n",
    "\n",
    "2. **经验回放**:\n",
    "   - 打破样本相关性\n",
    "   - 提高数据利用效率\n",
    "   - 使训练更稳定\n",
    "\n",
    "3. **目标网络**:\n",
    "   - 提供稳定的训练目标\n",
    "   - 防止追逐移动目标\n",
    "   - 周期性或软更新\n",
    "\n",
    "4. **ε-贪婪探索**:\n",
    "   - 平衡探索与利用\n",
    "   - 衰减策略很重要\n",
    "\n",
    "### 数学公式回顾\n",
    "\n",
    "**贝尔曼方程**:\n",
    "$$Q^*(s, a) = \\mathbb{E}[r + \\gamma \\max_{a'} Q^*(s', a')]$$\n",
    "\n",
    "**DQN 损失函数**:\n",
    "$$L(\\theta) = \\mathbb{E}[(r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a))^2]$$\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- 学习 Actor-Critic 方法（SAC、PPO）\n",
    "- 探索 DQN 变体（Double DQN、Dueling DQN）\n",
    "- 应用到更复杂的环境"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
