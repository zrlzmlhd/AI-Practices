{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-Agents 强化学习库完整教程\n",
    "\n",
    "本教程系统性地介绍 TF-Agents 库的核心概念和使用方法。\n",
    "\n",
    "## 目录\n",
    "1. [环境配置与导入](#1-环境配置与导入)\n",
    "2. [核心组件介绍](#2-核心组件介绍)\n",
    "3. [环境 (Environment)](#3-环境-environment)\n",
    "4. [策略 (Policy)](#4-策略-policy)\n",
    "5. [智能体 (Agent)](#5-智能体-agent)\n",
    "6. [经验回放 (Replay Buffer)](#6-经验回放-replay-buffer)\n",
    "7. [完整训练示例](#7-完整训练示例)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境配置与导入\n",
    "\n",
    "首先安装必要的依赖并导入所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖（如果需要）\n",
    "# !pip install tensorflow tf-agents gymnasium matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 设置日志级别\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# TF-Agents 核心导入\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "print(f\"TensorFlow 版本: {tf.__version__}\")\n",
    "print(f\"GPU 可用: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心组件介绍\n",
    "\n",
    "TF-Agents 的设计遵循模块化原则，核心组件包括：\n",
    "\n",
    "### 2.1 组件关系图\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         Training Loop                            │\n",
    "│  ┌───────────┐    ┌──────────────┐    ┌───────────────────────┐ │\n",
    "│  │Environment│◄──►│    Agent     │◄──►│    Replay Buffer      │ │\n",
    "│  │           │    │              │    │                       │ │\n",
    "│  │ - reset() │    │ - policy     │    │ - add_batch()         │ │\n",
    "│  │ - step()  │    │ - train()    │    │ - sample()            │ │\n",
    "│  └───────────┘    │ - networks   │    └───────────────────────┘ │\n",
    "│                   └──────────────┘                              │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2.2 数据流\n",
    "\n",
    "1. **数据收集**: Environment → Policy → Trajectory → Replay Buffer\n",
    "2. **训练更新**: Replay Buffer → Sample → Agent.train() → Network Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 环境 (Environment)\n",
    "\n",
    "环境是智能体交互的世界，定义了状态空间、动作空间和奖励函数。\n",
    "\n",
    "### 3.1 加载 Gym 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 CartPole 环境\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "# Python 环境（用于调试）\n",
    "py_env = suite_gym.load(env_name)\n",
    "\n",
    "# TensorFlow 环境（用于训练，支持批处理）\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "\n",
    "print(\"环境信息:\")\n",
    "print(f\"  名称: {env_name}\")\n",
    "print(f\"  观测空间: {tf_env.observation_spec()}\")\n",
    "print(f\"  动作空间: {tf_env.action_spec()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 理解 TimeStep\n",
    "\n",
    "`TimeStep` 是 TF-Agents 中描述环境状态的核心数据结构：\n",
    "\n",
    "$$\\text{TimeStep} = (\\text{step\\_type}, \\text{reward}, \\text{discount}, \\text{observation})$$\n",
    "\n",
    "- `step_type`: FIRST (回合开始), MID (中间), LAST (终止)\n",
    "- `reward`: 即时奖励 $r_t$\n",
    "- `discount`: 折扣因子 $\\gamma$\n",
    "- `observation`: 观测 $s_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置环境并获取初始 TimeStep\n",
    "time_step = tf_env.reset()\n",
    "\n",
    "print(\"初始 TimeStep:\")\n",
    "print(f\"  step_type: {time_step.step_type}\")\n",
    "print(f\"  reward: {time_step.reward}\")\n",
    "print(f\"  discount: {time_step.discount}\")\n",
    "print(f\"  observation: {time_step.observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 环境交互演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动执行几步交互\n",
    "print(\"环境交互演示:\")\n",
    "time_step = tf_env.reset()\n",
    "\n",
    "for i in range(5):\n",
    "    # 随机选择动作\n",
    "    action = tf.constant([np.random.randint(2)])\n",
    "    \n",
    "    # 执行动作\n",
    "    next_time_step = tf_env.step(action)\n",
    "    \n",
    "    print(f\"\\n步骤 {i+1}:\")\n",
    "    print(f\"  动作: {action.numpy()[0]}\")\n",
    "    print(f\"  奖励: {next_time_step.reward.numpy()[0]}\")\n",
    "    print(f\"  观测: {next_time_step.observation.numpy()[0][:2]}...\")\n",
    "    print(f\"  终止: {next_time_step.is_last().numpy()[0]}\")\n",
    "    \n",
    "    time_step = next_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 策略 (Policy)\n",
    "\n",
    "策略定义了智能体的行为方式，将观测映射到动作。\n",
    "\n",
    "### 4.1 数学定义\n",
    "\n",
    "$$\\pi: \\mathcal{S} \\to \\mathcal{P}(\\mathcal{A})$$\n",
    "\n",
    "- **确定性策略**: $a = \\pi(s)$\n",
    "- **随机策略**: $a \\sim \\pi(\\cdot|s)$\n",
    "\n",
    "### 4.2 随机策略示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建随机策略\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec()\n",
    ")\n",
    "\n",
    "# 使用策略选择动作\n",
    "time_step = tf_env.reset()\n",
    "action_step = random_policy.action(time_step)\n",
    "\n",
    "print(f\"策略选择的动作: {action_step.action.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 评估策略性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(environment, policy, num_episodes=10):\n",
    "    \"\"\"\n",
    "    评估策略的平均回合回报\n",
    "    \n",
    "    数学公式:\n",
    "    平均回报 = (1/N) * Σ G_i\n",
    "    其中 G_i = Σ r_t 是第 i 个回合的总奖励\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward.numpy()[0]\n",
    "        \n",
    "        total_return += episode_return\n",
    "    \n",
    "    return total_return / num_episodes\n",
    "\n",
    "# 评估随机策略\n",
    "avg_return = evaluate_policy(tf_env, random_policy, num_episodes=10)\n",
    "print(f\"随机策略平均回报: {avg_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 智能体 (Agent)\n",
    "\n",
    "智能体封装了学习算法，包括网络、优化器和训练逻辑。\n",
    "\n",
    "### 5.1 DQN 算法简介\n",
    "\n",
    "DQN 使用神经网络逼近 Q 函数：\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0=s, a_0=a\\right]$$\n",
    "\n",
    "训练目标（最小化 TD 误差）：\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}\\left[(r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta))^2\\right]$$\n",
    "\n",
    "### 5.2 构建 Q 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 Q 网络\n",
    "fc_layer_params = (100, 50)  # 两层全连接，分别 100 和 50 个神经元\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params\n",
    ")\n",
    "\n",
    "print(\"Q 网络结构:\")\n",
    "print(f\"  输入: {tf_env.observation_spec()}\")\n",
    "print(f\"  输出: {tf_env.action_spec()}\")\n",
    "print(f\"  隐藏层: {fc_layer_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 创建 DQN 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数配置\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon_greedy = 0.1\n",
    "target_update_period = 200\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 训练步数计数器\n",
    "train_step_counter = tf.Variable(0, dtype=tf.int64)\n",
    "\n",
    "# 创建 DQN 智能体\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    gamma=gamma,\n",
    "    epsilon_greedy=epsilon_greedy,\n",
    "    target_update_period=target_update_period\n",
    ")\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "print(\"DQN 智能体配置:\")\n",
    "print(f\"  学习率: {learning_rate}\")\n",
    "print(f\"  折扣因子 γ: {gamma}\")\n",
    "print(f\"  探索率 ε: {epsilon_greedy}\")\n",
    "print(f\"  目标网络更新周期: {target_update_period}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 经验回放 (Replay Buffer)\n",
    "\n",
    "经验回放存储历史交互数据，通过随机采样打破样本相关性。\n",
    "\n",
    "### 6.1 数学原理\n",
    "\n",
    "缓冲区存储经验元组：\n",
    "$$\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i, \\text{done}_i)\\}_{i=1}^{N}$$\n",
    "\n",
    "均匀采样：\n",
    "$$P(i) = \\frac{1}{|\\mathcal{D}|}$$\n",
    "\n",
    "### 6.2 创建回放缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建回放缓冲区\n",
    "replay_buffer_capacity = 100000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity\n",
    ")\n",
    "\n",
    "print(f\"回放缓冲区容量: {replay_buffer_capacity}\")\n",
    "print(f\"数据规范: {agent.collect_data_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 数据收集函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    \"\"\"\n",
    "    收集单步交互数据并存入缓冲区\n",
    "    \n",
    "    流程:\n",
    "    1. 获取当前状态 s_t\n",
    "    2. 策略选择动作 a_t\n",
    "    3. 环境执行动作，返回 (r_t, s_{t+1})\n",
    "    4. 构建 Trajectory 并存入缓冲区\n",
    "    \"\"\"\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    \n",
    "    # 构建轨迹\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    \n",
    "    # 存入缓冲区\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(environment, policy, buffer, num_steps):\n",
    "    \"\"\"收集多步数据\"\"\"\n",
    "    for _ in range(num_steps):\n",
    "        collect_step(environment, policy, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 预填充缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机策略预填充缓冲区\n",
    "initial_collect_steps = 1000\n",
    "\n",
    "tf_env.reset()\n",
    "collect_data(tf_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "print(f\"缓冲区当前大小: {replay_buffer.num_frames().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 完整训练示例\n",
    "\n",
    "将所有组件组合，实现完整的 DQN 训练循环。\n",
    "\n",
    "### 7.1 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参数\n",
    "num_iterations = 10000  # 总训练迭代数\n",
    "collect_steps_per_iteration = 1  # 每次迭代收集的步数\n",
    "batch_size = 64  # 训练批大小\n",
    "log_interval = 200  # 日志打印间隔\n",
    "eval_interval = 1000  # 评估间隔\n",
    "num_eval_episodes = 10  # 评估回合数\n",
    "\n",
    "print(\"训练配置:\")\n",
    "print(f\"  总迭代数: {num_iterations}\")\n",
    "print(f\"  批大小: {batch_size}\")\n",
    "print(f\"  评估间隔: {eval_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估环境\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "# 准备数据集\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2  # 连续两步用于计算 TD 目标\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# 编译训练函数（加速）\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# 记录训练历史\n",
    "returns = []\n",
    "losses = []\n",
    "\n",
    "# 初始评估\n",
    "avg_return = evaluate_policy(eval_tf_env, agent.policy, num_eval_episodes)\n",
    "returns.append(avg_return)\n",
    "print(f\"初始平均回报: {avg_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "tf_env.reset()\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    # 1. 收集数据\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(tf_env, agent.collect_policy, replay_buffer)\n",
    "    \n",
    "    # 2. 采样并训练\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    losses.append(train_loss.numpy())\n",
    "    \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    \n",
    "    # 3. 日志\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: Loss = {train_loss:.4f}\")\n",
    "    \n",
    "    # 4. 评估\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = evaluate_policy(eval_tf_env, agent.policy, num_eval_episodes)\n",
    "        returns.append(avg_return)\n",
    "        print(f\"Step {step}: Avg Return = {avg_return:.2f}\")\n",
    "\n",
    "print(\"\\n训练完成!\")\n",
    "print(f\"最终平均回报: {returns[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 可视化训练曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 回报曲线\n",
    "ax1 = axes[0]\n",
    "iterations_eval = [0] + list(range(eval_interval, num_iterations + 1, eval_interval))\n",
    "ax1.plot(iterations_eval, returns, 'b-o', linewidth=2, markersize=4)\n",
    "ax1.set_xlabel('Training Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Average Return', fontsize=12)\n",
    "ax1.set_title('DQN Training Progress', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 损失曲线（平滑）\n",
    "ax2 = axes[1]\n",
    "window = 100\n",
    "smoothed_losses = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(smoothed_losses, 'r-', linewidth=1, alpha=0.8)\n",
    "ax2.set_xlabel('Training Step', fontsize=12)\n",
    "ax2.set_ylabel('Loss (smoothed)', fontsize=12)\n",
    "ax2.set_title('Training Loss', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 测试训练好的智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试训练好的策略\n",
    "print(\"测试训练好的智能体:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for episode in range(3):\n",
    "    time_step = eval_tf_env.reset()\n",
    "    episode_return = 0.0\n",
    "    steps = 0\n",
    "    \n",
    "    while not time_step.is_last():\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = eval_tf_env.step(action_step.action)\n",
    "        episode_return += time_step.reward.numpy()[0]\n",
    "        steps += 1\n",
    "    \n",
    "    print(f\"回合 {episode + 1}: 步数 = {steps}, 总奖励 = {episode_return:.0f}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程介绍了 TF-Agents 的核心组件：\n",
    "\n",
    "1. **Environment**: 定义状态空间、动作空间和奖励函数\n",
    "2. **Policy**: 将观测映射到动作\n",
    "3. **Agent**: 封装学习算法和网络\n",
    "4. **Replay Buffer**: 存储和采样经验数据\n",
    "\n",
    "### 关键数学公式回顾\n",
    "\n",
    "**Q-Learning 更新**:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "**DQN 损失函数**:\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}[(r + \\gamma \\max_{a'} Q_{\\theta^-}(s',a') - Q_\\theta(s,a))^2]$$\n",
    "\n",
    "### 下一步学习\n",
    "\n",
    "- 查看 `02_dqn_cartpole.py` 了解完整的 DQN 实现\n",
    "- 学习 `03_sac_continuous_control.py` 了解连续控制\n",
    "- 探索 `04_ppo_agent.py` 学习策略梯度方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
