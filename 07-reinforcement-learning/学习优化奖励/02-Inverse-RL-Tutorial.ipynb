{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逆强化学习 (Inverse Reinforcement Learning) 深度教程\n",
    "\n",
    "## 目录\n",
    "1. [问题定义与动机](#1-问题定义与动机)\n",
    "2. [理论基础](#2-理论基础)\n",
    "3. [最大边际IRL](#3-最大边际irl)\n",
    "4. [最大熵IRL](#4-最大熵irl)\n",
    "5. [深度IRL与GAIL](#5-深度irl与gail)\n",
    "6. [实验与对比](#6-实验与对比)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 问题定义与动机\n",
    "\n",
    "### 1.1 从正向RL到逆向RL\n",
    "\n",
    "**正向强化学习 (Forward RL)**：\n",
    "- 输入：MDP = (S, A, P, R, γ)\n",
    "- 输出：最优策略 π*\n",
    "- 问题：给定奖励，找最优行为\n",
    "\n",
    "**逆强化学习 (Inverse RL)**：\n",
    "- 输入：MDP\\R = (S, A, P, ?, γ) + 专家演示 D = {τ₁, τ₂, ...}\n",
    "- 输出：奖励函数 R 使得专家行为是最优的\n",
    "- 问题：给定行为，推断奖励/意图\n",
    "\n",
    "### 1.2 为什么需要IRL？\n",
    "\n",
    "1. **奖励函数难以手工设计**\n",
    "   - 复杂任务的奖励函数可能有数百个组件\n",
    "   - 奖励工程容易导致\"奖励黑客\"行为\n",
    "\n",
    "2. **从演示中学习**\n",
    "   - 人类专家可以演示正确行为，但难以精确描述奖励\n",
    "   - \"我知道好的驾驶是什么样，但无法写出奖励公式\"\n",
    "\n",
    "3. **理解智能体意图**\n",
    "   - 通过观察行为推断目标\n",
    "   - 应用：人机交互、安全AI、行为预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 理论基础\n",
    "\n",
    "### 2.1 线性奖励函数假设\n",
    "\n",
    "IRL通常假设奖励函数是特征的线性组合：\n",
    "\n",
    "$$R(s) = \\theta^T \\phi(s) = \\sum_{i=1}^{d} \\theta_i \\phi_i(s)$$\n",
    "\n",
    "其中：\n",
    "- $\\phi: S \\rightarrow \\mathbb{R}^d$ 是特征提取函数\n",
    "- $\\theta \\in \\mathbb{R}^d$ 是待学习的奖励权重\n",
    "\n",
    "### 2.2 特征期望 (Feature Expectations)\n",
    "\n",
    "对于策略 $\\pi$，特征期望定义为：\n",
    "\n",
    "$$\\mu_\\pi = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\phi(s_t)\\right]$$\n",
    "\n",
    "这是策略的\"签名\"——描述了该策略会访问哪些类型的状态。\n",
    "\n",
    "### 2.3 IRL的模糊性问题\n",
    "\n",
    "**关键洞察**：存在无穷多个奖励函数与同一策略一致！\n",
    "\n",
    "- $R(s) = 0$ 对所有s：任何策略都是最优的\n",
    "- $R(s) = c$（常数）：同上\n",
    "- 任何使专家策略最优的R\n",
    "\n",
    "需要额外约束来选择\"正确的\"奖励函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_rl import (\n",
    "    IRLConfig,\n",
    "    Demonstration,\n",
    "    LinearFeatureExtractor,\n",
    "    MaxMarginIRL,\n",
    "    MaxEntropyIRL,\n",
    "    DeepIRL,\n",
    "    GAILDiscriminator,\n",
    "    compute_feature_matching_loss,\n",
    ")\n",
    "\n",
    "def visualize_irl_ambiguity():\n",
    "    \"\"\"可视化IRL的模糊性问题。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 简单的2D状态空间\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 三个不同的奖励函数，都使得向(1,1)移动是最优的\n",
    "    goal = np.array([1.0, 1.0])\n",
    "    \n",
    "    # R1: 负距离\n",
    "    R1 = -np.sqrt((X - goal[0])**2 + (Y - goal[1])**2)\n",
    "    \n",
    "    # R2: 负距离的平方\n",
    "    R2 = -((X - goal[0])**2 + (Y - goal[1])**2)\n",
    "    \n",
    "    # R3: 指数衰减\n",
    "    R3 = np.exp(-np.sqrt((X - goal[0])**2 + (Y - goal[1])**2))\n",
    "    \n",
    "    rewards = [R1, R2, R3]\n",
    "    titles = ['$R_1 = -||s - g||$', '$R_2 = -||s - g||^2$', '$R_3 = e^{-||s - g||}$']\n",
    "    \n",
    "    for ax, R, title in zip(axes, rewards, titles):\n",
    "        contour = ax.contourf(X, Y, R, levels=20, cmap='viridis')\n",
    "        ax.plot(goal[0], goal[1], 'r*', markersize=15, label='Goal')\n",
    "        ax.set_xlabel('$s_1$')\n",
    "        ax.set_ylabel('$s_2$')\n",
    "        ax.set_title(title)\n",
    "        plt.colorbar(contour, ax=ax)\n",
    "    \n",
    "    plt.suptitle('IRL Ambiguity: Different Rewards, Same Optimal Policy', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"关键洞察：这三个奖励函数都使得向(1,1)移动是最优的！\")\n",
    "    print(\"IRL需要额外的约束（如最大边际、最大熵）来选择唯一解。\")\n",
    "\n",
    "visualize_irl_ambiguity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 最大边际IRL (Max-Margin IRL)\n",
    "\n",
    "### 3.1 核心思想\n",
    "\n",
    "找到奖励权重 $\\theta$，使得专家策略的期望回报最大化地超过其他所有策略：\n",
    "\n",
    "$$\\max_\\theta \\min_\\pi \\left[ \\theta^T (\\mu_E - \\mu_\\pi) \\right]$$\n",
    "\n",
    "约束 $||\\theta||_2 \\leq 1$\n",
    "\n",
    "### 3.2 几何解释\n",
    "\n",
    "在特征期望空间中，寻找最优分离超平面，将专家行为与其他行为分开。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_max_margin_irl():\n",
    "    \"\"\"演示最大边际IRL的几何直觉。\"\"\"\n",
    "    \n",
    "    # 模拟特征期望（2D用于可视化）\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 专家的特征期望\n",
    "    expert_mu = np.array([3.0, 4.0])\n",
    "    \n",
    "    # 其他策略的特征期望\n",
    "    other_policies_mu = np.array([\n",
    "        [1.0, 1.5],\n",
    "        [2.0, 1.0],\n",
    "        [1.5, 2.5],\n",
    "        [0.5, 3.0],\n",
    "        [2.5, 2.0],\n",
    "        [1.0, 0.5],\n",
    "    ])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 左图：特征期望空间\n",
    "    ax = axes[0]\n",
    "    ax.scatter(other_policies_mu[:, 0], other_policies_mu[:, 1], \n",
    "               c='blue', s=100, label='Other Policies $\\mu_\\pi$', alpha=0.6)\n",
    "    ax.scatter(expert_mu[0], expert_mu[1], c='red', s=200, \n",
    "               marker='*', label='Expert $\\mu_E$', zorder=5)\n",
    "    \n",
    "    # 找到凸包中最近点\n",
    "    closest_point = np.mean(other_policies_mu, axis=0)  # 简化\n",
    "    \n",
    "    # 分离超平面方向\n",
    "    theta = expert_mu - closest_point\n",
    "    theta = theta / np.linalg.norm(theta)\n",
    "    \n",
    "    # 绘制分离线\n",
    "    midpoint = (expert_mu + closest_point) / 2\n",
    "    perp = np.array([-theta[1], theta[0]])\n",
    "    line_points = np.array([midpoint - 3*perp, midpoint + 3*perp])\n",
    "    ax.plot(line_points[:, 0], line_points[:, 1], 'g--', linewidth=2, \n",
    "            label='Separating Hyperplane')\n",
    "    \n",
    "    # 绘制θ方向\n",
    "    ax.arrow(midpoint[0], midpoint[1], theta[0], theta[1],\n",
    "             head_width=0.15, head_length=0.1, fc='green', ec='green')\n",
    "    ax.annotate('$\\\\theta$', midpoint + theta + 0.2, fontsize=14)\n",
    "    \n",
    "    ax.set_xlabel('Feature Expectation $\\mu_1$')\n",
    "    ax.set_ylabel('Feature Expectation $\\mu_2$')\n",
    "    ax.set_title('Max-Margin IRL: Finding Separating Hyperplane')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 右图：算法迭代过程\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # 模拟迭代\n",
    "    margins = [0.5, 1.2, 1.8, 2.1, 2.3, 2.35, 2.38]\n",
    "    ax.plot(margins, 'b-o', linewidth=2, markersize=8)\n",
    "    ax.axhline(y=2.4, color='r', linestyle='--', label='Optimal Margin')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Margin $||\\mu_E - \\mu_\\pi||$')\n",
    "    ax.set_title('Max-Margin IRL Convergence')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n学习到的奖励权重 θ: [{theta[0]:.3f}, {theta[1]:.3f}]\")\n",
    "    print(f\"归一化后: ||θ|| = {np.linalg.norm(theta):.3f}\")\n",
    "\n",
    "demonstrate_max_margin_irl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 实现与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模拟演示数据\n",
    "state_dim = 4\n",
    "feature_dim = 10\n",
    "\n",
    "# 特征提取器\n",
    "feature_extractor = LinearFeatureExtractor(\n",
    "    state_dim=state_dim,\n",
    "    feature_type='rbf',\n",
    "    num_features=feature_dim,\n",
    ")\n",
    "\n",
    "# 生成专家演示（假设专家偏好高特征值的状态）\n",
    "def generate_expert_demonstrations(n_demos: int, traj_length: int) -> List[Demonstration]:\n",
    "    \"\"\"生成模拟的专家演示。\"\"\"\n",
    "    demos = []\n",
    "    \n",
    "    for _ in range(n_demos):\n",
    "        # 专家倾向于访问特定区域\n",
    "        states = []\n",
    "        state = np.random.randn(state_dim) * 0.5  # 从随机位置开始\n",
    "        \n",
    "        for t in range(traj_length):\n",
    "            states.append(state.copy())\n",
    "            # 专家向目标区域移动\n",
    "            target = np.array([1.0, 1.0, 0.5, 0.5])\n",
    "            state = state + 0.1 * (target - state) + 0.05 * np.random.randn(state_dim)\n",
    "        \n",
    "        demos.append(Demonstration(\n",
    "            states=np.array(states),\n",
    "            actions=np.zeros((traj_length, 1)),  # 动作不重要\n",
    "        ))\n",
    "    \n",
    "    return demos\n",
    "\n",
    "# 生成演示\n",
    "demonstrations = generate_expert_demonstrations(n_demos=20, traj_length=50)\n",
    "print(f\"生成了 {len(demonstrations)} 条专家演示\")\n",
    "print(f\"每条轨迹长度: {len(demonstrations[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置IRL\n",
    "config = IRLConfig(\n",
    "    discount_factor=0.99,\n",
    "    learning_rate=0.1,\n",
    "    max_iterations=50,\n",
    "    convergence_threshold=0.01,\n",
    "    feature_dim=feature_dim,\n",
    ")\n",
    "\n",
    "# 训练Max-Margin IRL\n",
    "maxmargin_irl = MaxMarginIRL(\n",
    "    config=config,\n",
    "    feature_extractor=feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"训练 Max-Margin IRL...\")\n",
    "reward_weights = maxmargin_irl.fit(demonstrations)\n",
    "\n",
    "print(f\"\\n学习到的奖励权重:\")\n",
    "print(f\"  形状: {reward_weights.shape}\")\n",
    "print(f\"  范围: [{reward_weights.min():.4f}, {reward_weights.max():.4f}]\")\n",
    "print(f\"  L2范数: {np.linalg.norm(reward_weights):.4f}\")\n",
    "\n",
    "# 可视化训练过程\n",
    "if maxmargin_irl._iteration_history:\n",
    "    margins = [h['margin'] for h in maxmargin_irl._iteration_history]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(margins, 'b-o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Margin')\n",
    "    plt.title('Max-Margin IRL Training Progress')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 最大熵IRL (Maximum Entropy IRL)\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "将专家行为建模为轨迹上的Boltzmann分布：\n",
    "\n",
    "$$P(\\tau | \\theta) = \\frac{1}{Z(\\theta)} \\exp\\left(\\sum_{t=0}^{T} R_\\theta(s_t, a_t)\\right)$$\n",
    "\n",
    "最大化演示数据的对数似然：\n",
    "\n",
    "$$\\max_\\theta \\sum_{\\tau \\in D} \\log P(\\tau | \\theta)$$\n",
    "\n",
    "### 4.2 优势\n",
    "\n",
    "1. **处理次优专家**：不假设专家完美最优\n",
    "2. **概率框架**：提供不确定性估计\n",
    "3. **最大熵原则**：在满足约束的分布中选择熵最大的（最小偏见）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_maxent_irl_concept():\n",
    "    \"\"\"可视化最大熵IRL的概念。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 左图：轨迹概率分布\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # 模拟不同奖励下的轨迹概率\n",
    "    trajectory_rewards = np.linspace(-5, 5, 100)\n",
    "    temperatures = [0.5, 1.0, 2.0]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        probs = np.exp(trajectory_rewards / temp)\n",
    "        probs = probs / probs.sum()\n",
    "        ax.plot(trajectory_rewards, probs, label=f'T = {temp}')\n",
    "    \n",
    "    ax.set_xlabel('Trajectory Reward $R(\\\\tau)$')\n",
    "    ax.set_ylabel('$P(\\\\tau | \\\\theta)$')\n",
    "    ax.set_title('Boltzmann Distribution over Trajectories')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 中图：特征匹配\n",
    "    ax = axes[1]\n",
    "    \n",
    "    iterations = range(50)\n",
    "    expert_feature = 2.5\n",
    "    model_features = [0.5 + 2 * (1 - np.exp(-i/10)) for i in iterations]\n",
    "    \n",
    "    ax.axhline(y=expert_feature, color='r', linestyle='--', label='Expert $\\mu_E$')\n",
    "    ax.plot(iterations, model_features, 'b-', label='Model $\\mu_\\\\theta$')\n",
    "    ax.fill_between(iterations, model_features, expert_feature, alpha=0.3)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Feature Expectation')\n",
    "    ax.set_title('Feature Matching in MaxEnt IRL')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 右图：梯度更新\n",
    "    ax = axes[2]\n",
    "    \n",
    "    # 梯度 = μ_E - μ_θ\n",
    "    gradients = [expert_feature - mf for mf in model_features]\n",
    "    ax.plot(iterations, gradients, 'g-', linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Gradient $\\\\nabla_\\\\theta \\mathcal{L}$')\n",
    "    ax.set_title('Gradient: $\\mu_E - \\mu_\\\\theta$')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"MaxEnt IRL 关键洞察:\")\n",
    "    print(\"1. 高奖励轨迹概率更高，但不是确定性的\")\n",
    "    print(\"2. 通过特征匹配来学习奖励权重\")\n",
    "    print(\"3. 梯度简单：专家特征期望 - 模型特征期望\")\n",
    "\n",
    "visualize_maxent_irl_concept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Max-Entropy IRL\n",
    "maxent_irl = MaxEntropyIRL(\n",
    "    config=config,\n",
    "    feature_extractor=feature_extractor,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "print(\"训练 Max-Entropy IRL...\")\n",
    "maxent_weights = maxent_irl.fit(demonstrations)\n",
    "\n",
    "print(f\"\\n学习到的奖励权重:\")\n",
    "print(f\"  范围: [{maxent_weights.min():.4f}, {maxent_weights.max():.4f}]\")\n",
    "\n",
    "# 对比两种方法的权重\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(len(reward_weights)), reward_weights, alpha=0.7)\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Weight')\n",
    "axes[0].set_title('Max-Margin IRL Weights')\n",
    "\n",
    "axes[1].bar(range(len(maxent_weights)), maxent_weights, alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].set_title('Max-Entropy IRL Weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 深度IRL与GAIL\n",
    "\n",
    "### 5.1 深度IRL\n",
    "\n",
    "用神经网络替代线性奖励函数：\n",
    "\n",
    "$$R_\\theta(s) = f_\\theta(s)$$\n",
    "\n",
    "可以学习任意复杂的奖励函数。\n",
    "\n",
    "### 5.2 GAIL (Generative Adversarial Imitation Learning)\n",
    "\n",
    "将IRL视为生成对抗网络：\n",
    "\n",
    "- **判别器 D**: 区分专家和策略生成的状态-动作对\n",
    "- **生成器 π**: 策略网络，试图欺骗判别器\n",
    "\n",
    "损失函数：\n",
    "$$\\min_\\pi \\max_D \\mathbb{E}_{\\pi_E}[\\log D(s,a)] + \\mathbb{E}_\\pi[\\log(1-D(s,a))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gail_concept():\n",
    "    \"\"\"可视化GAIL的对抗训练过程。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # 模拟训练过程\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 专家数据分布（固定）\n",
    "    expert_mean = np.array([2.0, 2.0])\n",
    "    expert_cov = np.array([[0.3, 0.1], [0.1, 0.3]])\n",
    "    expert_data = np.random.multivariate_normal(expert_mean, expert_cov, 200)\n",
    "    \n",
    "    # 策略数据分布（逐渐接近专家）\n",
    "    policy_means = [\n",
    "        np.array([0.0, 0.0]),\n",
    "        np.array([1.0, 1.0]),\n",
    "        np.array([1.8, 1.8]),\n",
    "    ]\n",
    "    \n",
    "    for idx, (ax_row, policy_mean) in enumerate(zip(axes.T, policy_means)):\n",
    "        policy_data = np.random.multivariate_normal(policy_mean, expert_cov, 200)\n",
    "        \n",
    "        # 上排：数据分布\n",
    "        ax = ax_row[0]\n",
    "        ax.scatter(expert_data[:, 0], expert_data[:, 1], c='red', alpha=0.5, \n",
    "                   label='Expert', s=20)\n",
    "        ax.scatter(policy_data[:, 0], policy_data[:, 1], c='blue', alpha=0.5, \n",
    "                   label='Policy', s=20)\n",
    "        ax.set_xlabel('$s_1$')\n",
    "        ax.set_ylabel('$s_2$')\n",
    "        ax.set_title(f'Iteration {idx * 50}')\n",
    "        ax.legend()\n",
    "        ax.set_xlim(-2, 4)\n",
    "        ax.set_ylim(-2, 4)\n",
    "        \n",
    "        # 下排：判别器决策边界\n",
    "        ax = ax_row[1]\n",
    "        \n",
    "        # 模拟判别器输出\n",
    "        x = np.linspace(-2, 4, 50)\n",
    "        y = np.linspace(-2, 4, 50)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # 简化的判别器决策边界\n",
    "        D = 1 / (1 + np.exp(-2 * (np.sqrt((X - expert_mean[0])**2 + \n",
    "                                          (Y - expert_mean[1])**2) - \n",
    "                                  np.sqrt((X - policy_mean[0])**2 + \n",
    "                                          (Y - policy_mean[1])**2))))\n",
    "        \n",
    "        contour = ax.contourf(X, Y, D, levels=20, cmap='RdBu_r', alpha=0.7)\n",
    "        ax.contour(X, Y, D, levels=[0.5], colors='black', linewidths=2)\n",
    "        ax.set_xlabel('$s_1$')\n",
    "        ax.set_ylabel('$s_2$')\n",
    "        ax.set_title(f'Discriminator $D(s)$')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GAIL训练过程:\")\n",
    "    print(\"1. 判别器学习区分专家（红）和策略（蓝）\")\n",
    "    print(\"2. 策略学习欺骗判别器，生成类似专家的行为\")\n",
    "    print(\"3. 最终两者分布重合，判别器无法区分\")\n",
    "\n",
    "visualize_gail_concept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试GAIL判别器\n",
    "action_dim = 2\n",
    "\n",
    "discriminator = GAILDiscriminator(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dims=(64, 64),\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "# 生成专家和策略数据\n",
    "batch_size = 64\n",
    "\n",
    "# 专家数据（来自演示）\n",
    "expert_states = demonstrations[0].states[:batch_size]\n",
    "expert_actions = np.random.randn(batch_size, action_dim)\n",
    "\n",
    "# 策略数据（随机）\n",
    "policy_states = np.random.randn(batch_size, state_dim)\n",
    "policy_actions = np.random.randn(batch_size, action_dim)\n",
    "\n",
    "# 训练判别器\n",
    "print(\"训练GAIL判别器...\")\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    stats = discriminator.update(\n",
    "        expert_states, expert_actions,\n",
    "        policy_states, policy_actions,\n",
    "    )\n",
    "    losses.append(stats['total_loss'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Update Step')\n",
    "plt.ylabel('Discriminator Loss')\n",
    "plt.title('GAIL Discriminator Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 测试判别器输出\n",
    "expert_d = discriminator.forward(expert_states[:10], expert_actions[:10])\n",
    "policy_d = discriminator.forward(policy_states[:10], policy_actions[:10])\n",
    "\n",
    "print(f\"\\n专家状态判别器输出（应接近1）: {expert_d.mean():.4f}\")\n",
    "print(f\"策略状态判别器输出（应接近0）: {policy_d.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 实验与对比\n",
    "\n",
    "### 6.1 方法对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_irl_methods():\n",
    "    \"\"\"对比不同IRL方法。\"\"\"\n",
    "    \n",
    "    # 使用相同的演示数据\n",
    "    methods = {\n",
    "        'Max-Margin': MaxMarginIRL(config=config, feature_extractor=feature_extractor),\n",
    "        'Max-Entropy': MaxEntropyIRL(config=config, feature_extractor=feature_extractor),\n",
    "        'Deep IRL': DeepIRL(state_dim=state_dim, hidden_dims=(32, 32), config=config),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, irl in methods.items():\n",
    "        print(f\"训练 {name}...\")\n",
    "        weights = irl.fit(demonstrations)\n",
    "        \n",
    "        # 计算专家状态的奖励\n",
    "        expert_rewards = []\n",
    "        for demo in demonstrations[:5]:\n",
    "            for state in demo.states[:10]:\n",
    "                r = irl.compute_reward(state)\n",
    "                expert_rewards.append(r)\n",
    "        \n",
    "        # 计算随机状态的奖励\n",
    "        random_rewards = []\n",
    "        for _ in range(50):\n",
    "            state = np.random.randn(state_dim) * 2\n",
    "            r = irl.compute_reward(state)\n",
    "            random_rewards.append(r)\n",
    "        \n",
    "        results[name] = {\n",
    "            'expert_mean': np.mean(expert_rewards),\n",
    "            'expert_std': np.std(expert_rewards),\n",
    "            'random_mean': np.mean(random_rewards),\n",
    "            'random_std': np.std(random_rewards),\n",
    "            'gap': np.mean(expert_rewards) - np.mean(random_rewards),\n",
    "        }\n",
    "    \n",
    "    # 可视化结果\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    expert_means = [results[m]['expert_mean'] for m in methods]\n",
    "    expert_stds = [results[m]['expert_std'] for m in methods]\n",
    "    random_means = [results[m]['random_mean'] for m in methods]\n",
    "    random_stds = [results[m]['random_std'] for m in methods]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, expert_means, width, yerr=expert_stds, \n",
    "                   label='Expert States', capsize=5)\n",
    "    bars2 = ax.bar(x + width/2, random_means, width, yerr=random_stds,\n",
    "                   label='Random States', capsize=5)\n",
    "    \n",
    "    ax.set_ylabel('Learned Reward')\n",
    "    ax.set_title('IRL Methods: Expert vs Random State Rewards')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods.keys())\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印详细结果\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"IRL方法对比结果\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'方法':<15} {'专家奖励':<15} {'随机奖励':<15} {'差距':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, res in results.items():\n",
    "        print(f\"{name:<15} {res['expert_mean']:>+.4f}±{res['expert_std']:.4f}  \"\n",
    "              f\"{res['random_mean']:>+.4f}±{res['random_std']:.4f}  {res['gap']:>+.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = compare_irl_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 特征匹配分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_matching():\n",
    "    \"\"\"分析IRL的特征匹配效果。\"\"\"\n",
    "    \n",
    "    # 计算专家特征期望\n",
    "    expert_features = maxent_irl.compute_feature_expectations(demonstrations)\n",
    "    \n",
    "    # 模拟不同策略的特征期望\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    policy_features_list = []\n",
    "    policy_names = ['Random', 'Semi-Optimal', 'Near-Optimal']\n",
    "    \n",
    "    # 随机策略\n",
    "    random_features = np.random.randn(feature_dim) * 0.5\n",
    "    policy_features_list.append(random_features)\n",
    "    \n",
    "    # 半优策略\n",
    "    semi_features = expert_features * 0.6 + np.random.randn(feature_dim) * 0.2\n",
    "    policy_features_list.append(semi_features)\n",
    "    \n",
    "    # 近优策略\n",
    "    near_features = expert_features * 0.95 + np.random.randn(feature_dim) * 0.05\n",
    "    policy_features_list.append(near_features)\n",
    "    \n",
    "    # 计算特征匹配损失\n",
    "    losses = []\n",
    "    for pf in policy_features_list:\n",
    "        loss = compute_feature_matching_loss(expert_features, pf, 'l2')\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # 可视化\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 左图：特征期望对比\n",
    "    ax = axes[0]\n",
    "    x = np.arange(feature_dim)\n",
    "    width = 0.2\n",
    "    \n",
    "    ax.bar(x - 1.5*width, expert_features, width, label='Expert', alpha=0.8)\n",
    "    for i, (pf, name) in enumerate(zip(policy_features_list, policy_names)):\n",
    "        ax.bar(x + (i-0.5)*width, pf, width, label=name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Feature Expectation')\n",
    "    ax.set_title('Feature Expectations: Expert vs Policies')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 右图：特征匹配损失\n",
    "    ax = axes[1]\n",
    "    bars = ax.bar(policy_names, losses, color=['red', 'orange', 'green'])\n",
    "    ax.set_ylabel('Feature Matching Loss')\n",
    "    ax.set_title('Feature Matching Loss (Lower = Better)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, loss in zip(bars, losses):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{loss:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_feature_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结与最佳实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                         逆强化学习核心总结                                   ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  1. 问题本质                                                                 ║\n",
    "║     • 从行为推断意图/奖励                                                    ║\n",
    "║     • 解决奖励函数设计困难的问题                                             ║\n",
    "║     • 内在模糊性需要额外约束解决                                             ║\n",
    "║                                                                              ║\n",
    "║  2. 方法选择                                                                 ║\n",
    "║     • Max-Margin: 假设专家最优，几何直觉强                                   ║\n",
    "║     • Max-Entropy: 概率框架，处理次优专家                                    ║\n",
    "║     • Deep IRL: 复杂奖励，需要更多数据                                       ║\n",
    "║     • GAIL: 不显式恢复奖励，直接模仿                                         ║\n",
    "║                                                                              ║\n",
    "║  3. 实践建议                                                                 ║\n",
    "║     • 特征设计很重要（领域知识）                                             ║\n",
    "║     • 演示质量 > 演示数量                                                    ║\n",
    "║     • 验证学习到的奖励是否合理                                               ║\n",
    "║     • 考虑可解释性需求                                                       ║\n",
    "║                                                                              ║\n",
    "║  4. 常见陷阱                                                                 ║\n",
    "║     • 奖励退化（R=0 everywhere）                                             ║\n",
    "║     • 过拟合演示噪声                                                         ║\n",
    "║     • 分布偏移（covariate shift）                                            ║\n",
    "║                                                                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习题\n",
    "\n",
    "1. **理论题**：证明当折扣因子 γ→1 时，MaxEnt IRL 的解等价于行为克隆。\n",
    "\n",
    "2. **实现题**：实现一个简单的 Bayesian IRL，使用 MCMC 采样奖励后验。\n",
    "\n",
    "3. **实验题**：比较 GAIL 和 Behavioral Cloning 在不同演示数量下的性能。\n",
    "\n",
    "4. **分析题**：设计实验研究 IRL 对演示噪声（次优专家）的鲁棒性。\n",
    "\n",
    "5. **应用题**：使用 IRL 从驾驶数据中学习奖励函数，分析学习到的奖励组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "1. Ng, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. ICML.\n",
    "\n",
    "2. Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. ICML.\n",
    "\n",
    "3. Ziebart, B. D., et al. (2008). Maximum entropy inverse reinforcement learning. AAAI.\n",
    "\n",
    "4. Ho, J., & Ermon, S. (2016). Generative adversarial imitation learning. NeurIPS.\n",
    "\n",
    "5. Fu, J., Luo, K., & Levine, S. (2018). Learning robust rewards with adversarial inverse reinforcement learning. ICLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
