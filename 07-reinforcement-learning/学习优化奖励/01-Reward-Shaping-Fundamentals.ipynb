{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 奖励塑形 (Reward Shaping) 深度教程\n",
    "\n",
    "## 目录\n",
    "1. [问题背景与动机](#1-问题背景与动机)\n",
    "2. [理论基础：势能函数与策略不变性](#2-理论基础势能函数与策略不变性)\n",
    "3. [核心算法实现](#3-核心算法实现)\n",
    "4. [实验：GridWorld导航](#4-实验gridworld导航)\n",
    "5. [自适应奖励塑形](#5-自适应奖励塑形)\n",
    "6. [与其他方法对比](#6-与其他方法对比)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 问题背景与动机\n",
    "\n",
    "### 1.1 稀疏奖励问题\n",
    "\n",
    "在许多强化学习任务中，智能体只有在达成最终目标时才能获得奖励。例如：\n",
    "- **机器人导航**：只有到达目标位置才获得+1奖励\n",
    "- **游戏通关**：只有击败Boss才获得奖励\n",
    "- **机械臂操作**：只有成功抓取物体才获得奖励\n",
    "\n",
    "这导致了**信用分配问题** (Credit Assignment Problem)：智能体难以判断哪些动作导致了最终的成功或失败。\n",
    "\n",
    "### 1.2 朴素奖励塑形的陷阱\n",
    "\n",
    "一个自然的想法是手动设计额外的奖励信号。但这可能导致**策略偏移**：智能体学会最大化塑形奖励而非真正的目标。\n",
    "\n",
    "**经典案例**：在船只竞速游戏中，研究者给予智能体收集金币的额外奖励。结果智能体学会了原地打转收集金币，而不是完成比赛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 理论基础：势能函数与策略不变性\n",
    "\n",
    "### 2.1 Ng-Harada-Russell定理 (1999)\n",
    "\n",
    "**核心问题**：如何设计奖励塑形函数，使得最优策略保持不变？\n",
    "\n",
    "**定理 (Policy Invariance)**：对于MDP $M = (S, A, P, R, \\gamma)$，设 $M' = (S, A, P, R', \\gamma)$ 为塑形后的MDP，其中：\n",
    "\n",
    "$$R'(s, a, s') = R(s, a, s') + F(s, a, s')$$\n",
    "\n",
    "当且仅当存在势能函数 $\\Phi: S \\rightarrow \\mathbb{R}$，使得：\n",
    "\n",
    "$$F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)$$\n",
    "\n",
    "时，$M$ 和 $M'$ 具有相同的最优策略集合。\n",
    "\n",
    "### 2.2 直觉理解\n",
    "\n",
    "势能函数 $\\Phi(s)$ 衡量状态的\"潜力\"或\"价值预估\"。塑形奖励本质上是：\n",
    "\n",
    "> **进入高势能状态获得奖励，离开高势能状态受到惩罚**\n",
    "\n",
    "关键洞察：由于 $\\gamma < 1$，沿任意轨迹的塑形奖励之和为有限值，不会无限累积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_pbrs_theory():\n",
    "    \"\"\"可视化势能函数与奖励塑形的关系。\"\"\"\n",
    "    \n",
    "    # 一维状态空间示例\n",
    "    states = np.linspace(0, 10, 100)\n",
    "    goal = 10.0\n",
    "    \n",
    "    # 势能函数：负距离（越近目标势能越高）\n",
    "    potential = -np.abs(states - goal)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 图1: 势能函数\n",
    "    axes[0].plot(states, potential, 'b-', linewidth=2)\n",
    "    axes[0].axvline(x=goal, color='r', linestyle='--', label='Goal')\n",
    "    axes[0].set_xlabel('State (Position)')\n",
    "    axes[0].set_ylabel('$\\Phi(s)$')\n",
    "    axes[0].set_title('Potential Function')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 图2: 塑形奖励（向目标移动）\n",
    "    gamma = 0.99\n",
    "    # 假设从每个状态向右移动一步\n",
    "    shaping_bonus = gamma * potential[1:] - potential[:-1]\n",
    "    axes[1].plot(states[:-1], shaping_bonus, 'g-', linewidth=2)\n",
    "    axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1].set_xlabel('Current State')\n",
    "    axes[1].set_ylabel('$F(s, s\\')$')\n",
    "    axes[1].set_title('Shaping Bonus (Moving Right)')\n",
    "    \n",
    "    # 图3: 轨迹上的累积塑形奖励\n",
    "    trajectory_lengths = range(1, 50)\n",
    "    cumulative_shaping = []\n",
    "    \n",
    "    for T in trajectory_lengths:\n",
    "        # 从状态0出发，每步向右移动\n",
    "        total = sum(gamma**t * (gamma * potential[min(t+1, 99)] - potential[min(t, 99)]) \n",
    "                   for t in range(T))\n",
    "        cumulative_shaping.append(total)\n",
    "    \n",
    "    axes[2].plot(trajectory_lengths, cumulative_shaping, 'purple', linewidth=2)\n",
    "    axes[2].set_xlabel('Trajectory Length')\n",
    "    axes[2].set_ylabel('Cumulative Shaping')\n",
    "    axes[2].set_title('Total Shaping Reward (Bounded!)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 理论分析\n",
    "    print(\"=\" * 60)\n",
    "    print(\"理论分析：为什么PBRS保证策略不变性？\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\"\"  \n",
    "对于任意轨迹 τ = (s₀, a₀, s₁, a₁, ..., sₜ)，累积塑形奖励为：\n",
    "\n",
    "  Σ γᵗ F(sₜ, aₜ, sₜ₊₁) = Σ γᵗ [γΦ(sₜ₊₁) - Φ(sₜ)]\n",
    "                        = γΦ(s₁) - Φ(s₀) + γ²Φ(s₂) - γΦ(s₁) + ...\n",
    "                        = γᵀΦ(sₜ) - Φ(s₀)  (望远镜求和)\n",
    "\n",
    "由于 |Φ(s)| 有界，累积塑形奖励有界。\n",
    "因此，最大化 R + F 等价于最大化 R（对于足够长的horizon）。\n",
    "    \"\"\")\n",
    "\n",
    "demonstrate_pbrs_theory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 核心算法实现\n",
    "\n",
    "### 3.1 基于距离的势能函数\n",
    "\n",
    "对于目标到达任务，最自然的势能函数是负距离：\n",
    "\n",
    "$$\\Phi(s) = -\\|s_{pos} - g\\|_p$$\n",
    "\n",
    "其中 $g$ 是目标位置，$\\|\\cdot\\|_p$ 是 $L_p$ 范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_shaping import (\n",
    "    ShapedRewardConfig,\n",
    "    DistanceBasedShaper,\n",
    "    SubgoalBasedShaper,\n",
    "    AdaptiveRewardShaper,\n",
    "    DynamicShapingConfig,\n",
    ")\n",
    "\n",
    "# 创建基于距离的奖励塑形器\n",
    "goal = np.array([10.0, 10.0])\n",
    "config = ShapedRewardConfig(\n",
    "    discount_factor=0.99,\n",
    "    shaping_weight=1.0,\n",
    ")\n",
    "\n",
    "shaper = DistanceBasedShaper(\n",
    "    goal_position=goal,\n",
    "    norm_order=2,  # 欧几里得距离\n",
    "    scale=1.0,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# 演示势能函数\n",
    "test_states = [\n",
    "    np.array([0.0, 0.0]),    # 远离目标\n",
    "    np.array([5.0, 5.0]),    # 中间位置\n",
    "    np.array([9.0, 9.0]),    # 接近目标\n",
    "    np.array([10.0, 10.0]),  # 目标位置\n",
    "]\n",
    "\n",
    "print(\"状态势能值示例：\")\n",
    "print(\"-\" * 50)\n",
    "for state in test_states:\n",
    "    potential = shaper.potential(state)\n",
    "    distance = np.linalg.norm(state - goal)\n",
    "    print(f\"状态 {state} -> 距离: {distance:.2f}, 势能: {potential:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 可视化势能场"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_potential_field(shaper, goal, grid_size=50):\n",
    "    \"\"\"可视化2D势能场。\"\"\"\n",
    "    \n",
    "    x = np.linspace(-2, 12, grid_size)\n",
    "    y = np.linspace(-2, 12, grid_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 计算每个点的势能\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state = np.array([X[i, j], Y[i, j]])\n",
    "            Z[i, j] = shaper.potential(state)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 等高线图\n",
    "    contour = axes[0].contourf(X, Y, Z, levels=20, cmap='viridis')\n",
    "    axes[0].plot(goal[0], goal[1], 'r*', markersize=20, label='Goal')\n",
    "    axes[0].set_xlabel('X')\n",
    "    axes[0].set_ylabel('Y')\n",
    "    axes[0].set_title('Potential Function $\\Phi(s)$')\n",
    "    axes[0].legend()\n",
    "    plt.colorbar(contour, ax=axes[0])\n",
    "    \n",
    "    # 梯度场（指向势能增加方向）\n",
    "    # 计算梯度\n",
    "    grad_y, grad_x = np.gradient(Z, y[1]-y[0], x[1]-x[0])\n",
    "    \n",
    "    # 子采样以便可视化\n",
    "    skip = 3\n",
    "    axes[1].quiver(\n",
    "        X[::skip, ::skip], Y[::skip, ::skip],\n",
    "        grad_x[::skip, ::skip], grad_y[::skip, ::skip],\n",
    "        scale=50, alpha=0.7\n",
    "    )\n",
    "    axes[1].plot(goal[0], goal[1], 'r*', markersize=20, label='Goal')\n",
    "    axes[1].set_xlabel('X')\n",
    "    axes[1].set_ylabel('Y')\n",
    "    axes[1].set_title('Gradient Field $\\\\nabla\\Phi(s)$ (Direction of Positive Shaping)')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlim(-2, 12)\n",
    "    axes[1].set_ylim(-2, 12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_potential_field(shaper, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实验：GridWorld导航\n",
    "\n",
    "### 4.1 环境定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv:\n",
    "    \"\"\"简单的GridWorld环境用于演示奖励塑形。\n",
    "    \n",
    "    智能体需要从起点导航到目标位置，可以设置障碍物。\n",
    "    \"\"\"\n",
    "    \n",
    "    ACTIONS = {\n",
    "        0: np.array([-1, 0]),   # 上\n",
    "        1: np.array([1, 0]),    # 下\n",
    "        2: np.array([0, -1]),   # 左\n",
    "        3: np.array([0, 1]),    # 右\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 10,\n",
    "        start: Tuple[int, int] = (0, 0),\n",
    "        goal: Tuple[int, int] = (9, 9),\n",
    "        obstacles: Optional[List[Tuple[int, int]]] = None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.start = np.array(start)\n",
    "        self.goal = np.array(goal)\n",
    "        self.obstacles = set(obstacles) if obstacles else set()\n",
    "        \n",
    "        self.state = self.start.copy()\n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.state = self.start.copy()\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"执行动作，返回(next_state, reward, done, info)。\"\"\"\n",
    "        \n",
    "        # 计算新位置\n",
    "        new_state = self.state + self.ACTIONS[action]\n",
    "        \n",
    "        # 边界检查\n",
    "        new_state = np.clip(new_state, 0, self.size - 1)\n",
    "        \n",
    "        # 障碍物检查\n",
    "        if tuple(new_state) not in self.obstacles:\n",
    "            self.state = new_state\n",
    "        \n",
    "        # 检查是否到达目标\n",
    "        done = np.array_equal(self.state, self.goal)\n",
    "        \n",
    "        # 稀疏奖励：只有到达目标才有奖励\n",
    "        reward = 1.0 if done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done, {}\n",
    "    \n",
    "    def render(self, ax=None, trajectory=None):\n",
    "        \"\"\"可视化环境。\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # 绘制网格\n",
    "        ax.set_xlim(-0.5, self.size - 0.5)\n",
    "        ax.set_ylim(-0.5, self.size - 0.5)\n",
    "        ax.set_xticks(range(self.size))\n",
    "        ax.set_yticks(range(self.size))\n",
    "        ax.grid(True)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        # 绘制障碍物\n",
    "        for obs in self.obstacles:\n",
    "            ax.add_patch(plt.Rectangle(\n",
    "                (obs[1] - 0.5, obs[0] - 0.5), 1, 1,\n",
    "                facecolor='gray', edgecolor='black'\n",
    "            ))\n",
    "        \n",
    "        # 绘制起点和终点\n",
    "        ax.plot(self.start[1], self.start[0], 'go', markersize=15, label='Start')\n",
    "        ax.plot(self.goal[1], self.goal[0], 'r*', markersize=20, label='Goal')\n",
    "        \n",
    "        # 绘制轨迹\n",
    "        if trajectory is not None:\n",
    "            traj = np.array(trajectory)\n",
    "            ax.plot(traj[:, 1], traj[:, 0], 'b-', linewidth=2, alpha=0.7)\n",
    "            ax.plot(traj[-1, 1], traj[-1, 0], 'bs', markersize=10, label='Agent')\n",
    "        else:\n",
    "            ax.plot(self.state[1], self.state[0], 'bs', markersize=10, label='Agent')\n",
    "        \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.invert_yaxis()  # 使(0,0)在左上角\n",
    "        return ax\n",
    "\n",
    "# 创建带障碍物的环境\n",
    "obstacles = [(3, i) for i in range(7)] + [(6, i) for i in range(3, 10)]\n",
    "env = GridWorldEnv(size=10, obstacles=obstacles)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "env.render(ax)\n",
    "ax.set_title('GridWorld Environment with Obstacles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Q-Learning with and without Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"带奖励塑形的Q-Learning智能体。\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 0.1,\n",
    "        reward_shaper: Optional[DistanceBasedShaper] = None,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_shaper = reward_shaper\n",
    "        \n",
    "        # Q表：(state_size, state_size, n_actions)\n",
    "        self.q_table = np.zeros((state_size, state_size, n_actions))\n",
    "        \n",
    "    def get_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"ε-贪婪动作选择。\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        return np.argmax(self.q_table[state[0], state[1]])\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ) -> float:\n",
    "        \"\"\"Q-Learning更新，可选奖励塑形。\"\"\"\n",
    "        \n",
    "        # 应用奖励塑形\n",
    "        shaped_reward = reward\n",
    "        if self.reward_shaper is not None:\n",
    "            shaping_bonus = self.reward_shaper.compute_shaping_bonus(\n",
    "                state.astype(float),\n",
    "                next_state.astype(float),\n",
    "                done,\n",
    "            )\n",
    "            shaped_reward = reward + shaping_bonus\n",
    "        \n",
    "        # Q-Learning更新\n",
    "        current_q = self.q_table[state[0], state[1], action]\n",
    "        \n",
    "        if done:\n",
    "            target = shaped_reward\n",
    "        else:\n",
    "            next_q_max = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "            target = shaped_reward + self.gamma * next_q_max\n",
    "        \n",
    "        self.q_table[state[0], state[1], action] += self.lr * (target - current_q)\n",
    "        \n",
    "        return shaped_reward\n",
    "\n",
    "\n",
    "def train_agent(\n",
    "    env: GridWorldEnv,\n",
    "    agent: QLearningAgent,\n",
    "    n_episodes: int = 500,\n",
    "    max_steps: int = 200,\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"训练智能体并记录统计数据。\"\"\"\n",
    "    \n",
    "    history = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_lengths': [],\n",
    "        'success_rate': [],\n",
    "    }\n",
    "    \n",
    "    successes = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            shaped_reward = agent.update(state, action, reward, next_state, done)\n",
    "            total_reward += reward  # 记录原始奖励\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        history['episode_rewards'].append(total_reward)\n",
    "        history['episode_lengths'].append(step + 1)\n",
    "        successes.append(1 if done else 0)\n",
    "        \n",
    "        # 计算滑动窗口成功率\n",
    "        window = 50\n",
    "        if len(successes) >= window:\n",
    "            history['success_rate'].append(np.mean(successes[-window:]))\n",
    "        else:\n",
    "            history['success_rate'].append(np.mean(successes))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练两个智能体：有无奖励塑形\n",
    "env = GridWorldEnv(size=10, obstacles=obstacles)\n",
    "\n",
    "# 不使用奖励塑形\n",
    "agent_no_shaping = QLearningAgent(\n",
    "    state_size=10,\n",
    "    n_actions=4,\n",
    "    learning_rate=0.2,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.2,\n",
    "    reward_shaper=None,\n",
    ")\n",
    "\n",
    "# 使用距离基奖励塑形\n",
    "shaper = DistanceBasedShaper(\n",
    "    goal_position=np.array([9.0, 9.0]),\n",
    "    scale=0.1,  # 缩放塑形奖励\n",
    "    config=ShapedRewardConfig(discount_factor=0.99, shaping_weight=1.0),\n",
    ")\n",
    "\n",
    "agent_with_shaping = QLearningAgent(\n",
    "    state_size=10,\n",
    "    n_actions=4,\n",
    "    learning_rate=0.2,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.2,\n",
    "    reward_shaper=shaper,\n",
    ")\n",
    "\n",
    "print(\"训练无奖励塑形的智能体...\")\n",
    "history_no_shaping = train_agent(env, agent_no_shaping, n_episodes=500)\n",
    "\n",
    "print(\"训练有奖励塑形的智能体...\")\n",
    "history_with_shaping = train_agent(env, agent_with_shaping, n_episodes=500)\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_comparison(history_no_shaping, history_with_shaping):\n",
    "    \"\"\"对比训练曲线。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 成功率\n",
    "    axes[0].plot(history_no_shaping['success_rate'], label='No Shaping', alpha=0.8)\n",
    "    axes[0].plot(history_with_shaping['success_rate'], label='With PBRS', alpha=0.8)\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Success Rate')\n",
    "    axes[0].set_title('Learning Curve: Success Rate')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode长度\n",
    "    window = 20\n",
    "    lengths_no = np.convolve(history_no_shaping['episode_lengths'], \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "    lengths_with = np.convolve(history_with_shaping['episode_lengths'],\n",
    "                               np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1].plot(lengths_no, label='No Shaping', alpha=0.8)\n",
    "    axes[1].plot(lengths_with, label='With PBRS', alpha=0.8)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Episode Length')\n",
    "    axes[1].set_title('Learning Curve: Episode Length (Smoothed)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 累积奖励\n",
    "    cumsum_no = np.cumsum(history_no_shaping['episode_rewards'])\n",
    "    cumsum_with = np.cumsum(history_with_shaping['episode_rewards'])\n",
    "    \n",
    "    axes[2].plot(cumsum_no, label='No Shaping', alpha=0.8)\n",
    "    axes[2].plot(cumsum_with, label='With PBRS', alpha=0.8)\n",
    "    axes[2].set_xlabel('Episode')\n",
    "    axes[2].set_ylabel('Cumulative Reward')\n",
    "    axes[2].set_title('Cumulative Original Reward')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 统计摘要\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"训练结果对比\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n无奖励塑形:\")\n",
    "    print(f\"  最终成功率: {history_no_shaping['success_rate'][-1]:.2%}\")\n",
    "    print(f\"  首次达到80%成功率的episode: \", end=\"\")\n",
    "    idx = next((i for i, r in enumerate(history_no_shaping['success_rate']) if r >= 0.8), None)\n",
    "    print(f\"{idx}\" if idx else \"未达到\")\n",
    "    \n",
    "    print(f\"\\n有奖励塑形 (PBRS):\")\n",
    "    print(f\"  最终成功率: {history_with_shaping['success_rate'][-1]:.2%}\")\n",
    "    print(f\"  首次达到80%成功率的episode: \", end=\"\")\n",
    "    idx = next((i for i, r in enumerate(history_with_shaping['success_rate']) if r >= 0.8), None)\n",
    "    print(f\"{idx}\" if idx else \"未达到\")\n",
    "\n",
    "plot_training_comparison(history_no_shaping, history_with_shaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 可视化学习到的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(agent: QLearningAgent, env: GridWorldEnv, title: str):\n",
    "    \"\"\"可视化Q表对应的策略。\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 策略箭头图\n",
    "    ax = axes[0]\n",
    "    env.render(ax)\n",
    "    \n",
    "    # 动作到方向的映射（注意y轴反转）\n",
    "    action_to_arrow = {\n",
    "        0: (0, -0.3),   # 上 (y减少)\n",
    "        1: (0, 0.3),    # 下 (y增加)\n",
    "        2: (-0.3, 0),   # 左\n",
    "        3: (0.3, 0),    # 右\n",
    "    }\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) in env.obstacles:\n",
    "                continue\n",
    "            if np.array_equal([i, j], env.goal):\n",
    "                continue\n",
    "            \n",
    "            best_action = np.argmax(agent.q_table[i, j])\n",
    "            dx, dy = action_to_arrow[best_action]\n",
    "            ax.arrow(j, i, dx, dy, head_width=0.15, head_length=0.1,\n",
    "                    fc='blue', ec='blue', alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f'{title}: Learned Policy')\n",
    "    \n",
    "    # 价值函数热图\n",
    "    ax = axes[1]\n",
    "    value_map = np.max(agent.q_table, axis=2)\n",
    "    \n",
    "    # 遮盖障碍物\n",
    "    masked_value = np.ma.array(value_map)\n",
    "    for obs in env.obstacles:\n",
    "        masked_value[obs] = np.ma.masked\n",
    "    \n",
    "    im = ax.imshow(masked_value, cmap='hot', origin='upper')\n",
    "    ax.plot(env.goal[1], env.goal[0], 'g*', markersize=20)\n",
    "    ax.set_title(f'{title}: Value Function V(s) = max_a Q(s,a)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy(agent_no_shaping, env, \"Without Reward Shaping\")\n",
    "visualize_policy(agent_with_shaping, env, \"With PBRS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 轨迹演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env: GridWorldEnv, agent: QLearningAgent, max_steps: int = 100):\n",
    "    \"\"\"运行一个episode并返回轨迹。\"\"\"\n",
    "    state = env.reset()\n",
    "    trajectory = [state.copy()]\n",
    "    \n",
    "    # 使用贪婪策略\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        trajectory.append(next_state.copy())\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent.epsilon = original_epsilon\n",
    "    return trajectory\n",
    "\n",
    "# 比较两个智能体的轨迹\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "env_test = GridWorldEnv(size=10, obstacles=obstacles)\n",
    "\n",
    "traj_no_shaping = run_episode(env_test, agent_no_shaping)\n",
    "env_test.render(axes[0], trajectory=traj_no_shaping)\n",
    "axes[0].set_title(f'Without Shaping (Steps: {len(traj_no_shaping)-1})')\n",
    "\n",
    "traj_with_shaping = run_episode(env_test, agent_with_shaping)\n",
    "env_test.render(axes[1], trajectory=traj_with_shaping)\n",
    "axes[1].set_title(f'With PBRS (Steps: {len(traj_with_shaping)-1})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 自适应奖励塑形\n",
    "\n",
    "### 5.1 动机\n",
    "\n",
    "固定的塑形权重可能导致问题：\n",
    "- 权重过大：智能体可能过度依赖塑形奖励\n",
    "- 权重过小：塑形效果不明显\n",
    "\n",
    "**解决方案**：随着训练进行，逐渐减小塑形权重，让智能体最终完全依赖原始奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示自适应权重衰减\n",
    "base_shaper = DistanceBasedShaper(\n",
    "    goal_position=np.array([9.0, 9.0]),\n",
    "    scale=0.1,\n",
    ")\n",
    "\n",
    "adaptive_shaper = AdaptiveRewardShaper(\n",
    "    base_shaper=base_shaper,\n",
    "    dynamic_config=DynamicShapingConfig(\n",
    "        initial_weight=1.0,\n",
    "        decay_rate=0.999,\n",
    "        min_weight=0.01,\n",
    "        adaptation_method='exponential',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 模拟训练过程中的权重变化\n",
    "steps = 5000\n",
    "weights = []\n",
    "bonuses = []\n",
    "\n",
    "for _ in range(steps):\n",
    "    state = np.random.rand(2) * 10\n",
    "    next_state = state + np.random.randn(2) * 0.5\n",
    "    \n",
    "    bonus = adaptive_shaper.compute_shaping_bonus(state, next_state)\n",
    "    bonuses.append(bonus)\n",
    "    weights.append(adaptive_shaper.current_weight)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(weights)\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('Shaping Weight λ')\n",
    "axes[0].set_title('Adaptive Weight Decay')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 滑动平均\n",
    "window = 100\n",
    "smoothed_bonuses = np.convolve(np.abs(bonuses), np.ones(window)/window, mode='valid')\n",
    "axes[1].plot(smoothed_bonuses)\n",
    "axes[1].set_xlabel('Training Step')\n",
    "axes[1].set_ylabel('|Shaping Bonus|')\n",
    "axes[1].set_title('Absolute Shaping Bonus Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"初始权重: {weights[0]:.4f}\")\n",
    "print(f\"最终权重: {weights[-1]:.4f}\")\n",
    "print(f\"衰减比例: {weights[-1]/weights[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 与其他方法对比\n",
    "\n",
    "### 6.1 方法对比表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    '方法': ['PBRS', 'Naive Shaping', 'Curiosity (ICM)', 'HER', 'Curriculum'],\n",
    "    '策略不变性': ['✓ 保证', '✗ 不保证', '✗ 不保证', '✓ 保证', '部分保证'],\n",
    "    '需要领域知识': ['中等', '高', '低', '低', '中等'],\n",
    "    '适用空间': ['任意', '任意', '高维/连续', '目标条件', '任意'],\n",
    "    '计算开销': ['低', '低', '高', '中等', '中等'],\n",
    "    '典型应用': ['导航/控制', '简单任务', '视觉RL', '机器人操作', '复杂序列'],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df.set_index('方法', inplace=True)\n",
    "\n",
    "print(\"奖励优化方法对比\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 关键洞察总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                        奖励塑形核心洞察                                      ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  1. 理论保证                                                                 ║\n",
    "║     • PBRS是唯一保证策略不变性的奖励塑形形式                                 ║\n",
    "║     • 其他形式的塑形可能改变最优策略                                         ║\n",
    "║                                                                              ║\n",
    "║  2. 势能函数设计                                                             ║\n",
    "║     • 好的势能函数 ≈ 真实值函数的估计                                        ║\n",
    "║     • 常用选择：负距离、子目标进度、专家演示学习                             ║\n",
    "║                                                                              ║\n",
    "║  3. 实践建议                                                                 ║\n",
    "║     • 从简单的距离基势能开始                                                 ║\n",
    "║     • 使用自适应权重避免过度依赖塑形                                         ║\n",
    "║     • 监控原始奖励而非塑形奖励来评估性能                                     ║\n",
    "║                                                                              ║\n",
    "║  4. 局限性                                                                   ║\n",
    "║     • 需要领域知识设计势能函数                                               ║\n",
    "║     • 在迷宫等环境可能导致局部最优                                           ║\n",
    "║     • 不能替代好的探索策略                                                   ║\n",
    "║                                                                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习题\n",
    "\n",
    "1. **理论题**：证明对于无限horizon MDP，PBRS不改变状态值函数的相对排序。\n",
    "\n",
    "2. **实现题**：实现基于子目标的势能函数，用于多阶段导航任务。\n",
    "\n",
    "3. **实验题**：比较不同衰减策略（指数、线性、阶梯）对学习效率的影响。\n",
    "\n",
    "4. **分析题**：设计一个实验，验证非PBRS形式的奖励塑形如何改变最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "1. Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. ICML.\n",
    "\n",
    "2. Wiewiora, E. (2003). Potential-based shaping and Q-value initialization are equivalent. JAIR.\n",
    "\n",
    "3. Devlin, S., & Kudenko, D. (2012). Dynamic potential-based reward shaping. AAMAS.\n",
    "\n",
    "4. Brys, T., et al. (2015). Reinforcement learning from demonstration through shaping. IJCAI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
