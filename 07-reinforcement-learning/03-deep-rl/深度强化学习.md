# 深度强化学习 (Deep RL)

> 从 DQN 到现代深度强化学习算法

---

## 一、为什么需要深度强化学习？

### 1.1 表格型方法的局限

表格型 Q-Learning 面临的问题：

| 问题 | 说明 |
|------|------|
| **状态空间爆炸** | 围棋有 $10^{170}$ 种状态，无法存储 |
| **连续状态** | 机器人关节角度是连续值 |
| **无法泛化** | 每个状态独立学习，无法利用相似状态的知识 |
| **高维输入** | 图像有数百万像素，无法直接作为状态索引 |

### 1.2 函数近似

**核心思想**: 用参数化函数近似价值函数或策略

$$Q(s, a) \approx Q(s, a; \theta)$$

| 方法 | 特点 | 适用场景 |
|------|------|----------|
| 线性近似 | $Q = \phi(s, a)^T \theta$ | 特征工程明确 |
| **神经网络** | 自动特征提取 | 高维、复杂输入 |

---

## 二、DQN: 深度 Q 网络

### 2.1 历史背景

**2013年 DeepMind 里程碑**: 首次用深度学习在 Atari 游戏上达到人类水平

**突破点**: 直接从原始像素学习，无需人工特征

### 2.2 核心架构

```
┌─────────────────────────────────────────────────────────────────┐
│                         DQN 架构                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │ 游戏画面  │ →  │   卷积层      │ →  │   全连接层    │ → Q值   │
│  │ 84x84x4  │    │  (特征提取)   │    │   (价值估计)  │         │
│  └──────────┘    └──────────────┘    └──────────────┘          │
│                                                                 │
│  输入: 最近4帧     自动学习视觉特征    输出: 每个动作的Q值        │
│  的游戏画面                                                      │
└─────────────────────────────────────────────────────────────────┘
```

### 2.3 关键技术

#### 经验回放 (Experience Replay)

**问题**: 连续样本高度相关，违反 i.i.d. 假设

**解决方案**: 存储经验到缓冲区，随机采样训练

```python
from collections import deque
import random

class ReplayBuffer:
    """经验回放缓冲区"""

    def __init__(self, capacity: int = 100000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """存储经验"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        """随机采样批次"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )

    def __len__(self):
        return len(self.buffer)
```

**优点**:
- 打破样本相关性
- 提高数据利用效率
- 稳定训练过程

#### 目标网络 (Target Network)

**问题**: 更新 Q 网络时目标也在变化，导致不稳定

$$\text{Target} = R + \gamma \max_{a'} Q(S', a'; \theta)$$

**解决方案**: 使用固定的目标网络

```python
# 主网络: 用于选择动作和被更新
self.q_network = DQNNetwork()

# 目标网络: 用于计算目标值，定期同步
self.target_network = DQNNetwork()
self.target_network.load_state_dict(self.q_network.state_dict())

# 定期更新目标网络
def update_target_network(self):
    self.target_network.load_state_dict(self.q_network.state_dict())
```

### 2.4 DQN 算法

```
算法: Deep Q-Network (DQN)

初始化:
    - Q 网络 Q(s, a; θ)
    - 目标网络 Q̂(s, a; θ⁻) ← Q
    - 经验回放缓冲区 D

对于每个回合:
    初始化状态 s
    对于每一步:
        1. 使用 ε-greedy 选择动作 a
        2. 执行 a，观察 r, s'
        3. 存储 (s, a, r, s', done) 到 D
        4. 从 D 采样 mini-batch
        5. 计算目标: y = r + γ max_a' Q̂(s', a'; θ⁻)
        6. 梯度下降: θ ← θ - α∇_θ(y - Q(s, a; θ))²
        7. 每 C 步更新目标网络: θ⁻ ← θ
        8. s ← s'
```

### 2.5 PyTorch 实现

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

class DQNNetwork(nn.Module):
    """DQN 网络架构"""

    def __init__(self, input_shape, n_actions):
        super().__init__()

        # 卷积层 (用于图像输入)
        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        # 计算卷积输出大小
        conv_out_size = self._get_conv_out(input_shape)

        # 全连接层
        self.fc = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

    def _get_conv_out(self, shape):
        """计算卷积层输出大小"""
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        conv_out = self.conv(x).view(x.size(0), -1)
        return self.fc(conv_out)


class DQNAgent:
    """DQN 智能体"""

    def __init__(
        self,
        input_shape,
        n_actions,
        learning_rate=1e-4,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=100000,
        buffer_size=100000,
        batch_size=32,
        target_update_freq=1000,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.n_actions = n_actions
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        self.device = device

        # 网络
        self.q_network = DQNNetwork(input_shape, n_actions).to(device)
        self.target_network = DQNNetwork(input_shape, n_actions).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # 优化器
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # 经验回放
        self.replay_buffer = ReplayBuffer(buffer_size)

        # 计数器
        self.steps = 0

    def get_action(self, state, training=True):
        """ε-greedy 动作选择"""
        # 线性衰减 epsilon
        if training:
            self.epsilon = max(
                self.epsilon_end,
                1.0 - self.steps / self.epsilon_decay
            )

            if np.random.random() < self.epsilon:
                return np.random.randint(self.n_actions)

        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.q_network(state)
        return q_values.argmax(dim=1).item()

    def update(self, state, action, reward, next_state, done):
        """存储经验并训练"""
        self.replay_buffer.push(state, action, reward, next_state, done)
        self.steps += 1

        if len(self.replay_buffer) < self.batch_size:
            return

        # 采样批次
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(
            self.batch_size
        )

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # 计算当前 Q 值
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))

        # 计算目标 Q 值
        with torch.no_grad():
            next_q = self.target_network(next_states).max(1)[0]
            target_q = rewards + self.gamma * next_q * (1 - dones)

        # 计算损失并更新
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        self.optimizer.zero_grad()
        loss.backward()
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)
        self.optimizer.step()

        # 更新目标网络
        if self.steps % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        return loss.item()
```

---

## 三、DQN 改进算法

### 3.1 Double DQN

**问题**: 标准 DQN 的 max 操作导致过估计

**原因**: $\mathbb{E}[\max_a Q(s, a)] \geq \max_a \mathbb{E}[Q(s, a)]$

**解决方案**: 分离动作选择和评估

$$y = R + \gamma Q(S', \arg\max_{a'} Q(S', a'; \theta); \theta^-)$$

```python
# 标准 DQN
next_q = self.target_network(next_states).max(1)[0]

# Double DQN
next_actions = self.q_network(next_states).argmax(1)
next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
```

### 3.2 Dueling DQN

**思想**: 分解 Q 值为状态价值 V(s) 和优势函数 A(s, a)

$$Q(s, a) = V(s) + A(s, a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s, a')$$

```python
class DuelingDQN(nn.Module):
    """Dueling DQN 架构"""

    def __init__(self, input_shape, n_actions):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        conv_out_size = self._get_conv_out(input_shape)

        # 价值流
        self.value_stream = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

        # 优势流
        self.advantage_stream = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

    def forward(self, x):
        conv_out = self.conv(x).view(x.size(0), -1)

        value = self.value_stream(conv_out)
        advantage = self.advantage_stream(conv_out)

        # Q = V + (A - mean(A))
        q = value + advantage - advantage.mean(dim=1, keepdim=True)
        return q
```

### 3.3 Prioritized Experience Replay (PER)

**思想**: 优先采样 TD 误差大的经验

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

其中 $p_i = |\delta_i| + \epsilon$ 是优先级

```python
class PrioritizedReplayBuffer:
    """优先级经验回放"""

    def __init__(self, capacity, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha  # 优先级指数
        self.beta = beta    # 重要性采样指数
        self.buffer = []
        self.priorities = []
        self.position = 0

    def push(self, *experience):
        """添加经验，初始优先级设为最大"""
        max_priority = max(self.priorities) if self.priorities else 1.0

        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(max_priority)
        else:
            self.buffer[self.position] = experience
            self.priorities[self.position] = max_priority

        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        """按优先级采样"""
        priorities = np.array(self.priorities)
        probs = priorities ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)

        # 计算重要性采样权重
        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
        weights /= weights.max()

        samples = [self.buffer[i] for i in indices]
        return samples, indices, weights

    def update_priorities(self, indices, td_errors):
        """更新优先级"""
        for idx, td_error in zip(indices, td_errors):
            self.priorities[idx] = abs(td_error) + 1e-6
```

### 3.4 Rainbow DQN

**集成所有改进**:

| 组件 | 作用 |
|------|------|
| Double DQN | 减少过估计 |
| Dueling Network | 分离状态价值和优势 |
| PER | 高效采样 |
| Multi-step Returns | 平衡偏差和方差 |
| Distributional RL | 学习回报分布 |
| Noisy Networks | 探索机制 |

---

## 四、Actor-Critic 方法

### 4.1 基本思想

结合价值方法和策略方法的优点：

- **Actor (策略网络)**: 输出动作概率分布
- **Critic (价值网络)**: 评估状态/动作价值

```
┌───────────────────────────────────────────────────────┐
│                 Actor-Critic 架构                      │
├───────────────────────────────────────────────────────┤
│                                                       │
│     状态 s                                            │
│        │                                              │
│        ├──────────────┬───────────────┐              │
│        ↓              ↓               │              │
│    ┌───────┐     ┌───────┐           │              │
│    │ Actor │     │ Critic│           │              │
│    │ π(a|s)│     │ V(s)  │           │              │
│    └───┬───┘     └───┬───┘           │              │
│        │             │                │              │
│        ↓             ↓                │              │
│     动作 a       优势估计 A            │              │
│        │             │                │              │
│        ↓             ↓                │              │
│     环境交互 ←── 策略梯度更新 ←────────┘              │
│                                                       │
└───────────────────────────────────────────────────────┘
```

### 4.2 A2C (Advantage Actor-Critic)

**优势函数**:

$$A(s, a) = Q(s, a) - V(s) \approx R + \gamma V(s') - V(s)$$

**策略梯度**:

$$\nabla_\theta J(\theta) = \mathbb{E}_\pi[\nabla_\theta \log \pi(a|s; \theta) A(s, a)]$$

```python
class A2CNetwork(nn.Module):
    """A2C 共享特征网络"""

    def __init__(self, input_dim, n_actions):
        super().__init__()

        # 共享特征提取层
        self.shared = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )

        # Actor 头
        self.actor = nn.Linear(256, n_actions)

        # Critic 头
        self.critic = nn.Linear(256, 1)

    def forward(self, x):
        features = self.shared(x)
        policy = F.softmax(self.actor(features), dim=-1)
        value = self.critic(features)
        return policy, value


class A2CAgent:
    """A2C 智能体"""

    def __init__(self, input_dim, n_actions, lr=3e-4, gamma=0.99):
        self.gamma = gamma
        self.network = A2CNetwork(input_dim, n_actions)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        policy, _ = self.network(state)
        dist = torch.distributions.Categorical(policy)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def update(self, states, actions, rewards, next_states, dones, log_probs):
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        log_probs = torch.stack(log_probs)

        # 计算回报
        _, next_values = self.network(next_states)
        _, values = self.network(states)

        # TD 目标
        targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)

        # 优势
        advantages = targets - values.squeeze()

        # Actor 损失 (策略梯度)
        actor_loss = -(log_probs * advantages.detach()).mean()

        # Critic 损失 (价值函数)
        critic_loss = F.mse_loss(values.squeeze(), targets.detach())

        # 熵正则化 (鼓励探索)
        policy, _ = self.network(states)
        entropy = -(policy * policy.log()).sum(dim=-1).mean()

        # 总损失
        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()
```

---

## 五、PPO (Proximal Policy Optimization)

### 5.1 动机

策略梯度的问题：更新步长难以控制

- 步长太大：策略崩溃
- 步长太小：学习缓慢

### 5.2 核心思想

**限制策略更新幅度**:

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

**Clipped 目标函数**:

$$L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t\right)\right]$$

### 5.3 实现

```python
class PPOAgent:
    """PPO 智能体"""

    def __init__(
        self,
        input_dim,
        n_actions,
        lr=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.2,
        epochs=10,
        mini_batch_size=64
    ):
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.epochs = epochs
        self.mini_batch_size = mini_batch_size

        self.network = A2CNetwork(input_dim, n_actions)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

    def compute_gae(self, rewards, values, next_values, dones):
        """计算广义优势估计 (GAE)"""
        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)

        return torch.FloatTensor(advantages)

    def update(self, trajectories):
        """PPO 更新"""
        states = torch.FloatTensor(trajectories['states'])
        actions = torch.LongTensor(trajectories['actions'])
        old_log_probs = torch.FloatTensor(trajectories['log_probs'])
        rewards = trajectories['rewards']
        dones = trajectories['dones']

        # 计算旧的值和优势
        with torch.no_grad():
            old_policy, old_values = self.network(states)
            next_values = old_values[1:].squeeze().tolist() + [0]

        advantages = self.compute_gae(rewards, old_values.squeeze().tolist(), next_values, dones)
        returns = advantages + old_values.squeeze()

        # 标准化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # 多轮更新
        for _ in range(self.epochs):
            # Mini-batch 更新
            indices = np.random.permutation(len(states))

            for start in range(0, len(states), self.mini_batch_size):
                end = start + self.mini_batch_size
                batch_indices = indices[start:end]

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]

                # 计算新的策略和值
                policy, values = self.network(batch_states)
                dist = torch.distributions.Categorical(policy)
                new_log_probs = dist.log_prob(batch_actions)

                # 比率
                ratio = torch.exp(new_log_probs - batch_old_log_probs)

                # Clipped 目标
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()

                # Critic 损失
                critic_loss = F.mse_loss(values.squeeze(), batch_returns)

                # 熵
                entropy = dist.entropy().mean()

                # 总损失
                loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
                self.optimizer.step()
```

---

## 六、算法对比与选择

### 6.1 算法特性对比

| 算法 | 类型 | 样本效率 | 稳定性 | 适用场景 |
|------|------|----------|--------|----------|
| DQN | Off-Policy | 高 | 中 | 离散动作 |
| A2C | On-Policy | 低 | 中 | 简单任务 |
| PPO | On-Policy | 中 | 高 | 通用 |
| SAC | Off-Policy | 高 | 高 | 连续控制 |
| TD3 | Off-Policy | 高 | 高 | 连续控制 |

### 6.2 选择指南

```
开始
  │
  ├── 动作空间是离散的?
  │   ├── 是 → DQN 家族 (Double/Dueling/Rainbow)
  │   └── 否 → 连续控制算法
  │             ├── 需要最大熵?
  │             │   ├── 是 → SAC
  │             │   └── 否 → TD3 或 PPO
  │             └── 样本效率优先?
  │                 ├── 是 → SAC/TD3
  │                 └── 否 → PPO
```

---

## 七、实践建议

### 7.1 调参技巧

| 参数 | 建议范围 | 说明 |
|------|----------|------|
| 学习率 | 1e-4 ~ 3e-4 | 太大不稳定，太小收敛慢 |
| Batch Size | 32 ~ 256 | 视 GPU 内存而定 |
| γ (折扣因子) | 0.99 | 短期任务可用 0.95 |
| 目标网络更新 | 1000 ~ 10000 步 | 或软更新 τ=0.005 |
| ε 衰减 | 100k ~ 1M 步 | 视任务难度 |

### 7.2 常见问题

| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| 奖励不增长 | 探索不足 | 增大 ε，使用好奇心驱动 |
| 奖励波动大 | 学习率太大 | 降低学习率，增大 batch |
| 过拟合 | 样本多样性不足 | 增大 buffer，环境随机化 |
| 训练崩溃 | 梯度爆炸 | 梯度裁剪，降低学习率 |

### 7.3 推荐工具

- **Stable-Baselines3**: 高质量 RL 算法实现
- **RLlib**: 分布式 RL 框架
- **CleanRL**: 单文件、易读的实现
- **Gymnasium**: 标准环境接口

---

## 参考资料

1. Mnih et al., "Playing Atari with Deep Reinforcement Learning", 2013
2. Schulman et al., "Proximal Policy Optimization Algorithms", 2017
3. Hessel et al., "Rainbow: Combining Improvements in Deep RL", 2018

---

[返回上级](../README.md)
