{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度 Q 网络 (DQN) 实战教程\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "- 理解 DQN 解决的核心问题（表格方法的局限）\n",
    "- 掌握经验回放（Experience Replay）的原理与实现\n",
    "- 掌握目标网络（Target Network）的作用与实现\n",
    "- 实现完整的 DQN 算法\n",
    "- 理解 Double DQN 和 Dueling DQN 的改进\n",
    "- 在 CartPole 环境中训练和评估 DQN 智能体\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- Q-Learning 基础（贝尔曼方程、TD 误差）\n",
    "- PyTorch 神经网络基础\n",
    "- Python 面向对象编程\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "60-90 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第1部分：理论背景\n",
    "\n",
    "### 1.1 为什么需要深度强化学习？\n",
    "\n",
    "**表格型 Q-Learning 的局限**：\n",
    "\n",
    "| 问题 | 说明 |\n",
    "|------|------|\n",
    "| 状态空间爆炸 | 围棋有 $10^{170}$ 种状态，无法存储 |\n",
    "| 连续状态空间 | 机器人关节角度是连续值 |\n",
    "| 无法泛化 | 相似状态需要独立学习 |\n",
    "| 高维输入 | 图像有数百万像素 |\n",
    "\n",
    "**解决方案**：用神经网络逼近 Q 函数\n",
    "\n",
    "$$Q(s, a) \\approx Q(s, a; \\theta)$$\n",
    "\n",
    "### 1.2 DQN 的核心创新\n",
    "\n",
    "2013 年 DeepMind 提出 DQN，首次在 Atari 游戏上达到人类水平。两个关键技术：\n",
    "\n",
    "#### 经验回放 (Experience Replay)\n",
    "\n",
    "**问题**：连续采样的样本高度相关，违反 i.i.d. 假设\n",
    "\n",
    "**解决**：将经验 $(s, a, r, s', done)$ 存入缓冲区，训练时随机采样\n",
    "\n",
    "**优势**：\n",
    "- 打破样本相关性\n",
    "- 提高数据利用效率\n",
    "- 稳定训练过程\n",
    "\n",
    "#### 目标网络 (Target Network)\n",
    "\n",
    "**问题**：更新 Q 网络时，目标值也在变化，导致不稳定\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta)$$\n",
    "\n",
    "**解决**：使用固定的目标网络 $\\theta^-$，定期同步\n",
    "\n",
    "$$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "### 1.3 DQN 算法流程\n",
    "\n",
    "```\n",
    "初始化:\n",
    "    - Q 网络 Q(s, a; θ)\n",
    "    - 目标网络 Q̂(s, a; θ⁻) ← Q\n",
    "    - 经验回放缓冲区 D\n",
    "\n",
    "对于每个回合:\n",
    "    初始化状态 s\n",
    "    对于每一步:\n",
    "        1. 使用 ε-greedy 选择动作 a\n",
    "        2. 执行 a，观察 r, s'\n",
    "        3. 存储 (s, a, r, s', done) 到 D\n",
    "        4. 从 D 采样 mini-batch\n",
    "        5. 计算目标: y = r + γ max_a' Q̂(s', a'; θ⁻)\n",
    "        6. 梯度下降: θ ← θ - α∇_θ(y - Q(s, a; θ))²\n",
    "        7. 每 C 步更新目标网络: θ⁻ ← θ\n",
    "        8. s ← s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第2部分：环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 尝试导入 gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")\n",
    "\n",
    "# ============================================================\n",
    "# 配置\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# 可视化配置\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# 设备配置\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(\"环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CartPole 环境介绍\n",
    "\n",
    "CartPole 是经典的控制问题：通过左右移动小车来平衡竖直的杆子。\n",
    "\n",
    "- **状态空间**: 4 维连续 (小车位置、速度、杆角度、角速度)\n",
    "- **动作空间**: 2 个离散动作 (向左、向右)\n",
    "- **奖励**: 每步 +1（杆子保持平衡）\n",
    "- **终止条件**: 杆子倾斜超过 15°，或小车偏离中心超过 2.4 单位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 探索 CartPole 环境\n",
    "# ============================================================\n",
    "\n",
    "if HAS_GYM:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    print(\"CartPole-v1 环境信息:\")\n",
    "    print(f\"  状态空间: {env.observation_space}\")\n",
    "    print(f\"  状态维度: {env.observation_space.shape[0]}\")\n",
    "    print(f\"  动作空间: {env.action_space}\")\n",
    "    print(f\"  动作数量: {env.action_space.n}\")\n",
    "    \n",
    "    # 随机探索几步\n",
    "    state, _ = env.reset(seed=RANDOM_SEED)\n",
    "    print(f\"\\n初始状态: {state}\")\n",
    "    print(f\"  - 小车位置: {state[0]:.4f}\")\n",
    "    print(f\"  - 小车速度: {state[1]:.4f}\")\n",
    "    print(f\"  - 杆子角度: {state[2]:.4f}\")\n",
    "    print(f\"  - 杆子角速度: {state[3]:.4f}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第3部分：核心组件实现\n",
    "\n",
    "### 3.1 经验回放缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 经验回放缓冲区\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    \"\"\"单步转换数据\"\"\"\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    经验回放缓冲区\n",
    "    \n",
    "    核心功能:\n",
    "    - 存储交互经验\n",
    "    - 均匀随机采样\n",
    "    - 自动管理容量\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: 缓冲区最大容量\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"存储一条经验\"\"\"\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:\n",
    "        \"\"\"随机采样一个批次\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = np.array([t.state for t in batch], dtype=np.float32)\n",
    "        actions = np.array([t.action for t in batch], dtype=np.int64)\n",
    "        rewards = np.array([t.reward for t in batch], dtype=np.float32)\n",
    "        next_states = np.array([t.next_state for t in batch], dtype=np.float32)\n",
    "        dones = np.array([t.done for t in batch], dtype=np.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# 测试\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "for i in range(100):\n",
    "    buffer.push(\n",
    "        state=np.random.randn(4),\n",
    "        action=random.randint(0, 1),\n",
    "        reward=1.0,\n",
    "        next_state=np.random.randn(4),\n",
    "        done=False\n",
    "    )\n",
    "\n",
    "print(f\"缓冲区大小: {len(buffer)}\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "print(f\"采样批次 - states 形状: {states.shape}\")\n",
    "print(\"经验回放缓冲区测试通过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DQN 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DQN 网络架构\n",
    "# ============================================================\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    基础 DQN 网络\n",
    "    \n",
    "    结构: State -> FC -> ReLU -> FC -> ReLU -> FC -> Q-values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"正交初始化\"\"\"\n",
    "        for module in self.net:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前向传播，返回各动作的 Q 值\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 测试\n",
    "net = DQNNetwork(state_dim=4, action_dim=2, hidden_dim=128).to(DEVICE)\n",
    "x = torch.randn(32, 4).to(DEVICE)\n",
    "q_values = net(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {q_values.shape}\")\n",
    "print(f\"Q 值示例: {q_values[0].detach().cpu().numpy()}\")\n",
    "print(f\"网络参数量: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "print(\"DQN 网络测试通过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dueling DQN 网络\n",
    "\n",
    "**核心思想**：将 Q 值分解为状态价值 V(s) 和优势函数 A(s,a)\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s, a')$$\n",
    "\n",
    "**优势**：\n",
    "- 状态价值流独立学习状态好坏\n",
    "- 优势流专注于比较动作的相对好坏\n",
    "- 在动作影响不大时学习效率更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dueling DQN 网络架构\n",
    "# ============================================================\n",
    "\n",
    "class DuelingDQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN 网络\n",
    "    \n",
    "    结构:\n",
    "        State -> 共享层 -> 价值流 -> V(s)\n",
    "                       -> 优势流 -> A(s,a)\n",
    "        Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 价值流 V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # 优势流 A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature(x)\n",
    "        \n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # Q = V + (A - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "\n",
    "# 测试\n",
    "dueling_net = DuelingDQNNetwork(state_dim=4, action_dim=2).to(DEVICE)\n",
    "q_values = dueling_net(x)\n",
    "print(f\"Dueling DQN 输出形状: {q_values.shape}\")\n",
    "print(f\"参数量: {sum(p.numel() for p in dueling_net.parameters()):,}\")\n",
    "print(\"Dueling DQN 网络测试通过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第4部分：DQN 智能体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DQN 智能体\n",
    "# ============================================================\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN 智能体\n",
    "    \n",
    "    支持:\n",
    "    - 标准 DQN\n",
    "    - Double DQN (解耦动作选择和评估)\n",
    "    - Dueling DQN (分解 V 和 A)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.01,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        buffer_size: int = 100000,\n",
    "        batch_size: int = 64,\n",
    "        target_update_freq: int = 100,\n",
    "        double_dqn: bool = False,\n",
    "        dueling: bool = False,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.double_dqn = double_dqn\n",
    "        \n",
    "        # 设备\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # 网络\n",
    "        NetworkClass = DuelingDQNNetwork if dueling else DQNNetwork\n",
    "        self.q_network = NetworkClass(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_network = NetworkClass(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # 经验回放\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # 计数器\n",
    "        self.update_count = 0\n",
    "    \n",
    "    def get_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"ε-greedy 动作选择\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done) -> Optional[float]:\n",
    "        \"\"\"存储经验并训练\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # 采样批次\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states_t = torch.FloatTensor(states).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states_t = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # 当前 Q 值\n",
    "        current_q = self.q_network(states_t).gather(1, actions_t.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # 目标 Q 值\n",
    "        with torch.no_grad():\n",
    "            if self.double_dqn:\n",
    "                # Double DQN: 用在线网络选动作，目标网络评估\n",
    "                next_actions = self.q_network(next_states_t).argmax(dim=1)\n",
    "                next_q = self.target_network(next_states_t).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "            else:\n",
    "                next_q = self.target_network(next_states_t).max(dim=1)[0]\n",
    "            \n",
    "            target_q = rewards_t + self.gamma * next_q * (1 - dones_t)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # 优化\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新目标网络\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"衰减探索率\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"DQN 智能体定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第5部分：训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练函数\n",
    "# ============================================================\n",
    "\n",
    "def train_dqn(\n",
    "    env_name: str = 'CartPole-v1',\n",
    "    num_episodes: int = 200,\n",
    "    double_dqn: bool = False,\n",
    "    dueling: bool = False,\n",
    "    seed: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[DQNAgent, List[float]]:\n",
    "    \"\"\"\n",
    "    训练 DQN 智能体\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium 环境名称\n",
    "        num_episodes: 训练回合数\n",
    "        double_dqn: 是否使用 Double DQN\n",
    "        dueling: 是否使用 Dueling DQN\n",
    "        seed: 随机种子\n",
    "        verbose: 是否打印进度\n",
    "    \n",
    "    Returns:\n",
    "        (agent, rewards_history)\n",
    "    \"\"\"\n",
    "    if not HAS_GYM:\n",
    "        print(\"需要安装 gymnasium\")\n",
    "        return None, []\n",
    "    \n",
    "    # 设置种子\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 创建环境\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # 算法名称\n",
    "    algo_name = \"DQN\"\n",
    "    if double_dqn:\n",
    "        algo_name = \"Double \" + algo_name\n",
    "    if dueling:\n",
    "        algo_name = \"Dueling \" + algo_name\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"训练 {algo_name} on {env_name}\")\n",
    "        print(f\"状态维度: {state_dim}, 动作数: {action_dim}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    # 创建智能体\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100,\n",
    "        double_dqn=double_dqn,\n",
    "        dueling=dueling\n",
    "    )\n",
    "    \n",
    "    # 训练\n",
    "    rewards_history = []\n",
    "    best_avg = float('-inf')\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=seed + episode)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if verbose and (episode + 1) % 20 == 0:\n",
    "            avg = np.mean(rewards_history[-20:])\n",
    "            best_avg = max(best_avg, avg)\n",
    "            print(f\"回合 {episode+1:4d} | 平均: {avg:7.2f} | 最佳: {best_avg:7.2f} | ε: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练基础 DQN\n",
    "# ============================================================\n",
    "\n",
    "# 使用较少的回合数进行快速测试\n",
    "agent_dqn, rewards_dqn = train_dqn(\n",
    "    num_episodes=150,\n",
    "    double_dqn=False,\n",
    "    dueling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练 Double Dueling DQN\n",
    "# ============================================================\n",
    "\n",
    "agent_dd, rewards_dd = train_dqn(\n",
    "    num_episodes=150,\n",
    "    double_dqn=True,\n",
    "    dueling=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第6部分：结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 绘制学习曲线\n",
    "# ============================================================\n",
    "\n",
    "def plot_learning_curves(results: dict, window: int = 20):\n",
    "    \"\"\"绘制多算法学习曲线对比\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results.items()):\n",
    "        if len(rewards) >= window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            plt.plot(smoothed, label=name, color=colors[idx % len(colors)], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episode', fontsize=12)\n",
    "    plt.ylabel('Total Reward', fontsize=12)\n",
    "    plt.title('DQN 变体学习曲线对比', fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 绘制对比\n",
    "if rewards_dqn and rewards_dd:\n",
    "    plot_learning_curves({\n",
    "        'DQN': rewards_dqn,\n",
    "        'Double Dueling DQN': rewards_dd\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 评估智能体\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_agent(agent, env_name: str, num_episodes: int = 10):\n",
    "    \"\"\"评估智能体性能\"\"\"\n",
    "    if not HAS_GYM:\n",
    "        return []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state, training=False)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# 评估\n",
    "if agent_dd:\n",
    "    eval_rewards = evaluate_agent(agent_dd, 'CartPole-v1', num_episodes=20)\n",
    "    print(f\"\\n评估结果 (Double Dueling DQN):\")\n",
    "    print(f\"  平均奖励: {np.mean(eval_rewards):.2f}\")\n",
    "    print(f\"  标准差: {np.std(eval_rewards):.2f}\")\n",
    "    print(f\"  最高: {np.max(eval_rewards):.0f}\")\n",
    "    print(f\"  最低: {np.min(eval_rewards):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第7部分：交互式实验\n",
    "\n",
    "### 实验1：探索率衰减的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 探索率衰减可视化\n",
    "# ============================================================\n",
    "\n",
    "def visualize_epsilon_decay(decay_rates: List[float], episodes: int = 200):\n",
    "    \"\"\"可视化不同衰减率下的探索率变化\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for decay in decay_rates:\n",
    "        epsilons = []\n",
    "        eps = 1.0\n",
    "        for _ in range(episodes):\n",
    "            epsilons.append(eps)\n",
    "            eps = max(0.01, eps * decay)\n",
    "        plt.plot(epsilons, label=f'decay={decay}')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('ε-greedy 探索率衰减')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "visualize_epsilon_decay([0.99, 0.995, 0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验2：目标网络更新频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 目标网络更新频率的影响（说明性示例）\n",
    "# ============================================================\n",
    "\n",
    "print(\"目标网络更新频率的影响:\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"更新频率过低 (如每 1000 步):\")\n",
    "print(\"  - 优点: 目标更稳定\")\n",
    "print(\"  - 缺点: 学习较慢，目标可能过时\")\n",
    "print()\n",
    "print(\"更新频率过高 (如每 10 步):\")\n",
    "print(\"  - 优点: 目标更准确\")\n",
    "print(\"  - 缺点: 可能导致不稳定\")\n",
    "print()\n",
    "print(\"推荐范围: 100-1000 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **DQN 核心创新**：\n",
    "   - 经验回放：打破样本相关性\n",
    "   - 目标网络：稳定训练目标\n",
    "\n",
    "2. **Double DQN**：\n",
    "   - 解决 Q 值过估计问题\n",
    "   - 用在线网络选动作，目标网络评估\n",
    "\n",
    "3. **Dueling DQN**：\n",
    "   - 分解 Q = V + A\n",
    "   - 在动作影响不大时更高效\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 算法 | 特点 | 适用场景 |\n",
    "|------|------|----------|\n",
    "| DQN | 基础版本 | 简单问题 |\n",
    "| Double DQN | 减少过估计 | 需要稳定估计 |\n",
    "| Dueling DQN | 分离 V 和 A | 动作影响差异大 |\n",
    "| Rainbow | 集成所有改进 | 复杂问题 |\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "| 参数 | 推荐范围 | 说明 |\n",
    "|------|----------|------|\n",
    "| 学习率 | 1e-4 ~ 1e-3 | 太大不稳定 |\n",
    "| Batch Size | 32 ~ 256 | 视内存而定 |\n",
    "| γ (折扣因子) | 0.99 | 短期任务可用 0.95 |\n",
    "| 目标网络更新 | 100 ~ 1000 步 | 或软更新 τ=0.005 |\n",
    "| ε 衰减 | 0.995 ~ 0.9999 | 视任务难度 |\n",
    "\n",
    "### 下一步学习\n",
    "\n",
    "- 优先级经验回放 (PER)\n",
    "- 策略梯度方法 (A2C, PPO)\n",
    "- 连续动作空间算法 (SAC, TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 单元测试\n",
    "# ============================================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"运行所有单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: ReplayBuffer\n",
    "    try:\n",
    "        buf = ReplayBuffer(100)\n",
    "        for i in range(50):\n",
    "            buf.push(np.random.randn(4), 0, 1.0, np.random.randn(4), False)\n",
    "        assert len(buf) == 50\n",
    "        s, a, r, ns, d = buf.sample(32)\n",
    "        assert s.shape == (32, 4)\n",
    "        print(\"测试1通过: ReplayBuffer\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: DQNNetwork\n",
    "    try:\n",
    "        net = DQNNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        out = net(x)\n",
    "        assert out.shape == (32, 2)\n",
    "        print(\"测试2通过: DQNNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: DuelingDQNNetwork\n",
    "    try:\n",
    "        net = DuelingDQNNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        out = net(x)\n",
    "        assert out.shape == (32, 2)\n",
    "        print(\"测试3通过: DuelingDQNNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: DQNAgent\n",
    "    try:\n",
    "        agent = DQNAgent(4, 2, batch_size=32, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        \n",
    "        for _ in range(50):\n",
    "            agent.update(state, 0, 1.0, state, False)\n",
    "        print(\"测试4通过: DQNAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试5: Double DQN 模式\n",
    "    try:\n",
    "        agent = DQNAgent(4, 2, batch_size=32, double_dqn=True, device='cpu')\n",
    "        for _ in range(50):\n",
    "            agent.update(np.random.randn(4).astype(np.float32), 0, 1.0, \n",
    "                        np.random.randn(4).astype(np.float32), False)\n",
    "        print(\"测试5通过: Double DQN\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试5失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Mnih et al., \"Playing Atari with Deep Reinforcement Learning\", 2013\n",
    "2. van Hasselt et al., \"Deep Reinforcement Learning with Double Q-learning\", 2016\n",
    "3. Wang et al., \"Dueling Network Architectures for Deep Reinforcement Learning\", 2016\n",
    "4. Hessel et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning\", 2018\n",
    "\n",
    "---\n",
    "\n",
    "[返回上级](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
