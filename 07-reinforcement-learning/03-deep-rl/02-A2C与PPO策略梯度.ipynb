{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic 与 PPO 实战教程\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "- 理解策略梯度方法的核心思想\n",
    "- 掌握 Actor-Critic 架构的原理\n",
    "- 实现广义优势估计 (GAE)\n",
    "- 实现 A2C (Advantage Actor-Critic) 算法\n",
    "- 实现 PPO (Proximal Policy Optimization) 算法\n",
    "- 在 CartPole 环境中训练和比较不同算法\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- 强化学习基础（MDP、价值函数）\n",
    "- PyTorch 神经网络\n",
    "- 概率分布与采样\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "60-90 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第1部分：理论背景\n",
    "\n",
    "### 1.1 策略梯度定理\n",
    "\n",
    "**核心思想**：直接参数化策略 $\\pi_\\theta(a|s)$，通过梯度上升最大化期望回报。\n",
    "\n",
    "**策略梯度定理**：\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s, a)\\right]$$\n",
    "\n",
    "**直觉理解**：\n",
    "- 如果动作 $a$ 导致高回报，增加其概率\n",
    "- 如果动作 $a$ 导致低回报，减少其概率\n",
    "\n",
    "### 1.2 Actor-Critic 架构\n",
    "\n",
    "结合策略方法和价值方法的优点：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│              Actor-Critic 架构                   │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│                                                 │\n",
    "│    状态 s                                       │\n",
    "│       │                                         │\n",
    "│       ├──────────────┬────────────────┐        │\n",
    "│       ↓              ↓                │        │\n",
    "│   ┌───────┐     ┌───────┐            │        │\n",
    "│   │ Actor │     │ Critic│            │        │\n",
    "│   │ π(a|s)│     │ V(s)  │            │        │\n",
    "│   └───┬───┘     └───┬───┘            │        │\n",
    "│       │             │                 │        │\n",
    "│       ↓             ↓                 │        │\n",
    "│    动作 a      优势估计 A              │        │\n",
    "│       │             │                 │        │\n",
    "│       ↓             ↓                 │        │\n",
    "│    环境交互 ←── 策略梯度更新 ←─────────┘        │\n",
    "│                                                 │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "- **Actor (演员)**：策略网络，输出动作概率分布\n",
    "- **Critic (评论家)**：价值网络，评估状态价值\n",
    "\n",
    "### 1.3 优势函数与 GAE\n",
    "\n",
    "**优势函数** $A(s, a)$：动作 $a$ 相比平均水平的优势\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s) \\approx r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "**广义优势估计 (GAE)**：平衡偏差与方差\n",
    "\n",
    "$$\\hat{A}_t = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "其中 $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ 是 TD 误差。\n",
    "\n",
    "- $\\lambda = 0$：单步 TD 估计（低方差，高偏差）\n",
    "- $\\lambda = 1$：蒙特卡洛估计（高方差，低偏差）\n",
    "\n",
    "### 1.4 PPO 核心思想\n",
    "\n",
    "**问题**：策略梯度更新步长难以控制\n",
    "- 步长太大：策略崩溃\n",
    "- 步长太小：学习缓慢\n",
    "\n",
    "**PPO-Clip 解决方案**：限制策略更新幅度\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "**直觉**：当策略比率超出 $[1-\\epsilon, 1+\\epsilon]$ 范围时，梯度被截断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第2部分：环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Optional, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"请安装 gymnasium: pip install gymnasium\")\n",
    "\n",
    "# ============================================================\n",
    "# 配置\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "print(\"环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第3部分：核心组件实现\n",
    "\n",
    "### 3.1 轨迹缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 轨迹缓冲区（用于 on-policy 算法）\n",
    "# ============================================================\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    轨迹缓冲区\n",
    "    \n",
    "    与 off-policy 的经验回放不同，on-policy 方法需要完整轨迹，\n",
    "    数据使用后即丢弃。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma: float = 0.99, gae_lambda: float = 0.95):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"清空缓冲区\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def add(self, state, action, log_prob, reward, value, done):\n",
    "        \"\"\"添加单步数据\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_gae(self, last_value: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        计算广义优势估计 (GAE)\n",
    "        \n",
    "        GAE 公式:\n",
    "        δ_t = r_t + γV(s_{t+1}) - V(s_t)\n",
    "        Â_t = Σ_{l=0}^{∞} (γλ)^l δ_{t+l}\n",
    "        \n",
    "        Args:\n",
    "            last_value: 最后状态的价值估计（用于 bootstrap）\n",
    "        \n",
    "        Returns:\n",
    "            (returns, advantages): 回报和优势估计\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards)\n",
    "        values = np.array(self.values)\n",
    "        dones = np.array(self.dones)\n",
    "        n_steps = len(rewards)\n",
    "        \n",
    "        # 添加最后价值\n",
    "        values = np.append(values, last_value)\n",
    "        \n",
    "        # 计算 GAE\n",
    "        advantages = np.zeros(n_steps, dtype=np.float32)\n",
    "        gae = 0.0\n",
    "        \n",
    "        for t in reversed(range(n_steps)):\n",
    "            # TD 误差\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            # GAE 递推\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        # 回报 = 优势 + 价值\n",
    "        returns = advantages + values[:-1]\n",
    "        \n",
    "        return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "# 测试 GAE 计算\n",
    "buffer = RolloutBuffer(gamma=0.99, gae_lambda=0.95)\n",
    "for i in range(10):\n",
    "    buffer.add(\n",
    "        state=np.random.randn(4),\n",
    "        action=0,\n",
    "        log_prob=-0.5,\n",
    "        reward=1.0,\n",
    "        value=0.5,\n",
    "        done=(i == 9)\n",
    "    )\n",
    "\n",
    "returns, advantages = buffer.compute_gae(last_value=0.0)\n",
    "print(f\"缓冲区大小: {len(buffer)}\")\n",
    "print(f\"回报形状: {returns.shape}\")\n",
    "print(f\"优势形状: {advantages.shape}\")\n",
    "print(f\"回报示例: {returns[:5].numpy()}\")\n",
    "print(\"轨迹缓冲区测试通过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Actor-Critic 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Actor-Critic 共享参数网络\n",
    "# ============================================================\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic 网络\n",
    "    \n",
    "    结构:\n",
    "        State -> 共享层 -> Actor 头 -> π(a|s)\n",
    "                       -> Critic 头 -> V(s)\n",
    "    \n",
    "    共享特征层可以让策略和价值函数共享状态表示。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 共享特征层\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Actor 头（策略）\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic 头（价值）\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # 初始化\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"正交初始化\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Actor 输出层用较小增益\n",
    "        nn.init.orthogonal_(self.actor.weight, gain=0.01)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"返回动作 logits 和状态价值\"\"\"\n",
    "        features = self.shared(x)\n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return action_logits, value\n",
    "    \n",
    "    def get_action_and_value(\n",
    "        self, \n",
    "        state: torch.Tensor, \n",
    "        action: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        获取动作、对数概率、熵和价值\n",
    "        \n",
    "        Args:\n",
    "            state: 状态张量\n",
    "            action: 可选的指定动作（用于计算旧动作的新对数概率）\n",
    "        \n",
    "        Returns:\n",
    "            (action, log_prob, entropy, value)\n",
    "        \"\"\"\n",
    "        logits, value = self(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action, log_prob, entropy, value.squeeze(-1)\n",
    "\n",
    "\n",
    "# 测试\n",
    "net = ActorCriticNetwork(state_dim=4, action_dim=2).to(DEVICE)\n",
    "x = torch.randn(32, 4).to(DEVICE)\n",
    "\n",
    "action, log_prob, entropy, value = net.get_action_and_value(x)\n",
    "print(f\"动作形状: {action.shape}\")\n",
    "print(f\"对数概率形状: {log_prob.shape}\")\n",
    "print(f\"熵形状: {entropy.shape}\")\n",
    "print(f\"价值形状: {value.shape}\")\n",
    "print(f\"网络参数量: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "print(\"Actor-Critic 网络测试通过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第4部分：A2C 智能体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# A2C (Advantage Actor-Critic) 智能体\n",
    "# ============================================================\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    A2C 智能体\n",
    "    \n",
    "    A2C 是 A3C 的同步版本，核心思想：\n",
    "    1. 使用优势函数减少方差\n",
    "    2. 熵正则化鼓励探索\n",
    "    3. N-step 回报平衡偏差和方差\n",
    "    \n",
    "    损失函数:\n",
    "    L = L_policy + c1 * L_value - c2 * H[π]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        lr: float = 7e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        value_coef: float = 0.5,\n",
    "        entropy_coef: float = 0.01,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # 网络\n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # 缓冲区\n",
    "        self.buffer = RolloutBuffer(gamma=gamma, gae_lambda=gae_lambda)\n",
    "    \n",
    "    def get_action(self, state: np.ndarray) -> Tuple[int, float, float]:\n",
    "        \"\"\"选择动作\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = self.network.get_action_and_value(state_t)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store(self, state, action, log_prob, reward, value, done):\n",
    "        \"\"\"存储转换\"\"\"\n",
    "        self.buffer.add(state, action, log_prob, reward, value, done)\n",
    "    \n",
    "    def update(self, last_value: float) -> Dict[str, float]:\n",
    "        \"\"\"执行一次策略更新\"\"\"\n",
    "        # 获取数据\n",
    "        returns, advantages = self.buffer.compute_gae(last_value)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(self.buffer.states)).to(self.device)\n",
    "        actions = torch.LongTensor(self.buffer.actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(self.buffer.log_probs).to(self.device)\n",
    "        returns = returns.to(self.device)\n",
    "        advantages = advantages.to(self.device)\n",
    "        \n",
    "        # 标准化优势\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # 计算新的策略分布\n",
    "        _, new_log_probs, entropy, values = self.network.get_action_and_value(states, actions)\n",
    "        \n",
    "        # 策略损失\n",
    "        policy_loss = -(new_log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # 价值损失\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # 熵损失（负号因为要最大化熵）\n",
    "        entropy_loss = -entropy.mean()\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        # 优化\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空缓冲区\n",
    "        self.buffer.reset()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': -entropy_loss.item()\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"A2C 智能体定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第5部分：PPO 智能体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PPO (Proximal Policy Optimization) 智能体\n",
    "# ============================================================\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    PPO 智能体\n",
    "    \n",
    "    核心特点:\n",
    "    1. PPO-Clip: 限制策略更新幅度\n",
    "    2. 多轮 epoch: 每批数据可以训练多次\n",
    "    3. Mini-batch: 将大批次分成小批次\n",
    "    \n",
    "    PPO-Clip 目标:\n",
    "    L^{CLIP} = E[min(r_t(θ)Â_t, clip(r_t(θ), 1-ε, 1+ε)Â_t)]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        value_coef: float = 0.5,\n",
    "        entropy_coef: float = 0.01,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        n_epochs: int = 10,\n",
    "        mini_batch_size: int = 64,\n",
    "        device: str = 'auto'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.n_epochs = n_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # 网络\n",
    "        self.network = ActorCriticNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)\n",
    "        \n",
    "        # 缓冲区\n",
    "        self.buffer = RolloutBuffer(gamma=gamma, gae_lambda=gae_lambda)\n",
    "    \n",
    "    def get_action(self, state: np.ndarray) -> Tuple[int, float, float]:\n",
    "        \"\"\"选择动作\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = self.network.get_action_and_value(state_t)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store(self, state, action, log_prob, reward, value, done):\n",
    "        \"\"\"存储转换\"\"\"\n",
    "        self.buffer.add(state, action, log_prob, reward, value, done)\n",
    "    \n",
    "    def update(self, last_value: float) -> Dict[str, float]:\n",
    "        \"\"\"PPO 更新\"\"\"\n",
    "        # 获取数据\n",
    "        returns, advantages = self.buffer.compute_gae(last_value)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(self.buffer.states)).to(self.device)\n",
    "        actions = torch.LongTensor(self.buffer.actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(self.buffer.log_probs).to(self.device)\n",
    "        returns = returns.to(self.device)\n",
    "        advantages = advantages.to(self.device)\n",
    "        \n",
    "        # 标准化优势\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        batch_size = len(states)\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        update_count = 0\n",
    "        \n",
    "        # 多轮 epoch\n",
    "        for _ in range(self.n_epochs):\n",
    "            # 随机打乱\n",
    "            indices = np.random.permutation(batch_size)\n",
    "            \n",
    "            # Mini-batch 更新\n",
    "            for start in range(0, batch_size, self.mini_batch_size):\n",
    "                end = start + self.mini_batch_size\n",
    "                mb_idx = indices[start:end]\n",
    "                \n",
    "                mb_states = states[mb_idx]\n",
    "                mb_actions = actions[mb_idx]\n",
    "                mb_old_log_probs = old_log_probs[mb_idx]\n",
    "                mb_advantages = advantages[mb_idx]\n",
    "                mb_returns = returns[mb_idx]\n",
    "                \n",
    "                # 计算新策略\n",
    "                _, new_log_probs, entropy, values = self.network.get_action_and_value(\n",
    "                    mb_states, mb_actions\n",
    "                )\n",
    "                \n",
    "                # 计算比率\n",
    "                log_ratio = new_log_probs - mb_old_log_probs\n",
    "                ratio = torch.exp(log_ratio)\n",
    "                \n",
    "                # PPO-Clip 目标\n",
    "                surr1 = ratio * mb_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # 价值损失\n",
    "                value_loss = F.mse_loss(values, mb_returns)\n",
    "                \n",
    "                # 熵损失\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # 总损失\n",
    "                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "                \n",
    "                # 优化\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.mean().item()\n",
    "                update_count += 1\n",
    "        \n",
    "        # 清空缓冲区\n",
    "        self.buffer.reset()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / update_count,\n",
    "            'value_loss': total_value_loss / update_count,\n",
    "            'entropy': total_entropy / update_count\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"PPO 智能体定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第6部分：训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练函数\n",
    "# ============================================================\n",
    "\n",
    "def train_policy_gradient(\n",
    "    agent,\n",
    "    env_name: str = 'CartPole-v1',\n",
    "    num_episodes: int = 200,\n",
    "    n_steps: int = 128,\n",
    "    seed: int = 42,\n",
    "    algo_name: str = 'Agent',\n",
    "    verbose: bool = True\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    训练策略梯度智能体\n",
    "    \n",
    "    Args:\n",
    "        agent: A2C 或 PPO 智能体\n",
    "        env_name: 环境名称\n",
    "        num_episodes: 训练回合数（仅用于记录）\n",
    "        n_steps: 每次更新的步数\n",
    "        seed: 随机种子\n",
    "        algo_name: 算法名称\n",
    "        verbose: 是否打印进度\n",
    "    \n",
    "    Returns:\n",
    "        奖励历史\n",
    "    \"\"\"\n",
    "    if not HAS_GYM:\n",
    "        print(\"需要安装 gymnasium\")\n",
    "        return []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"训练 {algo_name} on {env_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    best_avg = float('-inf')\n",
    "    \n",
    "    state, _ = env.reset(seed=seed)\n",
    "    episode_reward = 0\n",
    "    total_steps = 0\n",
    "    max_steps = num_episodes * 500  # 估计最大步数\n",
    "    \n",
    "    while total_steps < max_steps:\n",
    "        # 收集 n_steps 步\n",
    "        for _ in range(n_steps):\n",
    "            action, log_prob, value = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store(state, action, log_prob, reward, value, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                rewards_history.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "                state, _ = env.reset()\n",
    "            \n",
    "            if total_steps >= max_steps:\n",
    "                break\n",
    "        \n",
    "        # 计算最后状态价值\n",
    "        _, _, last_value = agent.get_action(state)\n",
    "        \n",
    "        # 更新\n",
    "        agent.update(last_value)\n",
    "        \n",
    "        # 打印进度\n",
    "        if verbose and len(episode_rewards) >= 20 and len(episode_rewards) % 20 == 0:\n",
    "            avg = np.mean(episode_rewards[-20:])\n",
    "            best_avg = max(best_avg, avg)\n",
    "            print(f\"回合 {len(episode_rewards):4d} | 平均: {avg:7.2f} | 最佳: {best_avg:7.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练 A2C\n",
    "# ============================================================\n",
    "\n",
    "if HAS_GYM:\n",
    "    a2c_agent = A2CAgent(\n",
    "        state_dim=4,\n",
    "        action_dim=2,\n",
    "        lr=7e-4,\n",
    "        gamma=0.99\n",
    "    )\n",
    "    \n",
    "    rewards_a2c = train_policy_gradient(\n",
    "        a2c_agent,\n",
    "        num_episodes=150,\n",
    "        n_steps=5,\n",
    "        algo_name='A2C'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 训练 PPO\n",
    "# ============================================================\n",
    "\n",
    "if HAS_GYM:\n",
    "    ppo_agent = PPOAgent(\n",
    "        state_dim=4,\n",
    "        action_dim=2,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        clip_epsilon=0.2,\n",
    "        n_epochs=10,\n",
    "        mini_batch_size=64\n",
    "    )\n",
    "    \n",
    "    rewards_ppo = train_policy_gradient(\n",
    "        ppo_agent,\n",
    "        num_episodes=150,\n",
    "        n_steps=128,\n",
    "        algo_name='PPO'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第7部分：结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 绘制学习曲线对比\n",
    "# ============================================================\n",
    "\n",
    "def plot_comparison(results: dict, window: int = 20):\n",
    "    \"\"\"绘制算法对比图\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    \n",
    "    for idx, (name, rewards) in enumerate(results.items()):\n",
    "        if len(rewards) >= window:\n",
    "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            plt.plot(smoothed, label=name, color=colors[idx], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episode', fontsize=12)\n",
    "    plt.ylabel('Total Reward', fontsize=12)\n",
    "    plt.title('A2C vs PPO 学习曲线对比', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if HAS_GYM and rewards_a2c and rewards_ppo:\n",
    "    plot_comparison({\n",
    "        'A2C': rewards_a2c,\n",
    "        'PPO': rewards_ppo\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第8部分：交互式实验\n",
    "\n",
    "### 实验1：PPO 裁剪系数的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 可视化 PPO 裁剪函数\n",
    "# ============================================================\n",
    "\n",
    "def visualize_ppo_clip(epsilon: float = 0.2):\n",
    "    \"\"\"可视化 PPO 裁剪机制\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ratios = np.linspace(0.5, 1.5, 100)\n",
    "    \n",
    "    # 正优势情况\n",
    "    ax = axes[0]\n",
    "    advantage = 1.0\n",
    "    unclipped = ratios * advantage\n",
    "    clipped = np.clip(ratios, 1-epsilon, 1+epsilon) * advantage\n",
    "    objective = np.minimum(unclipped, clipped)\n",
    "    \n",
    "    ax.plot(ratios, unclipped, 'b--', label='未裁剪', alpha=0.7)\n",
    "    ax.plot(ratios, clipped, 'r--', label='裁剪后', alpha=0.7)\n",
    "    ax.plot(ratios, objective, 'g-', linewidth=2, label='PPO 目标')\n",
    "    ax.axvline(1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('策略比率 r(θ)')\n",
    "    ax.set_ylabel('目标值')\n",
    "    ax.set_title(f'正优势 (A > 0), ε = {epsilon}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 负优势情况\n",
    "    ax = axes[1]\n",
    "    advantage = -1.0\n",
    "    unclipped = ratios * advantage\n",
    "    clipped = np.clip(ratios, 1-epsilon, 1+epsilon) * advantage\n",
    "    objective = np.minimum(unclipped, clipped)\n",
    "    \n",
    "    ax.plot(ratios, unclipped, 'b--', label='未裁剪', alpha=0.7)\n",
    "    ax.plot(ratios, clipped, 'r--', label='裁剪后', alpha=0.7)\n",
    "    ax.plot(ratios, objective, 'g-', linewidth=2, label='PPO 目标')\n",
    "    ax.axvline(1-epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(1+epsilon, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('策略比率 r(θ)')\n",
    "    ax.set_ylabel('目标值')\n",
    "    ax.set_title(f'负优势 (A < 0), ε = {epsilon}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ppo_clip(epsilon=0.2)\n",
    "\n",
    "print(\"\\nPPO 裁剪机制解读:\")\n",
    "print(\"- 正优势时: 阻止策略比率过大 (防止过度增加动作概率)\")\n",
    "print(\"- 负优势时: 阻止策略比率过小 (防止过度减少动作概率)\")\n",
    "print(\"- 效果: 限制策略每步更新幅度，保证稳定性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验2：GAE λ 参数的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAE λ 参数说明\n",
    "# ============================================================\n",
    "\n",
    "print(\"GAE (广义优势估计) λ 参数的影响:\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"λ = 0 (单步 TD):\")\n",
    "print(\"  Â_t = δ_t = r_t + γV(s_{t+1}) - V(s_t)\")\n",
    "print(\"  特点: 低方差，高偏差（依赖价值函数准确性）\")\n",
    "print()\n",
    "print(\"λ = 1 (蒙特卡洛):\")\n",
    "print(\"  Â_t = Σ γ^l δ_{t+l} = R_t - V(s_t)\")\n",
    "print(\"  特点: 无偏，高方差\")\n",
    "print()\n",
    "print(\"λ = 0.95 (推荐):\")\n",
    "print(\"  在偏差和方差之间取得平衡\")\n",
    "print(\"  大多数任务效果良好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **策略梯度**：\n",
    "   - 直接优化策略参数\n",
    "   - 可以处理连续动作空间\n",
    "   - 可以学习随机策略\n",
    "\n",
    "2. **Actor-Critic**：\n",
    "   - Actor: 策略网络，决定动作\n",
    "   - Critic: 价值网络，评估状态\n",
    "   - 优势函数减少方差\n",
    "\n",
    "3. **GAE**：\n",
    "   - 平衡偏差与方差\n",
    "   - λ=0.95 是常用设置\n",
    "\n",
    "4. **PPO**：\n",
    "   - 裁剪机制保证稳定更新\n",
    "   - 允许多次使用同批数据\n",
    "   - 实现简单，效果好\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 算法 | 类型 | 样本效率 | 稳定性 | 实现复杂度 |\n",
    "|------|------|----------|--------|------------|\n",
    "| A2C | On-Policy | 低 | 中 | 简单 |\n",
    "| PPO | On-Policy | 中 | 高 | 中等 |\n",
    "| SAC | Off-Policy | 高 | 高 | 复杂 |\n",
    "\n",
    "### 调参建议\n",
    "\n",
    "| 参数 | A2C 推荐 | PPO 推荐 |\n",
    "|------|----------|----------|\n",
    "| 学习率 | 7e-4 | 3e-4 |\n",
    "| γ | 0.99 | 0.99 |\n",
    "| GAE λ | 0.95 | 0.95 |\n",
    "| Clip ε | - | 0.2 |\n",
    "| 熵系数 | 0.01 | 0.01 |\n",
    "| N-steps | 5 | 2048 |\n",
    "| Epochs | 1 | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 单元测试\n",
    "# ============================================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: RolloutBuffer\n",
    "    try:\n",
    "        buf = RolloutBuffer(gamma=0.99, gae_lambda=0.95)\n",
    "        for i in range(10):\n",
    "            buf.add(np.random.randn(4), 0, -0.5, 1.0, 0.5, i == 9)\n",
    "        returns, advantages = buf.compute_gae(0.0)\n",
    "        assert returns.shape == (10,)\n",
    "        assert advantages.shape == (10,)\n",
    "        print(\"测试1通过: RolloutBuffer\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: ActorCriticNetwork\n",
    "    try:\n",
    "        net = ActorCriticNetwork(4, 2, 64)\n",
    "        x = torch.randn(32, 4)\n",
    "        action, log_prob, entropy, value = net.get_action_and_value(x)\n",
    "        assert action.shape == (32,)\n",
    "        assert log_prob.shape == (32,)\n",
    "        assert value.shape == (32,)\n",
    "        print(\"测试2通过: ActorCriticNetwork\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: A2CAgent\n",
    "    try:\n",
    "        agent = A2CAgent(4, 2, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action, log_prob, value = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        \n",
    "        for i in range(10):\n",
    "            agent.store(state, 0, -0.5, 1.0, 0.5, i == 9)\n",
    "        loss_info = agent.update(0.0)\n",
    "        assert 'policy_loss' in loss_info\n",
    "        print(\"测试3通过: A2CAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: PPOAgent\n",
    "    try:\n",
    "        agent = PPOAgent(4, 2, mini_batch_size=32, device='cpu')\n",
    "        state = np.random.randn(4).astype(np.float32)\n",
    "        action, log_prob, value = agent.get_action(state)\n",
    "        assert 0 <= action < 2\n",
    "        \n",
    "        for i in range(64):\n",
    "            agent.store(state, 0, -0.5, 1.0, 0.5, i == 63)\n",
    "        loss_info = agent.update(0.0)\n",
    "        assert 'policy_loss' in loss_info\n",
    "        print(\"测试4通过: PPOAgent\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试5: GAE 计算正确性\n",
    "    try:\n",
    "        buf = RolloutBuffer(gamma=0.99, gae_lambda=0.95)\n",
    "        # 简单测试: 3步轨迹\n",
    "        for i in range(3):\n",
    "            buf.add(np.array([1.0]), 0, 0.0, 1.0, 0.5, i == 2)\n",
    "        returns, advantages = buf.compute_gae(0.0)\n",
    "        \n",
    "        # 手动验证\n",
    "        gamma, lam = 0.99, 0.95\n",
    "        v = [0.5, 0.5, 0.5, 0.0]\n",
    "        r = [1.0, 1.0, 1.0]\n",
    "        d = [0, 0, 1]\n",
    "        \n",
    "        expected = np.zeros(3)\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(3)):\n",
    "            delta = r[t] + gamma * v[t+1] * (1 - d[t]) - v[t]\n",
    "            gae = delta + gamma * lam * (1 - d[t]) * gae\n",
    "            expected[t] = gae\n",
    "        \n",
    "        assert np.allclose(advantages.numpy(), expected, atol=1e-5)\n",
    "        print(\"测试5通过: GAE 计算正确\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"测试5失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Mnih et al., \"Asynchronous Methods for Deep Reinforcement Learning\", 2016 (A3C/A2C)\n",
    "2. Schulman et al., \"Proximal Policy Optimization Algorithms\", 2017\n",
    "3. Schulman et al., \"High-Dimensional Continuous Control Using Generalized Advantage Estimation\", 2015\n",
    "4. Sutton & Barto, \"Reinforcement Learning: An Introduction\", Chapter 13\n",
    "\n",
    "---\n",
    "\n",
    "[返回上级](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
