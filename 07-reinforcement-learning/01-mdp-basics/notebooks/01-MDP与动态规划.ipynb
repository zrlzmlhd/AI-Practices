{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# 马尔可夫决策过程与动态规划\n",
    "\n",
    "**Markov Decision Process & Dynamic Programming**\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "完成本教程后，你将掌握：\n",
    "\n",
    "1. **MDP 数学框架**: 状态、动作、转移、奖励、折扣因子\n",
    "2. **贝尔曼方程**: 期望方程与最优方程的推导与直觉\n",
    "3. **动态规划算法**: 策略评估、策略改进、策略迭代、值迭代\n",
    "4. **实践技能**: 在 GridWorld 环境中实现和验证算法\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- Python 编程基础\n",
    "- NumPy 数组操作\n",
    "- 概率论基础（条件概率、期望）\n",
    "- 基础线性代数\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "**60-90 分钟** (可分多次完成)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [理论基础](#part1)\n",
    "   - 1.1 强化学习框架\n",
    "   - 1.2 MDP 形式化定义\n",
    "   - 1.3 策略与价值函数\n",
    "   - 1.4 贝尔曼方程\n",
    "\n",
    "2. [环境实现](#part2)\n",
    "   - 2.1 库导入与配置\n",
    "   - 2.2 GridWorld 环境\n",
    "   - 2.3 环境可视化\n",
    "\n",
    "3. [动态规划算法](#part3)\n",
    "   - 3.1 策略评估\n",
    "   - 3.2 策略改进\n",
    "   - 3.3 策略迭代\n",
    "   - 3.4 值迭代\n",
    "\n",
    "4. [结果分析](#part4)\n",
    "   - 4.1 算法对比\n",
    "   - 4.2 策略执行\n",
    "   - 4.3 参数实验\n",
    "\n",
    "5. [总结与练习](#part5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"part1\"></a>\n",
    "# Part 1: 理论基础\n",
    "\n",
    "## 1.1 强化学习框架\n",
    "\n",
    "强化学习研究**智能体 (Agent)** 如何在与**环境 (Environment)** 的交互中学习最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-rl-diagram",
   "metadata": {},
   "source": [
    "```\n",
    "                    动作 aₜ\n",
    "         ┌────────────────────┐\n",
    "         │                    ▼\n",
    "    ┌─────────┐          ┌─────────┐\n",
    "    │  Agent  │          │   Env   │\n",
    "    │ (智能体) │          │  (环境)  │\n",
    "    └─────────┘          └─────────┘\n",
    "         ▲                    │\n",
    "         │   状态 sₜ₊₁        │\n",
    "         │   奖励 rₜ₊₁        │\n",
    "         └────────────────────┘\n",
    "```\n",
    "\n",
    "**交互循环**：\n",
    "1. 智能体观测当前状态 $s_t$\n",
    "2. 根据策略选择动作 $a_t$\n",
    "3. 环境转移至新状态 $s_{t+1}$，反馈奖励 $r_{t+1}$\n",
    "4. 智能体利用反馈更新策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mdp-def",
   "metadata": {},
   "source": [
    "## 1.2 MDP 形式化定义\n",
    "\n",
    "马尔可夫决策过程由**五元组**定义：\n",
    "\n",
    "$$\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$$\n",
    "\n",
    "| 符号 | 名称 | 定义 |\n",
    "|:----:|:----:|:----|\n",
    "| $\\mathcal{S}$ | 状态空间 | 所有可能状态的集合 |\n",
    "| $\\mathcal{A}$ | 动作空间 | 所有可能动作的集合 |\n",
    "| $P$ | 转移函数 | $P(s' \\mid s, a) = \\Pr(S_{t+1}=s' \\mid S_t=s, A_t=a)$ |\n",
    "| $R$ | 奖励函数 | $R(s, a, s')$ 或简化为 $R(s, a)$ |\n",
    "| $\\gamma$ | 折扣因子 | $\\gamma \\in [0, 1]$，控制对未来奖励的重视程度 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-markov-property",
   "metadata": {},
   "source": [
    "### 马尔可夫性质\n",
    "\n",
    "> **核心假设**：未来状态仅依赖当前状态，与历史无关\n",
    "\n",
    "$$P(S_{t+1} \\mid S_t, A_t, S_{t-1}, A_{t-1}, \\ldots) = P(S_{t+1} \\mid S_t, A_t)$$\n",
    "\n",
    "**直觉理解**：当前状态包含了预测未来所需的全部信息。这一性质使得动态规划方法成为可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-policy-value",
   "metadata": {},
   "source": [
    "## 1.3 策略与价值函数\n",
    "\n",
    "### 策略 (Policy)\n",
    "\n",
    "策略 $\\pi$ 定义智能体在各状态下的行为方式：\n",
    "\n",
    "- **随机策略**: $\\pi(a \\mid s) = \\Pr(A_t = a \\mid S_t = s)$\n",
    "- **确定性策略**: $a = \\pi(s)$\n",
    "\n",
    "### 状态价值函数 $V^\\pi(s)$\n",
    "\n",
    "从状态 $s$ 出发，遵循策略 $\\pi$ 的期望累积回报：\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "### 动作价值函数 $Q^\\pi(s, a)$\n",
    "\n",
    "从状态 $s$ 执行动作 $a$，然后遵循策略 $\\pi$：\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bellman",
   "metadata": {},
   "source": [
    "## 1.4 贝尔曼方程\n",
    "\n",
    "### 贝尔曼期望方程\n",
    "\n",
    "给定策略 $\\pi$ 的价值函数满足递归关系：\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^\\pi(s') \\right]$$\n",
    "\n",
    "**直觉**: 当前价值 = 即时奖励期望 + 折扣后的后继状态价值期望\n",
    "\n",
    "### 贝尔曼最优方程\n",
    "\n",
    "最优价值函数满足：\n",
    "\n",
    "$$V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V^*(s') \\right]$$\n",
    "\n",
    "**直觉**: 最优价值 = 选择最优动作后能获得的最大期望回报"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"part2\"></a>\n",
    "# Part 2: 环境实现\n",
    "\n",
    "## 2.1 库导入与配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# 设置随机种子保证可复现性\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plt-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib 中文显示配置\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"环境配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-gridworld-intro",
   "metadata": {},
   "source": [
    "## 2.2 GridWorld 环境\n",
    "\n",
    "### 环境描述\n",
    "\n",
    "GridWorld 是强化学习的经典测试环境：\n",
    "\n",
    "- **目标**: 智能体从起点导航到目标点\n",
    "- **动作**: 上、下、左、右 四个方向移动\n",
    "- **奖励**: 每步 -1（鼓励快速到达），到达目标 +100\n",
    "- **障碍**: 无法通过的格子，碰到会停留原地\n",
    "\n",
    "```\n",
    "┌────┬────┬────┬────┐\n",
    "│ S  │    │    │    │   S: 起点 (0,0)\n",
    "├────┼────┼────┼────┤   G: 目标 (3,3)\n",
    "│    │ X  │    │    │   X: 障碍物\n",
    "├────┼────┼────┼────┤\n",
    "│    │    │    │    │\n",
    "├────┼────┼────┼────┤\n",
    "│    │    │    │ G  │\n",
    "└────┴────┴────┴────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config-dataclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GridConfig:\n",
    "    \"\"\"网格世界配置参数\"\"\"\n",
    "    size: int = 4\n",
    "    start: Tuple[int, int] = (0, 0)\n",
    "    goal: Tuple[int, int] = (3, 3)\n",
    "    obstacles: List[Tuple[int, int]] = field(default_factory=list)\n",
    "    step_reward: float = -1.0\n",
    "    goal_reward: float = 100.0\n",
    "    slip_prob: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.obstacles:\n",
    "            self.obstacles = [(1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建默认配置\n",
    "config = GridConfig()\n",
    "\n",
    "print(\"GridWorld 配置:\")\n",
    "print(f\"  网格大小: {config.size}×{config.size}\")\n",
    "print(f\"  起点: {config.start}\")\n",
    "print(f\"  目标: {config.goal}\")\n",
    "print(f\"  障碍物: {config.obstacles}\")\n",
    "print(f\"  每步奖励: {config.step_reward}\")\n",
    "print(f\"  目标奖励: {config.goal_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gridworld-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    网格世界 MDP 环境\n",
    "    \n",
    "    状态: (行, 列) 元组\n",
    "    动作: '上', '下', '左', '右'\n",
    "    \"\"\"\n",
    "    \n",
    "    ACTION_DELTAS = {\n",
    "        '上': (-1, 0),\n",
    "        '下': (1, 0),\n",
    "        '左': (0, -1),\n",
    "        '右': (0, 1)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: GridConfig):\n",
    "        self.config = config\n",
    "        self.size = config.size\n",
    "        self.goal = config.goal\n",
    "        self.obstacles = set(config.obstacles)\n",
    "        \n",
    "        # 构建状态空间\n",
    "        self.states = [\n",
    "            (i, j) for i in range(self.size) \n",
    "            for j in range(self.size)\n",
    "            if (i, j) not in self.obstacles\n",
    "        ]\n",
    "        \n",
    "        self.actions = list(self.ACTION_DELTAS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gridworld-methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加环境核心方法\n",
    "def is_terminal(self, state):\n",
    "    \"\"\"判断是否为终止状态\"\"\"\n",
    "    return state == self.goal\n",
    "\n",
    "def get_next_state(self, state, action):\n",
    "    \"\"\"计算下一状态\"\"\"\n",
    "    if self.is_terminal(state):\n",
    "        return state\n",
    "        \n",
    "    di, dj = self.ACTION_DELTAS[action]\n",
    "    ni = max(0, min(self.size - 1, state[0] + di))\n",
    "    nj = max(0, min(self.size - 1, state[1] + dj))\n",
    "    next_state = (ni, nj)\n",
    "    \n",
    "    if next_state in self.obstacles:\n",
    "        return state\n",
    "    return next_state\n",
    "\n",
    "def get_reward(self, state, next_state):\n",
    "    \"\"\"获取奖励\"\"\"\n",
    "    if next_state == self.goal:\n",
    "        return self.config.goal_reward\n",
    "    return self.config.step_reward\n",
    "\n",
    "def get_transitions(self, state, action):\n",
    "    \"\"\"获取状态转移分布\"\"\"\n",
    "    if self.is_terminal(state):\n",
    "        return [(state, 1.0, 0.0)]\n",
    "    \n",
    "    next_state = self.get_next_state(state, action)\n",
    "    reward = self.get_reward(state, next_state)\n",
    "    return [(next_state, 1.0, reward)]\n",
    "\n",
    "# 绑定方法到类\n",
    "GridWorld.is_terminal = is_terminal\n",
    "GridWorld.get_next_state = get_next_state\n",
    "GridWorld.get_reward = get_reward\n",
    "GridWorld.get_transitions = get_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境实例\n",
    "env = GridWorld(config)\n",
    "\n",
    "print(\"环境创建成功\")\n",
    "print(f\"  状态空间大小: {len(env.states)}\")\n",
    "print(f\"  动作空间: {env.actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-intro",
   "metadata": {},
   "source": [
    "## 2.3 环境可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid(env, title=\"GridWorld 环境\"):\n",
    "    \"\"\"可视化网格世界\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # 绘制网格线\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    # 标记特殊位置\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            y = env.size - i - 0.5\n",
    "            x = j + 0.5\n",
    "            \n",
    "            if (i, j) == env.config.start:\n",
    "                ax.text(x, y, 'S', ha='center', va='center', \n",
    "                       fontsize=20, fontweight='bold', color='blue')\n",
    "            elif (i, j) == env.goal:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='lightgreen', alpha=0.5))\n",
    "                ax.text(x, y, 'G', ha='center', va='center', \n",
    "                       fontsize=20, fontweight='bold', color='green')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='gray', alpha=0.8))\n",
    "                ax.text(x, y, 'X', ha='center', va='center', \n",
    "                       fontsize=20, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_grid(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"part3\"></a>\n",
    "# Part 3: 动态规划算法\n",
    "\n",
    "## 3.1 策略评估 (Policy Evaluation)\n",
    "\n",
    "### 算法思想\n",
    "\n",
    "给定策略 $\\pi$，通过迭代计算其价值函数 $V^\\pi$。\n",
    "\n",
    "**更新规则**:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "### 为什么这样做？\n",
    "\n",
    "1. **不动点迭代**: 贝尔曼方程定义了 $V^\\pi$ 为不动点\n",
    "2. **收缩映射**: 贝尔曼算子是 $\\gamma$-收缩，保证收敛\n",
    "3. **收敛速度**: 线性收敛，误差以 $\\gamma^k$ 速率衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-policy-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.99, theta=1e-6, max_iters=10000):\n",
    "    \"\"\"\n",
    "    策略评估：计算给定策略的状态价值函数\n",
    "    \n",
    "    Args:\n",
    "        env: 环境\n",
    "        policy: 策略 π(a|s)\n",
    "        gamma: 折扣因子\n",
    "        theta: 收敛阈值\n",
    "        max_iters: 最大迭代次数\n",
    "        \n",
    "    Returns:\n",
    "        (V, iterations): 价值函数和迭代次数\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    \n",
    "    for iteration in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "        \n",
    "        for state in env.states:\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            old_value = V[state]\n",
    "            new_value = 0.0\n",
    "            \n",
    "            # 贝尔曼期望方程\n",
    "            for action in env.actions:\n",
    "                action_prob = policy.get(state, {}).get(action, 0.0)\n",
    "                if action_prob > 0:\n",
    "                    for ns, tp, r in env.get_transitions(state, action):\n",
    "                        new_value += action_prob * tp * (r + gamma * V.get(ns, 0.0))\n",
    "            \n",
    "            V[state] = new_value\n",
    "            delta = max(delta, abs(old_value - new_value))\n",
    "        \n",
    "        if delta < theta:\n",
    "            return V, iteration\n",
    "    \n",
    "    return V, max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-random-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建均匀随机策略\n",
    "random_policy = {\n",
    "    s: {a: 0.25 for a in env.actions}\n",
    "    for s in env.states\n",
    "}\n",
    "\n",
    "print(\"随机策略示例 (每个动作概率相等):\")\n",
    "print(f\"状态 (0,0): {random_policy[(0, 0)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval-random",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估随机策略\n",
    "V_random, iters = policy_evaluation(env, random_policy, gamma=0.99)\n",
    "\n",
    "print(f\"策略评估完成，迭代次数: {iters}\")\n",
    "print(\"\\n状态价值函数 V^π(s):\")\n",
    "for i in range(env.size):\n",
    "    row = []\n",
    "    for j in range(env.size):\n",
    "        if (i, j) in V_random:\n",
    "            row.append(f\"{V_random[(i, j)]:7.2f}\")\n",
    "        else:\n",
    "            row.append(\"   X   \")\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-improvement-intro",
   "metadata": {},
   "source": [
    "## 3.2 策略改进 (Policy Improvement)\n",
    "\n",
    "### 算法思想\n",
    "\n",
    "基于当前价值函数，贪婪选择最优动作：\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "### 策略改进定理\n",
    "\n",
    "如果 $\\pi'$ 是关于 $V^\\pi$ 的贪婪策略，则 $V^{\\pi'}(s) \\geq V^\\pi(s)$ 对所有状态成立。\n",
    "\n",
    "**直觉**: 选择当前看起来最好的动作，不会使策略变差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-policy-improve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "    策略改进：基于价值函数构造贪婪策略\n",
    "    \n",
    "    Args:\n",
    "        env: 环境\n",
    "        V: 状态价值函数\n",
    "        gamma: 折扣因子\n",
    "        \n",
    "    Returns:\n",
    "        改进后的确定性策略\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    \n",
    "    for state in env.states:\n",
    "        if env.is_terminal(state):\n",
    "            policy[state] = {a: 0.25 for a in env.actions}\n",
    "            continue\n",
    "        \n",
    "        # 计算各动作的 Q 值\n",
    "        q_values = {}\n",
    "        for action in env.actions:\n",
    "            q_val = 0.0\n",
    "            for ns, tp, r in env.get_transitions(state, action):\n",
    "                q_val += tp * (r + gamma * V.get(ns, 0.0))\n",
    "            q_values[action] = q_val\n",
    "        \n",
    "        # 选择最优动作\n",
    "        best_value = max(q_values.values())\n",
    "        best_actions = [a for a, v in q_values.items() \n",
    "                       if abs(v - best_value) < 1e-9]\n",
    "        \n",
    "        policy[state] = {\n",
    "            a: 1.0 if a == best_actions[0] else 0.0\n",
    "            for a in env.actions\n",
    "        }\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pi-intro",
   "metadata": {},
   "source": [
    "## 3.3 策略迭代 (Policy Iteration)\n",
    "\n",
    "### 算法流程\n",
    "\n",
    "```\n",
    "π₀ → V^π₀ → π₁ → V^π₁ → π₂ → ... → π* → V*\n",
    "```\n",
    "\n",
    "1. 初始化随机策略 $\\pi_0$\n",
    "2. **策略评估**: 计算 $V^{\\pi_k}$\n",
    "3. **策略改进**: $\\pi_{k+1} = \\text{greedy}(V^{\\pi_k})$\n",
    "4. 若 $\\pi_{k+1} = \\pi_k$ 则停止，否则 $k \\leftarrow k+1$ 转步骤 2\n",
    "\n",
    "### 算法特点\n",
    "\n",
    "| 优点 | 缺点 |\n",
    "|:----:|:----:|\n",
    "| 外层迭代少（3-10次） | 每次评估代价高 |\n",
    "| 理论收敛保证 | 需要完整模型 |\n",
    "| 中间策略可执行 | 大状态空间开销大 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-policy-iteration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, max_iters=100, verbose=True):\n",
    "    \"\"\"\n",
    "    策略迭代算法\n",
    "    \n",
    "    Args:\n",
    "        env: 环境\n",
    "        gamma: 折扣因子\n",
    "        max_iters: 最大迭代次数\n",
    "        verbose: 是否打印进度\n",
    "        \n",
    "    Returns:\n",
    "        (policy, V, iterations)\n",
    "    \"\"\"\n",
    "    # 初始化均匀随机策略\n",
    "    policy = {s: {a: 0.25 for a in env.actions} for s in env.states}\n",
    "    V = {}\n",
    "    \n",
    "    for iteration in range(1, max_iters + 1):\n",
    "        # 策略评估\n",
    "        V, eval_iters = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"迭代 {iteration}: 策略评估用了 {eval_iters} 次内层迭代\")\n",
    "        \n",
    "        # 策略改进\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # 检查收敛\n",
    "        if policy == new_policy:\n",
    "            if verbose:\n",
    "                print(f\"\\n策略迭代收敛，总迭代: {iteration}\")\n",
    "            return new_policy, V, iteration\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy, V, max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-pi",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"运行策略迭代\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "policy_pi, V_pi, iters_pi = policy_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-vi-intro",
   "metadata": {},
   "source": [
    "## 3.4 值迭代 (Value Iteration)\n",
    "\n",
    "### 算法思想\n",
    "\n",
    "直接迭代贝尔曼最优方程，不显式维护策略：\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) [R(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "### 与策略迭代的对比\n",
    "\n",
    "| 维度 | 策略迭代 | 值迭代 |\n",
    "|:----:|:--------:|:------:|\n",
    "| 每次迭代 | 完全策略评估 | 单步贝尔曼更新 |\n",
    "| 迭代次数 | 少（3-10次） | 多（数百次） |\n",
    "| 每次代价 | 高 | 低 |\n",
    "| 中间结果 | 有可执行策略 | 无策略 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-value-iteration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-6, max_iters=10000, verbose=True):\n",
    "    \"\"\"\n",
    "    值迭代算法\n",
    "    \n",
    "    Args:\n",
    "        env: 环境\n",
    "        gamma: 折扣因子\n",
    "        theta: 收敛阈值\n",
    "        max_iters: 最大迭代次数\n",
    "        verbose: 是否打印进度\n",
    "        \n",
    "    Returns:\n",
    "        (policy, V, iterations)\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    \n",
    "    for iteration in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "        \n",
    "        for state in env.states:\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            old_value = V[state]\n",
    "            \n",
    "            # 贝尔曼最优方程\n",
    "            q_values = []\n",
    "            for action in env.actions:\n",
    "                q_val = 0.0\n",
    "                for ns, tp, r in env.get_transitions(state, action):\n",
    "                    q_val += tp * (r + gamma * V.get(ns, 0.0))\n",
    "                q_values.append(q_val)\n",
    "            \n",
    "            V[state] = max(q_values)\n",
    "            delta = max(delta, abs(old_value - V[state]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            if verbose:\n",
    "                print(f\"值迭代收敛，迭代次数: {iteration}\")\n",
    "            policy = policy_improvement(env, V, gamma)\n",
    "            return policy, V, iteration\n",
    "    \n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V, max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-vi",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"运行值迭代\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "policy_vi, V_vi, iters_vi = value_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"part4\"></a>\n",
    "# Part 4: 结果分析\n",
    "\n",
    "## 4.1 可视化策略和价值函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-policy-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy_values(env, policy, V, title):\n",
    "    \"\"\"可视化策略和价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    arrows = {'上': '↑', '下': '↓', '左': '←', '右': '→'}\n",
    "    \n",
    "    # 左图：策略\n",
    "    ax = axes[0]\n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            y = env.size - i - 0.5\n",
    "            x = j + 0.5\n",
    "            \n",
    "            if (i, j) == env.goal:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='lightgreen', alpha=0.5))\n",
    "                ax.text(x, y, 'G', ha='center', va='center', \n",
    "                       fontsize=16, fontweight='bold', color='green')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='gray', alpha=0.8))\n",
    "                ax.text(x, y, 'X', ha='center', va='center', \n",
    "                       fontsize=16, fontweight='bold', color='white')\n",
    "            elif (i, j) in policy:\n",
    "                best_action = max(policy[(i, j)], key=policy[(i, j)].get)\n",
    "                ax.text(x, y, arrows[best_action], ha='center', va='center', \n",
    "                       fontsize=20, color='blue')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('最优策略 π*', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # 右图：价值函数\n",
    "    ax = axes[1]\n",
    "    value_matrix = np.zeros((env.size, env.size))\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            value_matrix[i, j] = V.get((i, j), np.nan)\n",
    "    \n",
    "    im = ax.imshow(value_matrix, cmap='RdYlGn', aspect='equal')\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.goal:\n",
    "                ax.text(j, i, 'G', ha='center', va='center', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.text(j, i, 'X', ha='center', va='center', \n",
    "                       fontsize=14, fontweight='bold', color='white')\n",
    "            elif (i, j) in V:\n",
    "                ax.text(j, i, f'{V[(i, j)]:.1f}', ha='center', va='center', \n",
    "                       fontsize=10)\n",
    "    \n",
    "    ax.set_title('状态价值函数 V*', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-pi-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy_values(env, policy_pi, V_pi, \"策略迭代结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-vi-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy_values(env, policy_vi, V_vi, \"值迭代结果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-compare-intro",
   "metadata": {},
   "source": [
    "## 4.2 算法对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-algos",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"策略迭代 vs 值迭代\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'算法':<20} {'迭代次数':>15} {'特点':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'策略迭代':<20} {iters_pi:>15} {'精确评估，迭代少':>20}\")\n",
    "print(f\"{'值迭代':<20} {iters_vi:>15} {'截断评估，每次代价低':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 验证结果一致性\n",
    "value_diff = sum(abs(V_pi[s] - V_vi[s]) for s in env.states)\n",
    "print(f\"\\n价值函数总差异: {value_diff:.2e}\")\n",
    "\n",
    "if value_diff < 1e-5:\n",
    "    print(\"两种算法收敛到相同的最优解！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exec-intro",
   "metadata": {},
   "source": [
    "## 4.3 策略执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-execute-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_policy(env, policy, max_steps=50):\n",
    "    \"\"\"执行策略并返回轨迹\"\"\"\n",
    "    state = env.config.start\n",
    "    trajectory = [state]\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if env.is_terminal(state):\n",
    "            break\n",
    "        \n",
    "        best_action = max(policy[state], key=policy[state].get)\n",
    "        transitions = env.get_transitions(state, best_action)\n",
    "        next_state, _, reward = transitions[0]\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory.append(state)\n",
    "    \n",
    "    return total_reward, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, trajectory = execute_policy(env, policy_vi)\n",
    "\n",
    "print(\"执行最优策略:\")\n",
    "print(f\"  总奖励: {reward:.1f}\")\n",
    "print(f\"  步数: {len(trajectory) - 1}\")\n",
    "print(f\"  轨迹: {' → '.join([str(s) for s in trajectory])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectory(env, trajectory):\n",
    "    \"\"\"可视化执行轨迹\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    \n",
    "    for i in range(env.size + 1):\n",
    "        ax.axhline(y=i, color='black', linewidth=1)\n",
    "        ax.axvline(x=i, color='black', linewidth=1)\n",
    "    \n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.goal:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='lightgreen', alpha=0.5))\n",
    "            elif (i, j) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((j, env.size-i-1), 1, 1, \n",
    "                            facecolor='gray', alpha=0.8))\n",
    "    \n",
    "    # 绘制轨迹\n",
    "    for idx, (i, j) in enumerate(trajectory):\n",
    "        y = env.size - i - 0.5\n",
    "        x = j + 0.5\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.plot(x, y, 'bo', markersize=15, label='起点')\n",
    "        elif idx == len(trajectory) - 1:\n",
    "            ax.plot(x, y, 'g*', markersize=20, label='终点')\n",
    "        else:\n",
    "            ax.plot(x, y, 'r.', markersize=10)\n",
    "    \n",
    "    xs = [t[1] + 0.5 for t in trajectory]\n",
    "    ys = [env.size - t[0] - 0.5 for t in trajectory]\n",
    "    ax.plot(xs, ys, 'r-', linewidth=2, alpha=0.7, label='轨迹')\n",
    "    \n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('最优策略执行轨迹', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_trajectory(env, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-gamma-exp-intro",
   "metadata": {},
   "source": [
    "## 4.4 参数实验：折扣因子 γ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gamma-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.5, 0.9, 0.99]\n",
    "\n",
    "print(\"不同折扣因子 γ 对价值函数的影响:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for gamma in gammas:\n",
    "    _, V_gamma, iters = value_iteration(env, gamma=gamma, verbose=False)\n",
    "    \n",
    "    print(f\"\\nγ = {gamma}, 收敛迭代: {iters}\")\n",
    "    print(f\"  起点 V(0,0) = {V_gamma[(0, 0)]:.2f}\")\n",
    "    print(f\"  目标邻近 V(3,2) = {V_gamma[(3, 2)]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"观察：γ 越大，远离目标的状态价值越高（更重视长期回报）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"part5\"></a>\n",
    "# Part 5: 总结与练习\n",
    "\n",
    "## 核心知识点回顾\n",
    "\n",
    "### MDP 五元组\n",
    "\n",
    "$$\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$$\n",
    "\n",
    "### 贝尔曼方程\n",
    "\n",
    "| 类型 | 方程 | 用途 |\n",
    "|:----:|:----:|:----:|\n",
    "| 期望方程 | $V^\\pi(s) = \\sum_a \\pi(a|s)[R + \\gamma \\sum_{s'} P V^\\pi(s')]$ | 策略评估 |\n",
    "| 最优方程 | $V^*(s) = \\max_a [R + \\gamma \\sum_{s'} P V^*(s')]$ | 值迭代 |\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 维度 | 策略迭代 | 值迭代 |\n",
    "|:----:|:--------:|:------:|\n",
    "| 外层迭代 | 少 | 多 |\n",
    "| 内层代价 | 高 | 低 |\n",
    "| 中间策略 | 有 | 无 |\n",
    "| 适用场景 | 小状态空间 | 大状态空间 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-unit-tests",
   "metadata": {},
   "source": [
    "## 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: 状态空间\n",
    "    try:\n",
    "        assert len(env.states) == 15\n",
    "        assert (1, 1) not in env.states\n",
    "        print(\"测试1通过: 状态空间正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: 终止状态\n",
    "    try:\n",
    "        assert env.is_terminal((3, 3)) == True\n",
    "        assert env.is_terminal((0, 0)) == False\n",
    "        print(\"测试2通过: 终止状态判断正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: 状态转移\n",
    "    try:\n",
    "        next_s = env.get_next_state((0, 0), '下')\n",
    "        assert next_s == (1, 0)\n",
    "        next_s = env.get_next_state((0, 0), '上')\n",
    "        assert next_s == (0, 0)  # 边界\n",
    "        print(\"测试3通过: 状态转移正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: 策略迭代收敛\n",
    "    try:\n",
    "        assert iters_pi <= 10\n",
    "        print(f\"测试4通过: 策略迭代在 {iters_pi} 次收敛\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试5: 结果一致性\n",
    "    try:\n",
    "        diff = sum(abs(V_pi[s] - V_vi[s]) for s in env.states)\n",
    "        assert diff < 1e-5\n",
    "        print(\"测试5通过: 两种算法结果一致\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试5失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试6: 最优策略到达目标\n",
    "    try:\n",
    "        reward, traj = execute_policy(env, policy_vi)\n",
    "        assert traj[-1] == env.goal\n",
    "        print(f\"测试6通过: 最优策略在 {len(traj)-1} 步到达目标\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试6失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    return failed == 0\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercises",
   "metadata": {},
   "source": [
    "## 练习题\n",
    "\n",
    "### 练习 1: 修改环境\n",
    "\n",
    "创建一个带有更多障碍物的 5×5 网格，观察最优策略的变化。\n",
    "\n",
    "### 练习 2: 随机环境\n",
    "\n",
    "修改 `get_transitions` 方法实现随机转移（滑动概率 20%），比较确定性和随机环境下的策略差异。\n",
    "\n",
    "### 练习 3: 收敛分析\n",
    "\n",
    "绘制不同 γ 值下值迭代的收敛曲线（迭代次数 vs 最大价值变化）。\n",
    "\n",
    "### 练习 4: Q-Learning 预热\n",
    "\n",
    "实现从 V 计算 Q 的函数，为下一章 Q-Learning 做准备：\n",
    "\n",
    "$$Q(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-references",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Sutton & Barto, *Reinforcement Learning: An Introduction* (2nd ed.), Chapter 4\n",
    "2. Bellman, R. *Dynamic Programming*, Princeton University Press, 1957\n",
    "3. Howard, R. *Dynamic Programming and Markov Processes*, MIT Press, 1960\n",
    "4. David Silver, UCL RL Course, Lectures 2-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
