# MDP Basics: 知识点总结

> **马尔可夫决策过程与动态规划核心知识手册**

---

## 目录

1. [核心概念速查](#核心概念速查)
2. [MDP 五元组详解](#mdp-五元组详解)
3. [贝尔曼方程精要](#贝尔曼方程精要)
4. [动态规划算法](#动态规划算法)
5. [算法对比与选择](#算法对比与选择)
6. [面试高频问题](#面试高频问题)
7. [代码模板](#代码模板)
8. [学习路径](#学习路径)

---

## 核心概念速查

### 一句话理解

| 概念 | 本质 |
|:----:|:-----|
| **MDP** | 描述序贯决策问题的数学框架 |
| **马尔可夫性** | 未来只依赖现在，与历史无关 |
| **策略 π** | 状态到动作的映射规则 |
| **价值函数 V** | 从某状态出发的期望累积回报 |
| **贝尔曼方程** | 当前价值 = 即时奖励 + 折扣后续价值 |
| **策略迭代** | 评估-改进交替，外层迭代少 |
| **值迭代** | 直接迭代最优方程，每步代价低 |

### 关键公式卡片

```
┌─────────────────────────────────────────────────────────┐
│  贝尔曼期望方程 (Policy Evaluation)                      │
│                                                         │
│  V^π(s) = Σ_a π(a|s) [R(s,a) + γ Σ_{s'} P(s'|s,a) V^π(s')]  │
│                                                         │
│  含义: 当前价值 = Σ(动作概率 × [即时奖励 + 折扣×后继价值])   │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│  贝尔曼最优方程 (Value Iteration)                        │
│                                                         │
│  V*(s) = max_a [R(s,a) + γ Σ_{s'} P(s'|s,a) V*(s')]     │
│                                                         │
│  含义: 最优价值 = 选择最优动作后的最大期望回报              │
└─────────────────────────────────────────────────────────┘
```

---

## MDP 五元组详解

### 定义

$$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$

### 各组件解析

#### 1. 状态空间 $\mathcal{S}$

**定义**: 所有可能状态的集合

**类型**:
- 离散: 棋盘格局、网格位置
- 连续: 机器人关节角度、车辆位置

**设计要点**:
- 状态应满足马尔可夫性（包含预测未来所需的全部信息）
- 避免状态冗余，减少状态空间大小

#### 2. 动作空间 $\mathcal{A}$

**定义**: 智能体可采取的行为集合

**类型**:
- 离散: {上, 下, 左, 右}
- 连续: $[-1, 1]^n$ (扭矩向量)

**注意**: 动作空间可以是状态相关的 $\mathcal{A}(s)$

#### 3. 转移函数 $P$

**定义**: $P(s' | s, a) = \Pr(S_{t+1} = s' | S_t = s, A_t = a)$

**类型**:
- 确定性: $P \in \{0, 1\}$
- 随机性: $\sum_{s'} P(s'|s,a) = 1$

**表示方法**:
- 表格形式: $|S| \times |A| \times |S|$ 矩阵
- 模型形式: 模拟器/环境

#### 4. 奖励函数 $R$

**常见形式**:
- $R(s, a, s')$: 最通用
- $R(s, a)$: 期望奖励
- $R(s)$: 仅依赖状态

**设计原则**:
1. 稀疏 vs 密集: 稀疏更自然但学习困难
2. 避免奖励塑形副作用
3. 防止奖励黑客行为

#### 5. 折扣因子 $\gamma$

**作用**: 控制对未来奖励的重视程度

| $\gamma$ 值 | 效果 | 适用场景 |
|:-----------:|:-----|:---------|
| $\gamma = 0$ | 完全短视 | 即时决策 |
| $\gamma \approx 0.9$ | 平衡短长期 | 一般任务 |
| $\gamma \approx 0.99$ | 重视长期 | 延迟奖励任务 |
| $\gamma = 1$ | 无折扣 | 有限时间任务 |

**数学作用**:
- 保证无限序列求和收敛
- 影响值迭代收敛速度

---

## 贝尔曼方程精要

### 四种贝尔曼方程

```
          策略π                    最优*
        ┌──────────────────────────────────┐
状态V   │ V^π(s) = E_π[R + γV^π(s')]       │ V*(s) = max_a E[R + γV*(s')]
        │ [策略评估]                        │ [值迭代]
        ├──────────────────────────────────┤
动作Q   │ Q^π(s,a) = E[R + γE_π[Q^π(s',a')]]│ Q*(s,a) = E[R + γmax_{a'}Q*(s',a')]
        │ [SARSA基础]                       │ [Q-Learning基础]
        └──────────────────────────────────┘
```

### V 与 Q 的关系

**从 V 到 Q**:
$$Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')$$

**从 Q 到 V**:
$$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$$

**从 Q 到最优策略**:
$$\pi^*(s) = \arg\max_a Q^*(s,a)$$

### 备份图直觉

```
      V(s)                    Q(s,a)
       │                        │
   ┌───┼───┐                ┌───┴───┐
   a1  a2  a3               s'1    s'2
   │   │   │                │      │
   s'  s'  s'           ┌───┼───┐ ┌┴─┐
                        a'1 a'2 a'3 ...

V 备份: 对动作求期望       Q 备份: 对下一状态的 Q 求期望
```

---

## 动态规划算法

### 算法总览

```
                    初始化
                       │
          ┌────────────┴────────────┐
          │                         │
     策略迭代 PI                  值迭代 VI
          │                         │
    ┌─────┴─────┐                   │
    │           │                   │
  评估 E      改进 I              单步 max
    │           │                   │
    ▼           ▼                   ▼
  V^π_k      π_{k+1}              V_{k+1}
    │           │                   │
    └─────┬─────┘                   │
          │                         │
      π 不变?                    Δ < θ?
          │                         │
          ▼                         ▼
        π*, V*                    π*, V*
```

### 策略评估 (Policy Evaluation)

**输入**: 策略 π, 环境 (P, R, γ)
**输出**: 价值函数 V^π

**算法**:
```python
def policy_evaluation(policy, gamma, theta):
    V = {s: 0 for s in states}
    while True:
        delta = 0
        for s in states:
            v = V[s]
            V[s] = sum(
                policy[s][a] * sum(
                    P(s'|s,a) * (R + gamma * V[s'])
                    for s' in next_states(s, a)
                )
                for a in actions
            )
            delta = max(delta, |v - V[s]|)
        if delta < theta:
            return V
```

**复杂度**: O(|S|²|A|) 每次迭代

### 策略改进 (Policy Improvement)

**输入**: 价值函数 V, 环境 (P, R, γ)
**输出**: 改进后的策略 π'

**算法**:
```python
def policy_improvement(V, gamma):
    policy = {}
    for s in states:
        q_values = {
            a: sum(P(s'|s,a) * (R + gamma * V[s']) for s' in ...)
            for a in actions
        }
        best_a = argmax(q_values)
        policy[s] = {a: 1 if a == best_a else 0 for a in actions}
    return policy
```

**核心定理**: 策略改进定理保证 $V^{\pi'} \geq V^\pi$

### 策略迭代 (Policy Iteration)

**算法流程**:
```
1. 初始化随机策略 π_0
2. 循环:
   a. 策略评估: V^π_k ← solve(π_k)  [完全收敛]
   b. 策略改进: π_{k+1} ← greedy(V^π_k)
   c. 若 π_{k+1} = π_k, 返回
3. 返回 π*, V*
```

**收敛性**:
- 策略单调改进
- 有限策略空间 → 必收敛
- 通常 3-10 次外层迭代

### 值迭代 (Value Iteration)

**算法流程**:
```
1. 初始化 V_0(s) = 0 ∀s
2. 循环:
   V_{k+1}(s) = max_a [R(s,a) + γ Σ P(s'|s,a) V_k(s')]
3. |V_{k+1} - V_k| < θ 时停止
4. 提取策略: π(s) = argmax_a [...]
```

**收敛性**:
- 基于收缩映射定理
- 误差: $\|V_k - V^*\| \leq \gamma^k \|V_0 - V^*\|$

---

## 算法对比与选择

### 对比表

| 维度 | 策略迭代 | 值迭代 |
|:----:|:--------:|:------:|
| **外层迭代** | 少 (3-10) | 多 (数百) |
| **每次代价** | 高 (完全评估) | 低 (单步) |
| **中间策略** | 有 | 无 |
| **收敛判据** | 策略不变 | 价值收敛 |
| **γ敏感性** | 较低 | 较高 |
| **并行友好** | 一般 | 高 |

### 选择指南

```
                 状态空间大小
                      │
           ┌──────────┼──────────┐
           小                    大
      (< 10^4)                (> 10^4)
           │                      │
      策略迭代              值迭代/异步DP
           │                      │
       需要中间         ┌─────────┼─────────┐
       策略?           GPU       分布式      单机
           │            │         │          │
     ┌─────┴─────┐   批量VI   分布式VI    优先扫描
     是          否
     │           │
 策略迭代    任选
```

### 统一视角: 广义策略迭代 (GPI)

值迭代是策略迭代的特例:
- 策略迭代: 评估到收敛 (k = ∞)
- 值迭代: 评估一步 (k = 1)
- 一般化: 评估 k 步 (1 < k < ∞)

```
        评估                改进
    ┌─────────┐         ┌─────────┐
    │  V^π    │ ◄────── │    π    │
    └────┬────┘         └────┬────┘
         │                   ▲
         └───────────────────┘
            (GPI 循环)
```

---

## 面试高频问题

### Q1: 什么是马尔可夫性？为什么重要？

**答**: 马尔可夫性指 $P(s_{t+1}|s_t, s_{t-1}, ...) = P(s_{t+1}|s_t)$，即未来只依赖当前状态。

**重要性**:
1. 使价值函数定义合理（只依赖状态）
2. 使贝尔曼方程成立（递归分解）
3. 降低问题复杂度（无需存储历史）

### Q2: 策略迭代和值迭代的本质区别？

**答**: 评估的完整程度不同。

- **策略迭代**: 完全求解 V^π，再改进
- **值迭代**: 每步只做一次 Bellman backup

**类比**:
- 策略迭代 = 做完所有作业再看答案
- 值迭代 = 做一题看一题答案

### Q3: 为什么 γ < 1？

**答**:
1. **数学**: 保证无限序列求和收敛
2. **算法**: 收缩映射保证唯一不动点
3. **实际**: 未来不确定性增加，应降低权重

**特例**: γ = 1 仅在有限时间或保证终止的任务中可用

### Q4: 动态规划的局限是什么？

**答**:
1. **需要模型**: 必须知道 P(s'|s,a) 和 R
2. **维度灾难**: 状态空间指数增长
3. **表格存储**: 无法处理连续状态

**解决方向**: 无模型方法 (Q-Learning)、函数逼近 (DQN)

### Q5: 如何理解 "bootstrap"？

**答**: 用旧估计更新新估计，不等待完整回报。

$$V(s) \leftarrow R + \gamma V(s')$$

**对比**:
- Monte Carlo: 等待完整轨迹
- TD/DP: 用下一状态估计值

---

## 代码模板

### 策略评估模板

```python
def policy_evaluation(env, policy, gamma=0.99, theta=1e-6):
    """
    策略评估: V^π

    时间复杂度: O(|S|²|A|) per iteration
    空间复杂度: O(|S|)
    """
    V = {s: 0.0 for s in env.states}

    while True:
        delta = 0.0
        for s in env.states:
            if env.is_terminal(s):
                continue

            v = V[s]
            V[s] = sum(
                policy[s][a] * sum(
                    p * (r + gamma * V[s_])
                    for s_, p, r in env.transitions(s, a)
                )
                for a in env.actions
            )
            delta = max(delta, abs(v - V[s]))

        if delta < theta:
            return V
```

### 值迭代模板

```python
def value_iteration(env, gamma=0.99, theta=1e-6):
    """
    值迭代: V* → π*

    时间复杂度: O(|S||A|) per iteration
    空间复杂度: O(|S|)
    """
    V = {s: 0.0 for s in env.states}

    while True:
        delta = 0.0
        for s in env.states:
            if env.is_terminal(s):
                continue

            v = V[s]
            V[s] = max(
                sum(p * (r + gamma * V[s_])
                    for s_, p, r in env.transitions(s, a))
                for a in env.actions
            )
            delta = max(delta, abs(v - V[s]))

        if delta < theta:
            break

    # Extract policy
    policy = {}
    for s in env.states:
        q = {a: sum(p * (r + gamma * V[s_])
                    for s_, p, r in env.transitions(s, a))
             for a in env.actions}
        policy[s] = max(q, key=q.get)

    return V, policy
```

---

## 学习路径

### 阶段一: 理论基础 (本章)
- [x] MDP 定义与组件
- [x] 贝尔曼方程推导
- [x] 策略评估
- [x] 策略迭代
- [x] 值迭代

### 阶段二: 无模型方法
- [ ] 蒙特卡洛方法
- [ ] 时序差分 (TD)
- [ ] SARSA (在策略)
- [ ] Q-Learning (离策略)

### 阶段三: 函数逼近
- [ ] 线性函数逼近
- [ ] DQN 及其变体
- [ ] 经验回放
- [ ] 目标网络

### 阶段四: 策略梯度
- [ ] REINFORCE
- [ ] Actor-Critic
- [ ] A2C/A3C
- [ ] PPO/TRPO

### 阶段五: 高级主题
- [ ] SAC (最大熵 RL)
- [ ] 多智能体 RL
- [ ] 模型学习与规划
- [ ] 离线强化学习

---

## 快速复习清单

### 核心公式
- [ ] 贝尔曼期望方程
- [ ] 贝尔曼最优方程
- [ ] V 和 Q 的关系
- [ ] 累积回报定义

### 算法步骤
- [ ] 策略评估迭代更新
- [ ] 策略改进贪婪选择
- [ ] 策略迭代收敛判据
- [ ] 值迭代收敛判据

### 直觉理解
- [ ] 马尔可夫性的含义
- [ ] 折扣因子的作用
- [ ] bootstrap 的概念
- [ ] 收缩映射与收敛

---

*最后更新: 2024*

*相关代码: `src/` 目录*
