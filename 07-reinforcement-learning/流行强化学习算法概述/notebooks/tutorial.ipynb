{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 流行强化学习算法概述\n",
    "# Popular Reinforcement Learning Algorithms Overview\n",
    "\n",
    "---\n",
    "\n",
    "本教程深入讲解三种最流行的连续控制强化学习算法:\n",
    "\n",
    "| 算法 | 策略类型 | 核心创新 | 适用场景 |\n",
    "|------|----------|----------|----------|\n",
    "| **DDPG** | 确定性 | 将DQN扩展到连续动作空间 | 入门学习 |\n",
    "| **TD3** | 确定性 | 三重改进解决DDPG不稳定 | 稳定训练 |\n",
    "| **SAC** | 随机性 | 最大熵框架自动探索 | 生产部署 |\n",
    "\n",
    "**学习目标**:\n",
    "1. 理解Actor-Critic架构的数学原理\n",
    "2. 掌握三种算法的核心差异和选择依据\n",
    "3. 能够独立实现和调试这些算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境配置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子保证可复现性\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 检查设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入本地模块\n",
    "from core.config import BaseConfig\n",
    "from core.buffer import ReplayBuffer\n",
    "from core.networks import (\n",
    "    DeterministicActor, GaussianActor,\n",
    "    QNetwork, TwinQNetwork\n",
    ")\n",
    "\n",
    "from algorithms.ddpg import DDPGConfig, DDPGAgent\n",
    "from algorithms.td3 import TD3Config, TD3Agent\n",
    "from algorithms.sac import SACConfig, SACAgent\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 理论基础: 马尔可夫决策过程 (MDP)\n",
    "\n",
    "强化学习问题通常建模为**马尔可夫决策过程** $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "| 符号 | 含义 | 说明 |\n",
    "|------|------|------|\n",
    "| $\\mathcal{S}$ | 状态空间 | 环境所有可能状态的集合 |\n",
    "| $\\mathcal{A}$ | 动作空间 | 智能体可执行的所有动作 |\n",
    "| $P(s'|s,a)$ | 转移概率 | 执行动作后状态转移的概率 |\n",
    "| $R(s,a,s')$ | 奖励函数 | 转移获得的即时奖励 |\n",
    "| $\\gamma \\in [0,1]$ | 折扣因子 | 未来奖励的权重衰减 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 价值函数\n",
    "\n",
    "**状态价值函数** $V^\\pi(s)$: 从状态 $s$ 出发,遵循策略 $\\pi$ 的期望累积回报\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s\\right]$$\n",
    "\n",
    "**动作价值函数** $Q^\\pi(s, a)$: 在状态 $s$ 执行动作 $a$,然后遵循策略 $\\pi$ 的期望回报\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a\\right]$$\n",
    "\n",
    "两者关系:\n",
    "$$V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi}[Q^\\pi(s, a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 贝尔曼方程\n",
    "\n",
    "**贝尔曼期望方程** (策略评估):\n",
    "\n",
    "$$Q^\\pi(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s', a'}[Q^\\pi(s', a')]$$\n",
    "\n",
    "**贝尔曼最优方程** (策略优化):\n",
    "\n",
    "$$Q^*(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^*(s', a')]$$\n",
    "\n",
    "这是所有价值学习方法的核心—通过迭代逼近真实价值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化折扣因子的影响\n",
    "def visualize_discount():\n",
    "    \"\"\"展示不同 gamma 对未来奖励权重的影响\"\"\"\n",
    "    steps = np.arange(50)\n",
    "    gammas = [0.9, 0.95, 0.99]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        weights = gamma ** steps\n",
    "        ax.plot(steps, weights, label=f'γ = {gamma}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Future Timestep', fontsize=12)\n",
    "    ax.set_ylabel('Reward Weight', fontsize=12)\n",
    "    ax.set_title('Discount Factor: Impact on Future Reward Weighting', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_discount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 核心组件详解\n",
    "\n",
    "### 3.1 经验回放缓冲区 (Replay Buffer)\n",
    "\n",
    "**为什么需要经验回放?**\n",
    "\n",
    "1. **打破时序相关性**: 连续采样的数据高度相关,违反SGD的i.i.d.假设\n",
    "2. **提高样本效率**: 每个transition可被多次使用\n",
    "3. **稳定训练**: 防止灾难性遗忘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 ReplayBuffer 的使用\n",
    "buffer = ReplayBuffer(capacity=1000, state_dim=4, action_dim=2)\n",
    "\n",
    "# 模拟存储经验\n",
    "for i in range(500):\n",
    "    state = np.random.randn(4).astype(np.float32)\n",
    "    action = np.random.randn(2).astype(np.float32)\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randn(4).astype(np.float32)\n",
    "    done = i % 50 == 49  # 每50步结束一个episode\n",
    "    \n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "print(f\"Buffer ready for batch_size=64: {buffer.is_ready(64)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样一个batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "\n",
    "print(\"Batch shapes:\")\n",
    "print(f\"  States: {states.shape}\")\n",
    "print(f\"  Actions: {actions.shape}\")\n",
    "print(f\"  Rewards: {rewards.shape}\")\n",
    "print(f\"  Next states: {next_states.shape}\")\n",
    "print(f\"  Dones: {dones.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 神经网络架构\n",
    "\n",
    "Actor-Critic 架构包含两个网络:\n",
    "\n",
    "| 网络 | 输入 | 输出 | 作用 |\n",
    "|------|------|------|------|\n",
    "| **Actor** (策略网络) | 状态 $s$ | 动作 $a$ | 决定采取什么动作 |\n",
    "| **Critic** (价值网络) | 状态 $s$, 动作 $a$ | Q值 | 评估动作的好坏 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建确定性 Actor (用于 DDPG/TD3)\n",
    "state_dim = 11\n",
    "action_dim = 3\n",
    "max_action = 1.0\n",
    "\n",
    "det_actor = DeterministicActor(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    max_action=max_action,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "# 测试前向传播\n",
    "test_state = torch.randn(1, state_dim)\n",
    "action = det_actor(test_state)\n",
    "\n",
    "print(f\"Deterministic Actor:\")\n",
    "print(f\"  Input shape: {test_state.shape}\")\n",
    "print(f\"  Output shape: {action.shape}\")\n",
    "print(f\"  Action bounds: [{action.min().item():.3f}, {action.max().item():.3f}]\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in det_actor.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建随机 Actor (用于 SAC)\n",
    "gauss_actor = GaussianActor(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    max_action=max_action,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "# 随机采样\n",
    "action, log_prob = gauss_actor.sample(test_state)\n",
    "\n",
    "print(f\"\\nGaussian Actor:\")\n",
    "print(f\"  Action shape: {action.shape}\")\n",
    "print(f\"  Log prob shape: {log_prob.shape}\")\n",
    "print(f\"  Action bounds: [{action.min().item():.3f}, {action.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示随机策略的多样性\n",
    "test_state_batch = test_state.repeat(100, 1)\n",
    "actions_sampled, _ = gauss_actor.sample(test_state_batch)\n",
    "\n",
    "fig, axes = plt.subplots(1, action_dim, figsize=(12, 3))\n",
    "for i in range(action_dim):\n",
    "    axes[i].hist(actions_sampled[:, i].detach().numpy(), bins=20, edgecolor='black')\n",
    "    axes[i].set_xlabel(f'Action Dim {i+1}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution for Action {i+1}')\n",
    "\n",
    "plt.suptitle('Gaussian Actor: Action Distribution from Same State', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Twin Q-Networks\n",
    "\n",
    "**Q值过估计问题**:\n",
    "\n",
    "$$\\mathbb{E}[\\max_a Q(s', a)] \\geq \\max_a \\mathbb{E}[Q(s', a)]$$\n",
    "\n",
    "由于噪声,对最大值的估计总是偏高。\n",
    "\n",
    "**解决方案**: 使用两个独立的Q网络,取最小值:\n",
    "\n",
    "$$y = r + \\gamma \\min_{i=1,2} Q_{\\phi'_i}(s', a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Twin Q-Network\n",
    "twin_q = TwinQNetwork(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dims=[256, 256]\n",
    ")\n",
    "\n",
    "test_action = torch.randn(1, action_dim)\n",
    "q1, q2 = twin_q(test_state, test_action)\n",
    "q_min = twin_q.min_q(test_state, test_action)\n",
    "\n",
    "print(f\"Twin Q-Network:\")\n",
    "print(f\"  Q1: {q1.item():.4f}\")\n",
    "print(f\"  Q2: {q2.item():.4f}\")\n",
    "print(f\"  min(Q1, Q2): {q_min.item():.4f}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in twin_q.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. DDPG: Deep Deterministic Policy Gradient\n",
    "\n",
    "### 4.1 核心思想\n",
    "\n",
    "DDPG 将 DQN 扩展到连续动作空间:\n",
    "\n",
    "- **确定性策略**: $a = \\mu_\\theta(s)$ 直接输出动作\n",
    "- **经验回放**: 打破样本相关性\n",
    "- **目标网络**: 稳定训练目标\n",
    "\n",
    "### 4.2 数学原理\n",
    "\n",
    "**确定性策略梯度定理**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu}\\left[\\nabla_a Q(s, a)|_{a=\\mu(s)} \\nabla_\\theta \\mu_\\theta(s)\\right]$$\n",
    "\n",
    "**直觉理解**: 沿着Q函数对动作的梯度方向更新策略,使输出的动作获得更高的Q值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 DDPG Agent\n",
    "ddpg_config = DDPGConfig(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    max_action=1.0,\n",
    "    hidden_dims=[64, 64],  # 小网络用于演示\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    lr_actor=1e-4,\n",
    "    lr_critic=1e-3,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    exploration_noise=0.1,\n",
    ")\n",
    "\n",
    "ddpg_agent = DDPGAgent(ddpg_config)\n",
    "print(f\"DDPG Agent created on device: {ddpg_agent.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示动作选择\n",
    "state = np.random.randn(4).astype(np.float32)\n",
    "\n",
    "# 训练模式 (带噪声)\n",
    "ddpg_agent.train_mode()\n",
    "actions_train = [ddpg_agent.select_action(state) for _ in range(10)]\n",
    "\n",
    "# 评估模式 (确定性)\n",
    "ddpg_agent.eval_mode()\n",
    "actions_eval = [ddpg_agent.select_action(state, deterministic=True) for _ in range(10)]\n",
    "\n",
    "print(\"Training mode (with exploration noise):\")\n",
    "for i, a in enumerate(actions_train[:3]):\n",
    "    print(f\"  Action {i+1}: {a}\")\n",
    "\n",
    "print(\"\\nEvaluation mode (deterministic):\")\n",
    "for i, a in enumerate(actions_eval[:3]):\n",
    "    print(f\"  Action {i+1}: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 DDPG 的问题\n",
    "\n",
    "DDPG 存在以下已知问题:\n",
    "\n",
    "1. **Q值过估计**: 单一Q网络容易高估价值\n",
    "2. **训练不稳定**: 策略和价值函数相互影响导致振荡\n",
    "3. **探索不足**: 高斯噪声可能不够有效\n",
    "\n",
    "这些问题催生了 TD3 和 SAC。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. TD3: Twin Delayed DDPG\n",
    "\n",
    "### 5.1 三重改进\n",
    "\n",
    "TD3 通过三个关键改进解决 DDPG 的问题:\n",
    "\n",
    "| 改进 | 解决的问题 | 实现方式 |\n",
    "|------|------------|----------|\n",
    "| **Clipped Double Q** | Q值过估计 | $\\min(Q_1, Q_2)$ |\n",
    "| **Delayed Policy Update** | 训练不稳定 | 每d步更新一次Actor |\n",
    "| **Target Policy Smoothing** | Q函数脆弱性 | 目标动作加噪声 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 详细解析\n",
    "\n",
    "**1. Clipped Double Q-Learning**\n",
    "\n",
    "$$y = r + \\gamma \\min_{i=1,2} Q_{\\phi'_i}(s', \\tilde{a}')$$\n",
    "\n",
    "使用两个Q网络的最小值作为目标,提供保守的价值估计。\n",
    "\n",
    "**2. Delayed Policy Updates**\n",
    "\n",
    "每 $d$ 次 Critic 更新才更新一次 Actor (通常 $d=2$),让 Critic 先稳定。\n",
    "\n",
    "**3. Target Policy Smoothing**\n",
    "\n",
    "$$\\tilde{a}' = \\text{clip}(\\mu_{\\theta'}(s') + \\text{clip}(\\epsilon, -c, c), -a_{max}, a_{max})$$\n",
    "\n",
    "在目标动作上加入裁剪的噪声,平滑 Q 函数,防止策略利用 Q 函数的尖峰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 TD3 Agent\n",
    "td3_config = TD3Config(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    max_action=1.0,\n",
    "    hidden_dims=[64, 64],\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    lr_actor=3e-4,\n",
    "    lr_critic=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    policy_delay=2,      # 每2次Critic更新才更新Actor\n",
    "    target_noise=0.2,    # 目标策略噪声\n",
    "    noise_clip=0.5,      # 噪声裁剪范围\n",
    "    exploration_noise=0.1,\n",
    ")\n",
    "\n",
    "td3_agent = TD3Agent(td3_config)\n",
    "print(f\"TD3 Agent created\")\n",
    "print(f\"  Policy delay: {td3_config.policy_delay}\")\n",
    "print(f\"  Target noise: {td3_config.target_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 delayed policy update\n",
    "# 填充缓冲区\n",
    "for _ in range(200):\n",
    "    s = np.random.randn(4).astype(np.float32)\n",
    "    a = np.random.randn(2).astype(np.float32)\n",
    "    td3_agent.store_transition(s, a, 1.0, s, False)\n",
    "\n",
    "# 执行多次更新,观察 actor_loss 的出现模式\n",
    "for i in range(5):\n",
    "    metrics = td3_agent.update()\n",
    "    has_actor = \"actor_loss\" in metrics\n",
    "    print(f\"Update {i+1}: critic_loss={metrics['critic_loss']:.4f}, \"\n",
    "          f\"actor_loss={'%.4f' % metrics['actor_loss'] if has_actor else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. SAC: Soft Actor-Critic\n",
    "\n",
    "### 6.1 最大熵强化学习\n",
    "\n",
    "SAC 的目标不仅是最大化回报,还要最大化策略的**熵**:\n",
    "\n",
    "$$J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi}\\left[r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))\\right]$$\n",
    "\n",
    "其中 $\\mathcal{H}(\\pi) = -\\mathbb{E}[\\log \\pi(a|s)]$ 是熵。\n",
    "\n",
    "**为什么要最大化熵?**\n",
    "\n",
    "1. **鼓励探索**: 高熵意味着策略更随机,自然探索更多状态\n",
    "2. **鲁棒性**: 不会过度依赖单一最优路径\n",
    "3. **多模态**: 能学习多个接近最优的行为模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 软价值函数\n",
    "\n",
    "**软Q函数**:\n",
    "\n",
    "$$Q^\\pi(s, a) = r + \\gamma \\mathbb{E}_{s'}[V^\\pi(s')]$$\n",
    "\n",
    "**软状态价值函数**:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi}[Q^\\pi(s, a) - \\alpha \\log \\pi(a|s)]$$\n",
    "\n",
    "**软贝尔曼备份**:\n",
    "\n",
    "$$y = r + \\gamma (1-d)(\\min_{i=1,2} Q_{\\phi'_i}(s', a') - \\alpha \\log \\pi(a'|s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 自动温度调节\n",
    "\n",
    "温度参数 $\\alpha$ 控制探索-利用的权衡:\n",
    "\n",
    "- $\\alpha \\to 0$: 纯回报最大化 (像 TD3)\n",
    "- $\\alpha \\to \\infty$: 随机策略 (最大熵)\n",
    "\n",
    "SAC 自动学习 $\\alpha$ 来维持目标熵:\n",
    "\n",
    "$$J(\\alpha) = \\mathbb{E}_{a \\sim \\pi}[-\\alpha(\\log \\pi(a|s) + \\bar{\\mathcal{H}})]$$\n",
    "\n",
    "目标熵通常设为 $\\bar{\\mathcal{H}} = -\\dim(\\mathcal{A})$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 SAC Agent\n",
    "sac_config = SACConfig(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    max_action=1.0,\n",
    "    hidden_dims=[64, 64],\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    lr_actor=3e-4,\n",
    "    lr_critic=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    auto_alpha=True,     # 自动调节温度\n",
    "    initial_alpha=0.2,\n",
    ")\n",
    "\n",
    "sac_agent = SACAgent(sac_config)\n",
    "print(f\"SAC Agent created\")\n",
    "print(f\"  Auto alpha: {sac_config.auto_alpha}\")\n",
    "print(f\"  Initial alpha: {sac_agent.alpha:.4f}\")\n",
    "print(f\"  Target entropy: {sac_agent.target_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 SAC 的随机策略\n",
    "state = np.random.randn(4).astype(np.float32)\n",
    "\n",
    "print(\"SAC Stochastic Policy - Same state, different actions:\")\n",
    "for i in range(5):\n",
    "    action = sac_agent.select_action(state, deterministic=False)\n",
    "    print(f\"  Sample {i+1}: {action}\")\n",
    "\n",
    "print(\"\\nSAC Deterministic (evaluation mode):\")\n",
    "for i in range(3):\n",
    "    action = sac_agent.select_action(state, deterministic=True)\n",
    "    print(f\"  Sample {i+1}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示温度自动调节\n",
    "# 填充缓冲区\n",
    "for _ in range(200):\n",
    "    s = np.random.randn(4).astype(np.float32)\n",
    "    a = np.random.randn(2).astype(np.float32)\n",
    "    sac_agent.store_transition(s, a, 1.0, s, False)\n",
    "\n",
    "alphas = [sac_agent.alpha]\n",
    "entropies = []\n",
    "\n",
    "for i in range(100):\n",
    "    metrics = sac_agent.update()\n",
    "    alphas.append(sac_agent.alpha)\n",
    "    if 'entropy' in metrics:\n",
    "        entropies.append(metrics['entropy'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(alphas)\n",
    "ax1.set_xlabel('Update Step')\n",
    "ax1.set_ylabel('Alpha (Temperature)')\n",
    "ax1.set_title('Automatic Temperature Adjustment')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(entropies)\n",
    "ax2.axhline(y=-sac_agent.target_entropy, color='r', linestyle='--', label='Target Entropy')\n",
    "ax2.set_xlabel('Update Step')\n",
    "ax2.set_ylabel('Policy Entropy')\n",
    "ax2.set_title('Policy Entropy Over Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 算法对比与选择指南\n",
    "\n",
    "### 7.1 特性对比\n",
    "\n",
    "| 特性 | DDPG | TD3 | SAC |\n",
    "|------|------|-----|-----|\n",
    "| 策略类型 | 确定性 | 确定性 | 随机性 |\n",
    "| Q网络数量 | 1 | 2 (twin) | 2 (twin) |\n",
    "| 探索方式 | 外部噪声 | 外部噪声 | 熵最大化 |\n",
    "| 温度参数 | 无 | 无 | 自动调节 |\n",
    "| 稳定性 | 低 | 中 | 高 |\n",
    "| 样本效率 | 好 | 好 | 最好 |\n",
    "| 实现复杂度 | 简单 | 中等 | 中等 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 选择建议\n",
    "\n",
    "**选择 SAC 如果**:\n",
    "- 需要稳定可靠的训练\n",
    "- 任务需要探索\n",
    "- 不想调节探索噪声参数\n",
    "\n",
    "**选择 TD3 如果**:\n",
    "- 需要确定性策略\n",
    "- 对计算资源敏感\n",
    "- 任务相对简单\n",
    "\n",
    "**选择 DDPG 如果**:\n",
    "- 教学/学习目的\n",
    "- 作为基准对比\n",
    "- 简单任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化三种算法的特性差异\n",
    "algorithms = ['DDPG', 'TD3', 'SAC']\n",
    "categories = ['Stability', 'Sample Efficiency', 'Simplicity', \n",
    "              'Exploration', 'Hyperparameter Robustness']\n",
    "\n",
    "# 评分 (主观评估, 1-5)\n",
    "scores = {\n",
    "    'DDPG': [2, 3, 5, 2, 2],\n",
    "    'TD3':  [4, 4, 3, 3, 4],\n",
    "    'SAC':  [5, 5, 3, 5, 5],\n",
    "}\n",
    "\n",
    "# 雷达图\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]  # 闭合\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for algo, color in zip(algorithms, colors):\n",
    "    values = scores[algo] + scores[algo][:1]\n",
    "    ax.plot(angles, values, linewidth=2, label=algo, color=color)\n",
    "    ax.fill(angles, values, alpha=0.1, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=10)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.set_title('Algorithm Comparison', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 实践: 完整训练流程演示\n",
    "\n",
    "下面展示如何使用这些算法训练智能体。由于没有实际环境,我们使用模拟数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockEnvironment:\n",
    "    \"\"\"模拟环境用于演示训练流程\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=2, max_action=1.0):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self._step = 0\n",
    "        self._state = None\n",
    "        \n",
    "        class ActionSpace:\n",
    "            def __init__(self, dim, high):\n",
    "                self.shape = (dim,)\n",
    "                self.high = np.array([high] * dim)\n",
    "                self.low = -self.high\n",
    "            def sample(self):\n",
    "                return np.random.uniform(self.low, self.high)\n",
    "        \n",
    "        self.action_space = ActionSpace(action_dim, max_action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self._step = 0\n",
    "        self._state = np.random.randn(self.state_dim).astype(np.float32)\n",
    "        return self._state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "        # 简单奖励: 负距离 + 动作惩罚\n",
    "        reward = -np.sum(self._state**2) - 0.1 * np.sum(action**2)\n",
    "        \n",
    "        # 简单动力学\n",
    "        self._state = (\n",
    "            self._state * 0.95 + \n",
    "            np.tanh(action).sum() * 0.1 * np.ones(self.state_dim) +\n",
    "            np.random.randn(self.state_dim).astype(np.float32) * 0.05\n",
    "        )\n",
    "        \n",
    "        done = self._step >= 100\n",
    "        return self._state, reward, done, False, {}\n",
    "\n",
    "env = MockEnvironment()\n",
    "print(f\"Environment: state_dim={env.state_dim}, action_dim={env.action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_steps=1000, start_steps=200):\n",
    "    \"\"\"简化的训练循环\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_rewards = []\n",
    "    metrics_history = []\n",
    "    \n",
    "    agent.train_mode()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # 选择动作\n",
    "        if step < start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state)\n",
    "        \n",
    "        # 环境交互\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # 学习更新\n",
    "        if step >= start_steps:\n",
    "            metrics = agent.update()\n",
    "            if metrics:\n",
    "                metrics_history.append(metrics)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "    \n",
    "    return episode_rewards, metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练三种算法并比较\n",
    "results = {}\n",
    "\n",
    "# 使用较小的参数进行快速演示\n",
    "num_steps = 2000\n",
    "start_steps = 500\n",
    "\n",
    "# DDPG\n",
    "print(\"Training DDPG...\")\n",
    "ddpg = DDPGAgent(DDPGConfig(\n",
    "    state_dim=4, action_dim=2, max_action=1.0,\n",
    "    hidden_dims=[64, 64], buffer_size=5000, batch_size=64\n",
    "))\n",
    "results['DDPG'] = train_agent(ddpg, MockEnvironment(), num_steps, start_steps)\n",
    "\n",
    "# TD3\n",
    "print(\"Training TD3...\")\n",
    "td3 = TD3Agent(TD3Config(\n",
    "    state_dim=4, action_dim=2, max_action=1.0,\n",
    "    hidden_dims=[64, 64], buffer_size=5000, batch_size=64,\n",
    "    policy_delay=2\n",
    "))\n",
    "results['TD3'] = train_agent(td3, MockEnvironment(), num_steps, start_steps)\n",
    "\n",
    "# SAC\n",
    "print(\"Training SAC...\")\n",
    "sac = SACAgent(SACConfig(\n",
    "    state_dim=4, action_dim=2, max_action=1.0,\n",
    "    hidden_dims=[64, 64], buffer_size=5000, batch_size=64,\n",
    "    auto_alpha=True\n",
    "))\n",
    "results['SAC'] = train_agent(sac, MockEnvironment(), num_steps, start_steps)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练结果\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'DDPG': '#1f77b4', 'TD3': '#ff7f0e', 'SAC': '#2ca02c'}\n",
    "\n",
    "# Episode rewards\n",
    "for name, (rewards, _) in results.items():\n",
    "    ax1.plot(rewards, label=name, color=colors[name], alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Episode Return', fontsize=12)\n",
    "ax1.set_title('Training Progress: Episode Returns', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Critic loss\n",
    "for name, (_, metrics) in results.items():\n",
    "    if metrics:\n",
    "        losses = [m.get('critic_loss', 0) for m in metrics]\n",
    "        ax2.plot(losses, label=name, color=colors[name], alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Update Step', fontsize=12)\n",
    "ax2.set_ylabel('Critic Loss', fontsize=12)\n",
    "ax2.set_title('Training Progress: Critic Loss', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 调参指南\n",
    "\n",
    "### 9.1 推荐超参数\n",
    "\n",
    "| 参数 | DDPG | TD3 | SAC |\n",
    "|------|------|-----|-----|\n",
    "| `lr_actor` | 1e-4 | 3e-4 | 3e-4 |\n",
    "| `lr_critic` | 1e-3 | 3e-4 | 3e-4 |\n",
    "| `gamma` | 0.99 | 0.99 | 0.99 |\n",
    "| `tau` | 0.005 | 0.005 | 0.005 |\n",
    "| `batch_size` | 256 | 256 | 256 |\n",
    "| `buffer_size` | 1e6 | 1e6 | 1e6 |\n",
    "| `hidden_dims` | [256, 256] | [256, 256] | [256, 256] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 常见问题排查\n",
    "\n",
    "**Q值爆炸/NaN**:\n",
    "- 降低学习率\n",
    "- 检查奖励尺度\n",
    "- 添加梯度裁剪\n",
    "\n",
    "**训练不收敛**:\n",
    "- 增加 `start_timesteps` (更多随机探索)\n",
    "- 检查 `gamma` 是否合适\n",
    "- 增大 batch_size\n",
    "\n",
    "**探索不足**:\n",
    "- DDPG/TD3: 增大 `exploration_noise`\n",
    "- SAC: 检查 `alpha` 是否正常 (过低则探索不足)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 总结\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **DDPG** 开创了深度确定性策略梯度,但存在过估计和不稳定问题\n",
    "\n",
    "2. **TD3** 通过三重改进 (Clipped Double Q, Delayed Updates, Target Smoothing) 显著提升稳定性\n",
    "\n",
    "3. **SAC** 引入最大熵框架,实现自动探索-利用平衡,是目前最稳定的选择\n",
    "\n",
    "### 选择建议\n",
    "\n",
    "- **生产环境**: 首选 SAC\n",
    "- **需要确定性**: 选择 TD3\n",
    "- **学习目的**: 从 DDPG 开始\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- 在真实环境 (如 MuJoCo, Gymnasium) 中测试\n",
    "- 尝试调节超参数观察效果\n",
    "- 阅读原始论文深入理解数学推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Lillicrap et al. (2016). \"Continuous control with deep reinforcement learning\" (DDPG)\n",
    "2. Fujimoto et al. (2018). \"Addressing Function Approximation Error in Actor-Critic Methods\" (TD3)\n",
    "3. Haarnoja et al. (2018). \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning\" (SAC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
