# 流行强化学习算法核心知识点

> 本文档系统梳理 DDPG、TD3、SAC 三种算法的核心概念，帮助深入理解和面试复习。

---

## 目录

1. [理论基础](#1-理论基础)
2. [DDPG 深度解析](#2-ddpg-深度解析)
3. [TD3 核心改进](#3-td3-核心改进)
4. [SAC 最大熵框架](#4-sac-最大熵框架)
5. [算法对比与选择](#5-算法对比与选择)
6. [实践调参指南](#6-实践调参指南)
7. [高频面试题](#7-高频面试题)
8. [深度思考题](#8-深度思考题)

---

## 1. 理论基础

### 1.1 马尔可夫决策过程 (MDP)

**定义**: MDP 是一个五元组 $(S, A, P, R, \gamma)$

| 符号 | 名称 | 说明 |
|------|------|------|
| $S$ | 状态空间 | 所有可能状态的集合 |
| $A$ | 动作空间 | 所有可能动作的集合 |
| $P(s'\|s,a)$ | 转移概率 | 状态转移的概率分布 |
| $R(s,a,s')$ | 奖励函数 | 转移获得的即时奖励 |
| $\gamma \in [0,1]$ | 折扣因子 | 未来奖励的衰减系数 |

**马尔可夫性质**: 未来状态只依赖当前状态，与历史无关

$$P(s_{t+1}|s_t, a_t, s_{t-1}, ..., s_0) = P(s_{t+1}|s_t, a_t)$$

### 1.2 价值函数三剑客

**状态价值函数** $V^\pi(s)$:
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\right]$$

从状态 $s$ 出发，遵循策略 $\pi$ 的期望累积回报。

**动作价值函数** $Q^\pi(s, a)$:
$$Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a\right]$$

在状态 $s$ 执行动作 $a$，然后遵循 $\pi$ 的期望回报。

**优势函数** $A^\pi(s, a)$:
$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

动作 $a$ 相对于平均水平的优势。

**三者关系**:
$$V^\pi(s) = \mathbb{E}_{a \sim \pi}[Q^\pi(s, a)]$$
$$Q^\pi(s, a) = R(s,a) + \gamma \mathbb{E}_{s'}[V^\pi(s')]$$

### 1.3 贝尔曼方程

**贝尔曼期望方程** (评估给定策略):

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

$$Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q^\pi(s', a')$$

**贝尔曼最优方程** (找最优策略):

$$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$$

$$Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s', a')$$

### 1.4 策略梯度定理

**随机策略梯度**:

$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a)\right]$$

**确定性策略梯度** (DPG):

$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho^\mu}\left[\nabla_a Q(s, a)|_{a=\mu(s)} \nabla_\theta \mu_\theta(s)\right]$$

**核心区别**:
- 随机策略: 需要对动作积分，方差高
- 确定性策略: 只需链式法则，方差低

---

## 2. DDPG 深度解析

### 2.1 核心创新

DDPG (Deep Deterministic Policy Gradient) 将 DQN 扩展到连续动作空间。

**四大支柱**:

| 组件 | 作用 | 对应 DQN |
|------|------|---------|
| 确定性策略 | 输出连续动作 | argmax 替代 |
| 经验回放 | 打破样本相关 | 相同 |
| 目标网络 | 稳定训练目标 | 相同 |
| 探索噪声 | 保证充分探索 | ε-greedy 替代 |

### 2.2 网络结构

```
Actor Network:  state → [MLP] → tanh → action × max_action
Critic Network: (state, action) → [MLP] → Q-value
```

### 2.3 更新公式

**Critic 更新** (最小化 TD 误差):

$$L(\phi) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[(Q_\phi(s, a) - y)^2\right]$$

其中 TD 目标:
$$y = r + \gamma Q_{\phi'}(s', \mu_{\theta'}(s'))$$

**Actor 更新** (最大化 Q 值):

$$\nabla_\theta J \approx \frac{1}{N} \sum_i \nabla_a Q_\phi(s_i, a)|_{a=\mu_\theta(s_i)} \nabla_\theta \mu_\theta(s_i)$$

**目标网络软更新**:
$$\theta' \leftarrow \tau \theta + (1 - \tau) \theta'$$

### 2.4 DDPG 的问题

1. **Q 值过估计**: 单 Q 网络倾向高估
2. **训练不稳定**: Actor-Critic 相互影响
3. **探索敏感**: 高斯噪声需要调参

---

## 3. TD3 核心改进

### 3.1 三重改进详解

TD3 (Twin Delayed DDPG) 针对 DDPG 三大问题提出三个改进:

#### 改进一: Clipped Double Q-Learning

**问题**: Q 值过估计导致次优策略

**数学原理**: Jensen 不等式
$$\mathbb{E}[\max_a Q(s', a)] \geq \max_a \mathbb{E}[Q(s', a)]$$

**解决方案**: 使用两个独立 Q 网络，取最小值
$$y = r + \gamma \min_{i=1,2} Q_{\phi'_i}(s', \tilde{a}')$$

#### 改进二: Delayed Policy Updates

**问题**: 策略更新基于不准确的 Q 估计

**解决方案**: 每 $d$ 次 Critic 更新才更新一次 Actor (通常 $d=2$)

**直觉**: 让 Critic 先稳定下来，再指导 Actor 改进

#### 改进三: Target Policy Smoothing

**问题**: 策略可能利用 Q 函数的局部误差

**解决方案**: 在目标动作上加噪声
$$\tilde{a}' = \text{clip}(\mu_{\theta'}(s') + \text{clip}(\epsilon, -c, c), -a_{max}, a_{max})$$

**直觉**: 平滑 Q 函数景观，防止策略过拟合

### 3.2 TD3 vs DDPG 关键差异

| 方面 | DDPG | TD3 |
|------|------|-----|
| Q 网络数量 | 1 | 2 (twin) |
| 目标 Q 计算 | $Q'(s', \mu'(s'))$ | $\min(Q'_1, Q'_2)(s', \tilde{a}')$ |
| Actor 更新频率 | 每步 | 每 d 步 |
| 目标动作 | $\mu'(s')$ | $\mu'(s') + \epsilon$ |

---

## 4. SAC 最大熵框架

### 4.1 核心思想

SAC (Soft Actor-Critic) 的目标:
$$J(\pi) = \sum_{t=0}^{T} \mathbb{E}\left[r_t + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]$$

**最大化回报的同时最大化策略熵**。

### 4.2 为什么要最大化熵？

| 好处 | 说明 |
|------|------|
| 自动探索 | 高熵 = 随机性 = 探索 |
| 鲁棒性 | 不过拟合到单一路径 |
| 多模态 | 能学习多个近似最优行为 |
| 加速学习 | 防止过早收敛 |

### 4.3 软价值函数

**软 Q 函数**:
$$Q^\pi(s, a) = r + \gamma \mathbb{E}_{s'}[V^\pi(s')]$$

**软状态价值**:
$$V^\pi(s) = \mathbb{E}_{a \sim \pi}[Q^\pi(s, a) - \alpha \log \pi(a|s)]$$

**软贝尔曼备份**:
$$y = r + \gamma(1-d)(\min_{i=1,2} Q_{\phi'_i}(s', a') - \alpha \log \pi(a'|s'))$$

### 4.4 自动温度调节

温度 $\alpha$ 控制探索-利用权衡:
- $\alpha \to 0$: 纯回报最大化
- $\alpha \to \infty$: 随机策略

**自动调节目标**:
$$J(\alpha) = \mathbb{E}_{a \sim \pi}\left[-\alpha(\log \pi(a|s) + \bar{\mathcal{H}})\right]$$

目标熵通常设为 $\bar{\mathcal{H}} = -\dim(\mathcal{A})$。

### 4.5 重参数化技巧

为了通过采样反向传播，使用重参数化:
$$a = \tanh(\mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon), \quad \epsilon \sim \mathcal{N}(0, I)$$

**带 tanh 校正的对数概率**:
$$\log \pi(a|s) = \log \mathcal{N}(u; \mu, \sigma^2) - \sum_{i=1}^{d} \log(1 - \tanh^2(u_i))$$

---

## 5. 算法对比与选择

### 5.1 特性对比表

| 特性 | DDPG | TD3 | SAC |
|------|------|-----|-----|
| 策略类型 | 确定性 | 确定性 | 随机性 |
| Q 网络数 | 1 | 2 (twin) | 2 (twin) |
| 探索方式 | OU/高斯噪声 | 高斯噪声 | 熵最大化 |
| 策略更新 | 每步 | 延迟 (每d步) | 每步 |
| 温度参数 | 无 | 无 | 自动调节 |
| 稳定性 | 低 | 中 | 高 |
| 样本效率 | 好 | 好 | 最好 |
| 超参敏感度 | 高 | 中 | 低 |

### 5.2 选择决策树

```
需要连续动作控制？
├── 否 → 考虑 DQN 系列
└── 是 → 需要确定性策略？
    ├── 是 → 训练稳定性重要？
    │   ├── 是 → TD3
    │   └── 否 → DDPG (学习/简单任务)
    └── 否 → SAC (推荐默认选择)
```

### 5.3 性能排名 (一般情况)

**稳定性**: SAC > TD3 > DDPG

**样本效率**: SAC > TD3 ≈ DDPG

**最终性能**: SAC ≈ TD3 > DDPG

**实现复杂度**: DDPG < TD3 < SAC

---

## 6. 实践调参指南

### 6.1 推荐超参数

| 参数 | DDPG | TD3 | SAC | 说明 |
|------|------|-----|-----|------|
| `lr_actor` | 1e-4 | 3e-4 | 3e-4 | Actor 学习率 |
| `lr_critic` | 1e-3 | 3e-4 | 3e-4 | Critic 学习率 |
| `gamma` | 0.99 | 0.99 | 0.99 | 折扣因子 |
| `tau` | 0.005 | 0.005 | 0.005 | 软更新系数 |
| `batch_size` | 256 | 256 | 256 | 批次大小 |
| `buffer_size` | 1e6 | 1e6 | 1e6 | 缓冲区容量 |
| `hidden_dims` | [256,256] | [256,256] | [256,256] | 隐藏层 |
| `start_timesteps` | 25000 | 25000 | 10000 | 随机探索步数 |

### 6.2 常见问题排查

**Q 值爆炸/NaN**:
- 降低学习率 (先试 3e-5)
- 检查奖励尺度 (归一化到 [-1, 1])
- 添加梯度裁剪 (`torch.nn.utils.clip_grad_norm_`)

**训练不收敛**:
- 增加 `start_timesteps`
- 检查 `gamma` (过高导致不稳定)
- 增大 `batch_size`
- 检查环境是否正确

**探索不足**:
- DDPG/TD3: 增大 `exploration_noise` (0.1 → 0.3)
- SAC: 检查 `alpha` 是否正常 (应该 > 0.01)

**过拟合特定轨迹**:
- 增大 `buffer_size`
- 检查是否有状态/奖励泄漏

---

## 7. 高频面试题

### Q1: DDPG 和 DQN 的主要区别？

**答**:
- **动作空间**: DQN 处理离散动作 (通过 argmax)，DDPG 处理连续动作
- **策略**: DQN 隐式策略 (从 Q 导出)，DDPG 显式 Actor 网络
- **探索**: DQN 用 ε-greedy，DDPG 用高斯/OU 噪声
- **梯度**: DDPG 使用确定性策略梯度定理

### Q2: TD3 的三个改进分别解决什么问题？

**答**:
1. **Clipped Double Q**: 解决 Q 值过估计 (取 min)
2. **Delayed Policy Updates**: 解决 Actor-Critic 不稳定 (让 Critic 先稳定)
3. **Target Policy Smoothing**: 解决 Q 函数局部误差被利用 (加噪声平滑)

### Q3: SAC 为什么要最大化熵？

**答**:
1. **自动探索**: 高熵策略天然具有随机性
2. **鲁棒性**: 学习多个近似最优路径，不过拟合
3. **样本效率**: 防止过早收敛到局部最优
4. **理论保证**: 最大熵框架有完整的理论基础

### Q4: 为什么 SAC 比 TD3 更稳定？

**答**:
1. **随机策略**: 天然包含探索，不依赖外部噪声
2. **自动温度**: 自适应调节探索-利用平衡
3. **熵正则化**: 防止策略崩溃到确定性
4. **更平滑的优化目标**: 熵项使损失函数更光滑

### Q5: 解释目标网络的作用

**答**:
- **问题**: 直接用当前网络计算 TD 目标会导致自举不稳定
- **解决**: 使用缓慢更新的目标网络提供稳定目标
- **更新**: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$，$\tau$ 通常 0.005
- **效果**: 减少训练振荡，提高稳定性

---

## 8. 深度思考题

### 思考题 1: 探索与利用的本质

**问题**: 为什么 SAC 的熵最大化比简单的噪声探索更有效？

**分析方向**:
- 噪声探索: 盲目随机，不考虑状态
- 熵探索: 策略层面的随机，有方向性
- SAC 学习"何时"和"如何"探索

### 思考题 2: Off-Policy 的优势

**问题**: 为什么 DDPG/TD3/SAC 都是 off-policy？有什么优势？

**分析方向**:
- 样本复用: 每个转换可用多次
- 行为策略可以不同于目标策略
- 与 on-policy (如 PPO) 的权衡

### 思考题 3: Q 值过估计的根源

**问题**: Q 值过估计的数学根源是什么？除了 TD3 还有其他解决方案吗？

**分析方向**:
- Jensen 不等式 + 噪声
- 替代方案: Averaged DQN, Ensemble Q-learning
- 为什么 SAC 也用 twin Q？

### 思考题 4: 温度参数的语义

**问题**: SAC 中的 $\alpha$ 从物理意义上代表什么？

**分析方向**:
- 信息论视角: 策略与均匀分布的 KL 散度权重
- 贸易: 每 nat 熵值多少回报
- 目标熵 $-\dim(A)$ 的含义

### 思考题 5: 连续动作的挑战

**问题**: 为什么连续动作空间比离散空间难？

**分析方向**:
- 离散: 有限选项，可枚举
- 连续: 无限选项，需要函数逼近动作本身
- Actor-Critic 解耦的必要性

---

## 快速复习卡片

### DDPG 一句话

> Actor 输出确定性动作，Critic 评估 Q 值，用确定性策略梯度更新 Actor。

### TD3 一句话

> DDPG + (twin Q 取 min) + (延迟 Actor 更新) + (目标动作加噪声)。

### SAC 一句话

> 最大化 (回报 + α × 策略熵)，α 自动调节，随机策略天然探索。

---

## 参考资源

### 必读论文
1. [DDPG] Lillicrap et al. "Continuous control with deep reinforcement learning" (2016)
2. [TD3] Fujimoto et al. "Addressing Function Approximation Error in Actor-Critic Methods" (2018)
3. [SAC] Haarnoja et al. "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL" (2018)

### 推荐实现
- OpenAI Spinning Up: https://spinningup.openai.com/
- Stable Baselines3: https://github.com/DLR-RM/stable-baselines3
- CleanRL: https://github.com/vwxyzjn/cleanrl

### 调试技巧
- 先在简单环境 (Pendulum-v1) 验证
- 使用 tensorboard 监控 Q 值、损失、熵
- 检查动作分布是否合理
