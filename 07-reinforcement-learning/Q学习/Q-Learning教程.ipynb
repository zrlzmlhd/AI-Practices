{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Q-Learning 深度教程\n",
    "\n",
    "---\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将：\n",
    "\n",
    "1. **深入理解** 时序差分学习 (TD Learning) 的核心思想\n",
    "2. **掌握** Q-Learning 算法的数学原理与实现细节\n",
    "3. **实现** 从零开始的表格型 Q-Learning\n",
    "4. **理解** 探索与利用的平衡策略\n",
    "5. **对比** Q-Learning 和 SARSA 的行为差异\n",
    "6. **应用** 高级技巧如 Double Q-Learning\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "- 马尔可夫决策过程 (MDP) 基本概念\n",
    "- Python 和 NumPy 基础\n",
    "- 概率论基础\n",
    "\n",
    "## 预计时间\n",
    "\n",
    "90-120 分钟（可分多次完成）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第一部分：理论基础\n",
    "\n",
    "## 1.1 从动态规划到无模型学习\n",
    "\n",
    "### 动态规划的局限性\n",
    "\n",
    "在 MDP 基础模块中，我们学习了动态规划 (DP) 方法求解最优策略。DP 需要：\n",
    "\n",
    "1. **完整的环境模型**：状态转移概率 $P(s'|s,a)$ 和奖励函数 $R(s,a,s')$\n",
    "2. **遍历所有状态和动作**：计算复杂度随状态空间增大而爆炸\n",
    "\n",
    "**现实问题**：\n",
    "- 转移概率通常未知（如何精确计算开车时每个操作的后果？）\n",
    "- 状态空间可能极其庞大（围棋约有 $10^{170}$ 种状态）\n",
    "\n",
    "### 无模型强化学习\n",
    "\n",
    "**核心思想**：通过与环境**交互采样**学习最优策略，无需事先知道环境模型。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    强化学习方法分类                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌─────────────┐           ┌─────────────────────────────┐  │\n",
    "│  │  基于模型   │           │        无模型 (Model-Free)   │  │\n",
    "│  │ (Model-Based)│          ├──────────────┬──────────────┤  │\n",
    "│  │             │           │  基于价值    │  基于策略     │  │\n",
    "│  │ • 动态规划   │           │ (Value-Based)│(Policy-Based)│  │\n",
    "│  │ • 模型预测控制│          │              │              │  │\n",
    "│  │ • Dyna-Q    │           │ • Q-Learning │ • REINFORCE  │  │\n",
    "│  └─────────────┘           │ • SARSA      │ • Actor-Critic│  │\n",
    "│                            │ • DQN        │ • PPO        │  │\n",
    "│                            └──────────────┴──────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1.2 时序差分学习 (TD Learning)\n",
    "\n",
    "### 蒙特卡洛 vs 时序差分\n",
    "\n",
    "**蒙特卡洛方法** (Monte Carlo, MC)：\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]$$\n",
    "\n",
    "- 需要等待**回合结束**才能更新\n",
    "- $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$ 是完整回合的实际累积回报\n",
    "- 无偏估计，但方差较大\n",
    "\n",
    "**时序差分方法** (Temporal Difference, TD)：\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]$$\n",
    "\n",
    "- **每一步**都可以更新\n",
    "- 使用**自举** (Bootstrapping)：用估计值更新估计值\n",
    "- 有偏估计，但方差较小\n",
    "\n",
    "### TD 误差\n",
    "\n",
    "**TD 目标** (TD Target):\n",
    "\n",
    "$$\\text{TD Target} = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "\n",
    "**TD 误差** (TD Error):\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "\n",
    "**直觉理解**：\n",
    "- $\\delta_t > 0$：实际比预期好，应增大 $V(S_t)$\n",
    "- $\\delta_t < 0$：实际比预期差，应减小 $V(S_t)$\n",
    "- $\\delta_t = 0$：预测准确，价值已收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1.3 Q-Learning 算法\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "Q-Learning 是一种**离策略** (Off-Policy) TD 控制算法，直接学习最优动作价值函数 $Q^*$：\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "**分解来看**：\n",
    "\n",
    "```\n",
    "Q(S_t, A_t) ← Q(S_t, A_t) + α × [R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "              \\_________/       \\___________________________________/   \\________/\n",
    "               旧估计                      TD 目标                       旧估计\n",
    "                                 \\________________________________________________/\n",
    "                                                    TD 误差 δ_t\n",
    "```\n",
    "\n",
    "### 关键特性\n",
    "\n",
    "1. **离策略 (Off-Policy)**：学习最优策略，不受探索策略影响\n",
    "2. **使用 max 操作**：假设未来总是采取最优动作\n",
    "3. **收敛性保证**：在一定条件下收敛到 $Q^*$\n",
    "\n",
    "### 算法伪代码\n",
    "\n",
    "```\n",
    "算法: Q-Learning\n",
    "\n",
    "输入: 状态空间 S, 动作空间 A, 学习率 α, 折扣因子 γ, 探索率 ε\n",
    "输出: 最优 Q 函数\n",
    "\n",
    "1. 初始化 Q(s, a) = 0，对于所有 s ∈ S, a ∈ A\n",
    "2. 对于每个回合:\n",
    "   a. 初始化状态 S\n",
    "   b. 重复 (对于回合中的每一步):\n",
    "      i.   使用 ε-greedy 从 Q 选择动作 A\n",
    "      ii.  执行动作 A，观察奖励 R 和下一状态 S'\n",
    "      iii. Q(S, A) ← Q(S, A) + α[R + γ max_a Q(S', a) - Q(S, A)]\n",
    "      iv.  S ← S'\n",
    "   c. 直到 S 是终止状态\n",
    "3. 返回 Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第二部分：代码实现\n",
    "\n",
    "## 2.1 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 导入必要的库\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# 配置参数\n",
    "# ============================================================\n",
    "\n",
    "# 设置随机种子，确保结果可重复\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 可视化配置\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"环境配置完成！\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2.2 实现悬崖行走环境\n",
    "\n",
    "悬崖行走 (Cliff Walking) 是经典的强化学习测试环境，用于对比不同算法的行为特性：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 0\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 1\n",
    "│ .  .  .  .  .  .  .  .  .  .  .  .  │  row 2\n",
    "│ S  C  C  C  C  C  C  C  C  C  C  G  │  row 3\n",
    "└─────────────────────────────────────────────┘\n",
    "  0  1  2  3  4  5  6  7  8  9 10 11\n",
    "\n",
    "S: 起点    G: 目标    C: 悬崖\n",
    "```\n",
    "\n",
    "**规则**：\n",
    "- 动作空间：上(0)、右(1)、下(2)、左(3)\n",
    "- 每步奖励：-1（鼓励快速到达）\n",
    "- 掉入悬崖：-100 奖励，重置到起点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"\n",
    "    悬崖行走环境\n",
    "    \n",
    "    核心思想：\n",
    "        经典的强化学习测试环境，用于演示 Q-Learning 和 SARSA 的行为差异。\n",
    "        智能体需要从起点 S 到达目标 G，同时避免掉入悬崖 C。\n",
    "    \n",
    "    数学原理：\n",
    "        这是一个确定性 MDP，状态转移概率 P(s'|s,a) ∈ {0, 1}。\n",
    "        奖励函数设计鼓励快速到达目标并惩罚危险行为。\n",
    "    \n",
    "    问题背景：\n",
    "        该环境用于展示离策略 (Q-Learning) 和在策略 (SARSA) 算法的关键差异：\n",
    "        - Q-Learning 学习最优但风险高的路径（沿悬崖边）\n",
    "        - SARSA 学习安全但较长的路径（远离悬崖）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 动作定义：(行偏移, 列偏移)\n",
    "    ACTIONS = {\n",
    "        0: (-1, 0),   # 上\n",
    "        1: (0, 1),    # 右\n",
    "        2: (1, 0),    # 下\n",
    "        3: (0, -1)    # 左\n",
    "    }\n",
    "    ACTION_NAMES = ['上', '右', '下', '左']\n",
    "    \n",
    "    def __init__(self, height: int = 4, width: int = 12):\n",
    "        \"\"\"\n",
    "        初始化环境\n",
    "        \n",
    "        Args:\n",
    "            height: 网格高度，默认 4\n",
    "            width: 网格宽度，默认 12\n",
    "        \"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # 特殊位置定义\n",
    "        self.start = (height - 1, 0)           # 起点：左下角\n",
    "        self.goal = (height - 1, width - 1)    # 终点：右下角\n",
    "        self.cliff = [(height - 1, j) for j in range(1, width - 1)]  # 悬崖：底部中间\n",
    "        \n",
    "        # 当前状态\n",
    "        self.state = self.start\n",
    "        self.n_actions = 4\n",
    "        \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"重置环境到初始状态\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n",
    "        \"\"\"\n",
    "        执行动作\n",
    "        \n",
    "        Args:\n",
    "            action: 动作索引 (0-3)\n",
    "            \n",
    "        Returns:\n",
    "            (next_state, reward, done) 三元组\n",
    "        \"\"\"\n",
    "        # 计算下一位置（边界裁剪，撞墙则留在原地）\n",
    "        di, dj = self.ACTIONS[action]\n",
    "        new_i = np.clip(self.state[0] + di, 0, self.height - 1)\n",
    "        new_j = np.clip(self.state[1] + dj, 0, self.width - 1)\n",
    "        next_state = (int(new_i), int(new_j))\n",
    "        \n",
    "        # 检查是否掉入悬崖\n",
    "        if next_state in self.cliff:\n",
    "            self.state = self.start  # 重置到起点\n",
    "            return self.state, -100.0, False\n",
    "        \n",
    "        self.state = next_state\n",
    "        \n",
    "        # 检查是否到达目标\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 0.0, True\n",
    "        \n",
    "        return self.state, -1.0, False\n",
    "    \n",
    "    def render(self, path: Optional[List[Tuple[int, int]]] = None) -> None:\n",
    "        \"\"\"可视化环境\"\"\"\n",
    "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # 标记悬崖\n",
    "        for pos in self.cliff:\n",
    "            grid[pos[0]][pos[1]] = 'C'\n",
    "        \n",
    "        # 标记起点和终点\n",
    "        grid[self.start[0]][self.start[1]] = 'S'\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'\n",
    "        \n",
    "        # 标记路径\n",
    "        if path:\n",
    "            for pos in path[1:-1]:\n",
    "                if pos not in self.cliff and pos != self.start and pos != self.goal:\n",
    "                    grid[pos[0]][pos[1]] = '*'\n",
    "        \n",
    "        # 打印网格\n",
    "        print(\"┌\" + \"─\" * (self.width * 2 + 1) + \"┐\")\n",
    "        for row in grid:\n",
    "            print(\"│ \" + \" \".join(row) + \" │\")\n",
    "        print(\"└\" + \"─\" * (self.width * 2 + 1) + \"┘\")\n",
    "\n",
    "\n",
    "# 测试环境\n",
    "env = CliffWalkingEnv()\n",
    "print(\"悬崖行走环境:\")\n",
    "env.render()\n",
    "print(f\"\\n起点: {env.start}\")\n",
    "print(f\"终点: {env.goal}\")\n",
    "print(f\"悬崖位置: {env.cliff[:3]}...{env.cliff[-1]}\")\n",
    "print(f\"状态空间: {env.height * env.width} 个状态\")\n",
    "print(f\"动作空间: {env.n_actions} 个动作\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### 交互式探索：手动控制智能体\n",
    "\n",
    "让我们手动执行几个动作，感受环境的奖励设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动探索环境\n",
    "env = CliffWalkingEnv()\n",
    "state = env.reset()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"手动探索环境\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 执行一系列动作\n",
    "actions = [0, 0, 1, 1, 1]  # 上、上、右、右、右\n",
    "action_names = env.ACTION_NAMES\n",
    "\n",
    "print(f\"\\n初始状态: {state}\")\n",
    "total_reward = 0\n",
    "\n",
    "for action in actions:\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"动作: {action_names[action]:2s} | 状态: {state} -> {next_state} | 奖励: {reward:+.0f} | 累计: {total_reward:+.0f}\")\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(\"到达目标！\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n最终累计奖励: {total_reward}\")\n",
    "\n",
    "# 演示掉入悬崖\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"演示掉入悬崖\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done = env.step(1)  # 从起点向右 -> 掉入悬崖\n",
    "print(f\"从起点向右: 奖励 = {reward}, 状态重置到 {next_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 2.3 实现 Q-Learning 智能体\n",
    "\n",
    "现在让我们从零实现一个 Q-Learning 智能体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"\n",
    "    训练指标记录\n",
    "    \n",
    "    用于监控学习进度和分析算法性能。\n",
    "    \"\"\"\n",
    "    episode_rewards: List[float] = field(default_factory=list)\n",
    "    episode_lengths: List[int] = field(default_factory=list)\n",
    "    epsilon_history: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def get_moving_average(self, window: int = 10) -> np.ndarray:\n",
    "        \"\"\"计算移动平均\"\"\"\n",
    "        if len(self.episode_rewards) < window:\n",
    "            return np.array(self.episode_rewards)\n",
    "        return np.convolve(\n",
    "            self.episode_rewards,\n",
    "            np.ones(window) / window,\n",
    "            mode='valid'\n",
    "        )\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    表格型 Q-Learning 智能体\n",
    "    \n",
    "    核心思想：\n",
    "        Q-Learning 是一种离策略 (off-policy) TD 控制算法，\n",
    "        通过与环境交互直接学习最优动作价值函数 Q*。\n",
    "    \n",
    "    数学原理：\n",
    "        更新公式：\n",
    "        Q(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "        \n",
    "        其中：\n",
    "        - α: 学习率，控制更新步长\n",
    "        - γ: 折扣因子，权衡即时与未来奖励\n",
    "        - max_a Q(S_{t+1}, a): 下一状态的最优动作价值（离策略的关键）\n",
    "    \n",
    "    算法对比：\n",
    "        vs SARSA：Q-Learning 使用 max（离策略），SARSA 使用实际动作（在策略）\n",
    "        vs MC：Q-Learning 每步更新（TD），MC 需要完整回合\n",
    "    \n",
    "    复杂度：\n",
    "        - 空间：O(|S| × |A|)，存储 Q 表\n",
    "        - 时间：每步 O(|A|)，计算 max\n",
    "    \n",
    "    Attributes:\n",
    "        q_table: Q 值表，字典形式 {state: [Q(s,a0), Q(s,a1), ...]}\n",
    "        lr: 学习率 α\n",
    "        gamma: 折扣因子 γ\n",
    "        epsilon: 探索率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化 Q-Learning 智能体\n",
    "        \n",
    "        Args:\n",
    "            n_actions: 动作空间大小\n",
    "            learning_rate: 学习率，控制 Q 值更新步长\n",
    "            discount_factor: 折扣因子，权衡即时与未来奖励\n",
    "            epsilon: 初始探索率\n",
    "            epsilon_decay: 探索率衰减系数\n",
    "            epsilon_min: 最小探索率\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q 表：使用 defaultdict 自动初始化未访问状态\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "        # 训练指标\n",
    "        self.metrics = TrainingMetrics()\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        使用 ε-greedy 策略选择动作\n",
    "        \n",
    "        ε-greedy 策略：\n",
    "            - 以概率 ε 随机选择（探索）\n",
    "            - 以概率 1-ε 选择 Q 值最大的动作（利用）\n",
    "        \n",
    "        Args:\n",
    "            state: 当前状态\n",
    "            training: 是否处于训练模式\n",
    "            \n",
    "        Returns:\n",
    "            选择的动作索引\n",
    "        \"\"\"\n",
    "        # 训练时以 ε 概率随机探索\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        # 利用：选择 Q 值最大的动作\n",
    "        # 当存在多个最大值时，随机选择（打破平局）\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Q-Learning 更新规则\n",
    "        \n",
    "        更新公式：\n",
    "            Q(S,A) ← Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n",
    "        \n",
    "        Args:\n",
    "            state: 当前状态\n",
    "            action: 执行的动作\n",
    "            reward: 获得的奖励\n",
    "            next_state: 下一状态\n",
    "            done: 是否终止\n",
    "            \n",
    "        Returns:\n",
    "            TD 误差\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        # 计算 TD 目标\n",
    "        if done:\n",
    "            target = reward  # 终止状态没有后续奖励\n",
    "        else:\n",
    "            # Q-Learning 核心：使用 max 选择下一状态的最优动作\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # TD 误差：反映预测与实际的差距\n",
    "        td_error = target - current_q\n",
    "        \n",
    "        # 更新 Q 值：沿着减小 TD 误差的方向调整\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        \"\"\"衰减探索率\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# 创建智能体实例\n",
    "agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=0.5,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=1.0,  # 保持固定探索率\n",
    "    epsilon_min=0.1\n",
    ")\n",
    "\n",
    "print(\"Q-Learning 智能体配置:\")\n",
    "print(f\"  动作数量: {agent.n_actions}\")\n",
    "print(f\"  学习率 α: {agent.lr}\")\n",
    "print(f\"  折扣因子 γ: {agent.gamma}\")\n",
    "print(f\"  探索率 ε: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 交互式探索：观察 Q 值更新\n",
    "\n",
    "让我们手动执行几步，观察 Q 值如何更新："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察 Q 值更新过程\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"观察 Q 值更新过程\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "# 执行几步并观察 Q 值变化\n",
    "actions_to_take = [0, 0, 1, 1, 2]  # 上、上、右、右、下\n",
    "\n",
    "for i, action in enumerate(actions_to_take):\n",
    "    print(f\"\\n步骤 {i+1}:\")\n",
    "    print(f\"  当前状态: {state}\")\n",
    "    print(f\"  更新前 Q 值: {agent.q_table[state]}\")\n",
    "    \n",
    "    next_state, reward, done = env.step(action)\n",
    "    td_error = agent.update(state, action, reward, next_state, done)\n",
    "    \n",
    "    print(f\"  执行动作: {env.ACTION_NAMES[action]}\")\n",
    "    print(f\"  获得奖励: {reward}\")\n",
    "    print(f\"  TD 误差: {td_error:.4f}\")\n",
    "    print(f\"  更新后 Q 值: {agent.q_table[state]}\")\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(\"  到达目标！\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 2.4 训练循环实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(\n",
    "    env: CliffWalkingEnv,\n",
    "    agent: QLearningAgent,\n",
    "    episodes: int = 500,\n",
    "    max_steps: int = 200,\n",
    "    verbose: bool = True,\n",
    "    log_interval: int = 100\n",
    ") -> TrainingMetrics:\n",
    "    \"\"\"\n",
    "    训练 Q-Learning 智能体\n",
    "    \n",
    "    Args:\n",
    "        env: 环境实例\n",
    "        agent: Q-Learning 智能体\n",
    "        episodes: 训练回合数\n",
    "        max_steps: 每回合最大步数\n",
    "        verbose: 是否打印训练进度\n",
    "        log_interval: 日志打印间隔\n",
    "        \n",
    "    Returns:\n",
    "        训练历史记录\n",
    "    \"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 选择动作\n",
    "            action = agent.get_action(state, training=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 更新 Q 值\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 衰减探索率\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # 记录历史\n",
    "        metrics.episode_rewards.append(total_reward)\n",
    "        metrics.episode_lengths.append(steps)\n",
    "        metrics.epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # 打印进度\n",
    "        if verbose and (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-log_interval:])\n",
    "            avg_steps = np.mean(metrics.episode_lengths[-log_interval:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:6.1f} | \"\n",
    "                  f\"ε: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    agent.metrics = metrics\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# 训练智能体\n",
    "print(\"开始训练 Q-Learning...\\n\")\n",
    "env = CliffWalkingEnv()\n",
    "agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=0.5,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=1.0,  # 不衰减，便于观察行为\n",
    "    epsilon_min=0.1\n",
    ")\n",
    "\n",
    "metrics = train_q_learning(env, agent, episodes=500)\n",
    "\n",
    "print(f\"\\n训练完成！\")\n",
    "print(f\"最后 100 回合平均奖励: {np.mean(metrics.episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Q 表大小: {len(agent.q_table)} 个状态\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 2.5 可视化学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(metrics: TrainingMetrics, window: int = 10, title: str = \"Q-Learning\"):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    rewards = metrics.episode_rewards\n",
    "    smoothed_rewards = np.convolve(\n",
    "        rewards, np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    \n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue', label='原始奖励')\n",
    "    axes[0].plot(range(window-1, len(rewards)), smoothed_rewards, \n",
    "                 color='blue', linewidth=2, label=f'{window} 回合移动平均')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title(f'{title} 学习曲线')\n",
    "    axes[0].legend()\n",
    "    axes[0].axhline(y=-13, color='green', linestyle='--', alpha=0.7, label='最优 (理论)')\n",
    "    \n",
    "    # 步数曲线\n",
    "    steps = metrics.episode_lengths\n",
    "    smoothed_steps = np.convolve(\n",
    "        steps, np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    \n",
    "    axes[1].plot(steps, alpha=0.3, color='green', label='原始步数')\n",
    "    axes[1].plot(range(window-1, len(steps)), smoothed_steps,\n",
    "                 color='green', linewidth=2, label=f'{window} 回合移动平均')\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('每回合步数')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_curves(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 2.6 提取并可视化学到的策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(agent: QLearningAgent, env: CliffWalkingEnv, max_steps: int = 50) -> List[Tuple[int, int]]:\n",
    "    \"\"\"从训练好的智能体提取贪心策略路径\"\"\"\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)  # 不探索\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "def visualize_policy(agent: QLearningAgent, env: CliffWalkingEnv):\n",
    "    \"\"\"可视化学到的策略\"\"\"\n",
    "    arrow_map = {0: '↑', 1: '→', 2: '↓', 3: '←'}\n",
    "    \n",
    "    print(\"学到的策略 (贪心):\")\n",
    "    print(\"┌\" + \"───\" * env.width + \"┐\")\n",
    "    \n",
    "    for i in range(env.height):\n",
    "        row = \"│\"\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state == env.start:\n",
    "                row += \" S \"\n",
    "            elif state == env.goal:\n",
    "                row += \" G \"\n",
    "            elif state in env.cliff:\n",
    "                row += \" C \"\n",
    "            elif state in agent.q_table:\n",
    "                best_action = np.argmax(agent.q_table[state])\n",
    "                row += f\" {arrow_map[best_action]} \"\n",
    "            else:\n",
    "                row += \" . \"\n",
    "        print(row + \"│\")\n",
    "    \n",
    "    print(\"└\" + \"───\" * env.width + \"┘\")\n",
    "\n",
    "\n",
    "# 可视化策略\n",
    "print(\"=\" * 60)\n",
    "print(\"Q-Learning 学到的策略\")\n",
    "print(\"=\" * 60)\n",
    "visualize_policy(agent, env)\n",
    "\n",
    "# 提取并显示路径\n",
    "print(\"\\n学到的路径:\")\n",
    "path = extract_path(agent, env)\n",
    "env.reset()\n",
    "env.render(path)\n",
    "print(f\"路径长度: {len(path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 2.7 Q 值表可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(agent: QLearningAgent, env: CliffWalkingEnv):\n",
    "    \"\"\"可视化 Q 表和价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 准备价值函数数据\n",
    "    v_table = np.zeros((env.height, env.width))\n",
    "    for i in range(env.height):\n",
    "        for j in range(env.width):\n",
    "            state = (i, j)\n",
    "            if state in agent.q_table:\n",
    "                v_table[i, j] = np.max(agent.q_table[state])\n",
    "            else:\n",
    "                v_table[i, j] = 0\n",
    "    \n",
    "    # 价值函数热力图\n",
    "    im = axes[0].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "    axes[0].set_title('状态价值函数 V(s) = max_a Q(s,a)')\n",
    "    axes[0].set_xlabel('列')\n",
    "    axes[0].set_ylabel('行')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # 标记悬崖位置\n",
    "    for pos in env.cliff:\n",
    "        axes[0].add_patch(plt.Rectangle(\n",
    "            (pos[1]-0.5, pos[0]-0.5), 1, 1,\n",
    "            fill=True, color='black', alpha=0.5\n",
    "        ))\n",
    "    \n",
    "    # Q 值分布直方图\n",
    "    q_values = []\n",
    "    for state, q_array in agent.q_table.items():\n",
    "        if state not in env.cliff:\n",
    "            q_values.extend(q_array.tolist())\n",
    "    \n",
    "    axes[1].hist(q_values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[1].set_xlabel('Q Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Q 值分布')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_q_table(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第三部分：Q-Learning vs SARSA 对比\n",
    "\n",
    "## 3.1 SARSA 智能体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA 智能体 (On-Policy TD Control)\n",
    "    \n",
    "    核心思想：\n",
    "        SARSA (State-Action-Reward-State-Action) 是一种在策略 (on-policy) \n",
    "        TD 控制算法，学习当前行为策略的价值函数 Q^π。\n",
    "    \n",
    "    数学原理：\n",
    "        更新公式：\n",
    "        Q(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n",
    "        \n",
    "        与 Q-Learning 的关键区别：\n",
    "        - 使用实际采取的下一动作 A_{t+1}，而非 max\n",
    "        - 学习的是当前 ε-greedy 策略的价值，而非最优策略\n",
    "    \n",
    "    算法对比：\n",
    "        在悬崖行走环境中：\n",
    "        - Q-Learning：学习沿悬崖边的最短路径（因为更新不考虑探索）\n",
    "        - SARSA：学习远离悬崖的安全路径（考虑探索时掉下悬崖的可能）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.q_table: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        self.metrics = TrainingMetrics()\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"ε-greedy 策略选择动作\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        q_values = self.q_table[state]\n",
    "        max_q = np.max(q_values)\n",
    "        max_actions = np.where(np.isclose(q_values, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        next_action: int,  # SARSA 需要下一个动作！\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        SARSA 更新规则\n",
    "        \n",
    "        与 Q-Learning 的关键区别：使用实际的 next_action，而非 max\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # SARSA 核心：使用实际的 next_action\n",
    "            target = reward + self.gamma * self.q_table[next_state][next_action]\n",
    "        \n",
    "        td_error = target - current_q\n",
    "        self.q_table[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "def train_sarsa(\n",
    "    env: CliffWalkingEnv,\n",
    "    agent: SARSAAgent,\n",
    "    episodes: int = 500,\n",
    "    max_steps: int = 200,\n",
    "    verbose: bool = True,\n",
    "    log_interval: int = 100\n",
    ") -> TrainingMetrics:\n",
    "    \"\"\"训练 SARSA 智能体\"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        # SARSA: 先选择初始动作\n",
    "        action = agent.get_action(state, training=True)\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # SARSA: 在更新前选择下一个动作\n",
    "            next_action = agent.get_action(next_state, training=True)\n",
    "            \n",
    "            # SARSA 更新需要 next_action\n",
    "            agent.update(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            # 状态和动作传递\n",
    "            state = next_state\n",
    "            action = next_action  # 关键！\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        metrics.episode_rewards.append(total_reward)\n",
    "        metrics.episode_lengths.append(steps)\n",
    "        metrics.epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        if verbose and (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(metrics.episode_rewards[-log_interval:])\n",
    "            avg_steps = np.mean(metrics.episode_lengths[-log_interval:])\n",
    "            print(f\"Episode {episode + 1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg Steps: {avg_steps:6.1f} | \"\n",
    "                  f\"ε: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    agent.metrics = metrics\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"SARSA 智能体和训练函数定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 3.2 对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 对比实验：Q-Learning vs SARSA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"悬崖行走环境: Q-Learning vs SARSA 对比实验\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 实验参数\n",
    "EPISODES = 500\n",
    "LEARNING_RATE = 0.5\n",
    "EPSILON = 0.1  # 固定探索率，便于观察行为差异\n",
    "\n",
    "# 创建环境\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "# Q-Learning 智能体\n",
    "q_agent = QLearningAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epsilon=EPSILON,\n",
    "    epsilon_decay=1.0,  # 不衰减\n",
    "    epsilon_min=EPSILON\n",
    ")\n",
    "\n",
    "# SARSA 智能体\n",
    "sarsa_agent = SARSAAgent(\n",
    "    n_actions=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epsilon=EPSILON,\n",
    "    epsilon_decay=1.0,\n",
    "    epsilon_min=EPSILON\n",
    ")\n",
    "\n",
    "# 训练\n",
    "print(\"\\n训练 Q-Learning...\")\n",
    "q_metrics = train_q_learning(env, q_agent, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\n训练 SARSA...\")\n",
    "sarsa_metrics = train_sarsa(env, sarsa_agent, episodes=EPISODES, verbose=True)\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 3.3 可视化对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(q_metrics: TrainingMetrics, sarsa_metrics: TrainingMetrics, window: int = 10):\n",
    "    \"\"\"绘制 Q-Learning 和 SARSA 的学习曲线对比\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    q_smooth = np.convolve(q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    sarsa_smooth = np.convolve(sarsa_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[0].plot(q_smooth, label='Q-Learning', color='blue', alpha=0.8, linewidth=2)\n",
    "    axes[0].plot(sarsa_smooth, label='SARSA', color='red', alpha=0.8, linewidth=2)\n",
    "    axes[0].axhline(y=-13, color='green', linestyle='--', alpha=0.7, label='最优路径 (-13)')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Total Reward')\n",
    "    axes[0].set_title('学习曲线: 回合奖励')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 步数曲线\n",
    "    q_steps_smooth = np.convolve(q_metrics.episode_lengths, np.ones(window)/window, mode='valid')\n",
    "    sarsa_steps_smooth = np.convolve(sarsa_metrics.episode_lengths, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1].plot(q_steps_smooth, label='Q-Learning', color='blue', alpha=0.8, linewidth=2)\n",
    "    axes[1].plot(sarsa_steps_smooth, label='SARSA', color='red', alpha=0.8, linewidth=2)\n",
    "    axes[1].set_xlabel('Episode')\n",
    "    axes[1].set_ylabel('Steps')\n",
    "    axes[1].set_title('学习曲线: 回合步数')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 统计\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"最后 100 回合统计\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Q-Learning: 平均奖励 = {np.mean(q_metrics.episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"SARSA:      平均奖励 = {np.mean(sarsa_metrics.episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "\n",
    "plot_comparison(q_metrics, sarsa_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 3.4 提取并对比学到的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取路径\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"学到的策略路径对比\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nQ-Learning 学到的路径 (倾向最短路径，沿悬崖边):\")\n",
    "q_path = extract_path(q_agent, env)\n",
    "env.reset()\n",
    "env.render(q_path)\n",
    "print(f\"路径长度: {len(q_path) - 1} 步\")\n",
    "\n",
    "print(\"\\nSARSA 学到的路径 (倾向安全路径，远离悬崖):\")\n",
    "\n",
    "# SARSA 需要稍微修改 extract_path\n",
    "def extract_path_sarsa(agent: SARSAAgent, env: CliffWalkingEnv, max_steps: int = 50):\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_action(state, training=False)\n",
    "        next_state, _, done = env.step(action)\n",
    "        path.append(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return path\n",
    "\n",
    "sarsa_path = extract_path_sarsa(sarsa_agent, env)\n",
    "env.reset()\n",
    "env.render(sarsa_path)\n",
    "print(f\"路径长度: {len(sarsa_path) - 1} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 3.5 行为差异分析\n",
    "\n",
    "### 为什么 Q-Learning 选择悬崖边路径？\n",
    "\n",
    "Q-Learning 更新使用 `max`，学习的是**最优策略的价值**：\n",
    "- 假设执行最优策略，不会掉入悬崖\n",
    "- 沿悬崖边的路径最短，奖励最高\n",
    "- 但训练时的 ε-greedy 探索会导致实际掉入悬崖\n",
    "\n",
    "**结果**：学到的策略是最优的，但训练过程中经常失败\n",
    "\n",
    "### 为什么 SARSA 选择安全路径？\n",
    "\n",
    "SARSA 使用实际采取的动作，学习的是**当前 ε-greedy 策略的价值**：\n",
    "- 考虑到探索时可能随机选择动作\n",
    "- 靠近悬崖时，探索可能导致掉落\n",
    "- 因此远离悬崖的路径价值更高\n",
    "\n",
    "**结果**：学到的策略更保守，但训练过程更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化价值函数对比\n",
    "def visualize_value_comparison(q_agent, sarsa_agent, env):\n",
    "    \"\"\"对比两种算法学到的价值函数\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx, (agent, name) in enumerate([(q_agent, 'Q-Learning'), (sarsa_agent, 'SARSA')]):\n",
    "        v_table = np.zeros((env.height, env.width))\n",
    "        for i in range(env.height):\n",
    "            for j in range(env.width):\n",
    "                state = (i, j)\n",
    "                if state in agent.q_table:\n",
    "                    v_table[i, j] = np.max(agent.q_table[state])\n",
    "        \n",
    "        im = axes[idx].imshow(v_table, cmap='RdYlGn', aspect='auto')\n",
    "        axes[idx].set_title(f'{name} 价值函数 V(s)')\n",
    "        axes[idx].set_xlabel('列')\n",
    "        axes[idx].set_ylabel('行')\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "        \n",
    "        # 标记悬崖\n",
    "        for pos in env.cliff:\n",
    "            axes[idx].add_patch(plt.Rectangle(\n",
    "                (pos[1]-0.5, pos[0]-0.5), 1, 1,\n",
    "                fill=True, color='black', alpha=0.5\n",
    "            ))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_value_comparison(q_agent, sarsa_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第四部分：高级技巧\n",
    "\n",
    "## 4.1 Double Q-Learning\n",
    "\n",
    "### 过估计问题\n",
    "\n",
    "Q-Learning 的 `max` 操作导致系统性过估计：\n",
    "\n",
    "- 假设 Q 值估计有噪声：$\\hat{Q}(s,a) = Q^*(s,a) + \\epsilon_a$\n",
    "- $\\mathbb{E}[\\max_a \\hat{Q}(s,a)] \\geq \\max_a \\mathbb{E}[\\hat{Q}(s,a)]$\n",
    "- 这种正偏差会通过 bootstrapping 累积\n",
    "\n",
    "### 解决方案：Double Q-Learning\n",
    "\n",
    "解耦动作选择和价值评估：\n",
    "\n",
    "$$Q_1(S,A) \\leftarrow Q_1(S,A) + \\alpha[R + \\gamma Q_2(S', \\arg\\max_a Q_1(S',a)) - Q_1(S,A)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    \"\"\"\n",
    "    Double Q-Learning 智能体\n",
    "    \n",
    "    核心思想：\n",
    "        通过维护两个 Q 表，解耦动作选择和价值评估，减少过估计偏差。\n",
    "    \n",
    "    数学原理：\n",
    "        以 0.5 概率选择更新 Q1 或 Q2：\n",
    "        - 更新 Q1：用 Q1 选择动作，Q2 评估价值\n",
    "        - 更新 Q2：用 Q2 选择动作，Q1 评估价值\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        epsilon_min: float = 0.01\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # 两个独立的 Q 表\n",
    "        self.q_table1: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        self.q_table2: Dict[Any, np.ndarray] = defaultdict(\n",
    "            lambda: np.zeros(n_actions)\n",
    "        )\n",
    "        \n",
    "        self.metrics = TrainingMetrics()\n",
    "        \n",
    "    def get_action(self, state: Any, training: bool = True) -> int:\n",
    "        \"\"\"使用两个 Q 表的和选择动作\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        combined_q = self.q_table1[state] + self.q_table2[state]\n",
    "        max_q = np.max(combined_q)\n",
    "        max_actions = np.where(np.isclose(combined_q, max_q))[0]\n",
    "        return np.random.choice(max_actions)\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: Any,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: Any,\n",
    "        done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"Double Q-Learning 更新\"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # 更新 Q1：用 Q1 选择动作，Q2 评估\n",
    "            current_q = self.q_table1[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table1[next_state])\n",
    "                target = reward + self.gamma * self.q_table2[next_state][best_action]\n",
    "            td_error = target - current_q\n",
    "            self.q_table1[state][action] += self.lr * td_error\n",
    "        else:\n",
    "            # 更新 Q2：用 Q2 选择动作，Q1 评估\n",
    "            current_q = self.q_table2[state][action]\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                best_action = np.argmax(self.q_table2[next_state])\n",
    "                target = reward + self.gamma * self.q_table1[next_state][best_action]\n",
    "            td_error = target - current_q\n",
    "            self.q_table2[state][action] += self.lr * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    # 兼容属性\n",
    "    @property\n",
    "    def q_table(self):\n",
    "        \"\"\"返回合并的 Q 表\"\"\"\n",
    "        combined = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        for state in set(self.q_table1.keys()) | set(self.q_table2.keys()):\n",
    "            combined[state] = (self.q_table1[state] + self.q_table2[state]) / 2\n",
    "        return combined\n",
    "\n",
    "\n",
    "print(\"Double Q-Learning 智能体定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 4.2 三种算法对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Q-Learning vs Double Q-Learning vs SARSA 对比\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = CliffWalkingEnv()\n",
    "\n",
    "# 创建三种智能体\n",
    "q_agent = QLearningAgent(\n",
    "    n_actions=4, learning_rate=0.5, epsilon=0.1,\n",
    "    epsilon_decay=1.0, epsilon_min=0.1\n",
    ")\n",
    "\n",
    "double_q_agent = DoubleQLearningAgent(\n",
    "    n_actions=4, learning_rate=0.5, epsilon=0.1,\n",
    "    epsilon_decay=1.0, epsilon_min=0.1\n",
    ")\n",
    "\n",
    "sarsa_agent = SARSAAgent(\n",
    "    n_actions=4, learning_rate=0.5, epsilon=0.1,\n",
    "    epsilon_decay=1.0, epsilon_min=0.1\n",
    ")\n",
    "\n",
    "# 训练\n",
    "print(\"\\n训练 Q-Learning...\")\n",
    "q_metrics = train_q_learning(env, q_agent, episodes=500, verbose=False)\n",
    "print(f\"完成！最后100回合平均奖励: {np.mean(q_metrics.episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "print(\"\\n训练 Double Q-Learning...\")\n",
    "double_q_metrics = train_q_learning(env, double_q_agent, episodes=500, verbose=False)\n",
    "print(f\"完成！最后100回合平均奖励: {np.mean(double_q_metrics.episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "print(\"\\n训练 SARSA...\")\n",
    "sarsa_metrics = train_sarsa(env, sarsa_agent, episodes=500, verbose=False)\n",
    "print(f\"完成！最后100回合平均奖励: {np.mean(sarsa_metrics.episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制三种算法对比\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "window = 10\n",
    "q_smooth = np.convolve(q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "double_smooth = np.convolve(double_q_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "sarsa_smooth = np.convolve(sarsa_metrics.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax.plot(q_smooth, label='Q-Learning', alpha=0.8, linewidth=2)\n",
    "ax.plot(double_smooth, label='Double Q-Learning', alpha=0.8, linewidth=2)\n",
    "ax.plot(sarsa_smooth, label='SARSA', alpha=0.8, linewidth=2)\n",
    "ax.axhline(y=-13, color='green', linestyle='--', alpha=0.7, label='最优 (-13)')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('三种 TD 控制算法对比 (悬崖行走)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第五部分：总结与练习\n",
    "\n",
    "## 5.1 核心要点回顾\n",
    "\n",
    "### Q-Learning 算法\n",
    "\n",
    "| 特性 | 描述 |\n",
    "|------|------|\n",
    "| 类型 | 离策略 (Off-Policy) TD 控制 |\n",
    "| 更新公式 | $Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\max_a Q(S',a) - Q(S,A)]$ |\n",
    "| 学习目标 | 最优动作价值函数 $Q^*$ |\n",
    "| 优点 | 直接学习最优策略，样本可重用 |\n",
    "| 缺点 | 可能过估计，探索时不安全 |\n",
    "\n",
    "### 算法对比\n",
    "\n",
    "| 算法 | 策略类型 | TD 目标 | 适用场景 |\n",
    "|------|----------|---------|----------|\n",
    "| Q-Learning | Off-Policy | $\\max_a Q(S',a)$ | 追求最优性能 |\n",
    "| SARSA | On-Policy | $Q(S',A')$ | 需要安全探索 |\n",
    "| Double Q-Learning | Off-Policy | $Q_2(S', \\arg\\max_a Q_1(S',a))$ | 减少过估计 |\n",
    "\n",
    "### 推荐超参数\n",
    "\n",
    "| 参数 | 典型值 | 说明 |\n",
    "|------|--------|------|\n",
    "| $\\alpha$ (学习率) | 0.1 ~ 0.5 | 表格型可用较大值 |\n",
    "| $\\gamma$ (折扣因子) | 0.99 | 接近 1 重视长期 |\n",
    "| $\\epsilon$ (初始探索率) | 1.0 | 从完全探索开始 |\n",
    "| $\\epsilon_{min}$ | 0.01 ~ 0.1 | 保持少量探索 |\n",
    "| 衰减率 | 0.99 ~ 0.999 | 控制探索下降 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## 5.2 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"运行所有单元测试\"\"\"\n",
    "    print(\"开始单元测试...\\n\")\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # 测试1: 环境基本功能\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        state = env.reset()\n",
    "        assert state == (3, 0), f\"起始状态错误: {state}\"\n",
    "        \n",
    "        next_state, reward, done = env.step(0)  # 向上\n",
    "        assert next_state == (2, 0), f\"移动后状态错误: {next_state}\"\n",
    "        assert reward == -1.0, f\"奖励错误: {reward}\"\n",
    "        \n",
    "        print(\"测试1通过: 环境基本功能正常\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试1失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试2: 悬崖惩罚\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        env.reset()\n",
    "        next_state, reward, done = env.step(1)  # 向右进入悬崖\n",
    "        assert reward == -100.0, f\"悬崖惩罚错误: {reward}\"\n",
    "        assert next_state == env.start, \"掉入悬崖后应重置到起点\"\n",
    "        \n",
    "        print(\"测试2通过: 悬崖惩罚正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试2失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试3: Q-Learning 更新\n",
    "    try:\n",
    "        agent = QLearningAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "        state = (0, 0)\n",
    "        next_state = (0, 1)\n",
    "        \n",
    "        assert agent.q_table[state][0] == 0.0, \"初始 Q 值应为 0\"\n",
    "        \n",
    "        td_error = agent.update(state, 0, -1.0, next_state, False)\n",
    "        # Q(s,a) = 0 + 0.5 * (-1 + 0.9 * 0 - 0) = -0.5\n",
    "        assert np.isclose(agent.q_table[state][0], -0.5), \\\n",
    "            f\"Q值更新错误: {agent.q_table[state][0]}\"\n",
    "        \n",
    "        print(\"测试3通过: Q-Learning 更新正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试3失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试4: SARSA 更新\n",
    "    try:\n",
    "        agent = SARSAAgent(n_actions=4, learning_rate=0.5, discount_factor=0.9)\n",
    "        state = (0, 0)\n",
    "        next_state = (0, 1)\n",
    "        \n",
    "        agent.q_table[next_state] = np.array([1.0, 2.0, 0.0, 0.0])\n",
    "        agent.update(state, 0, -1.0, next_state, 1, False)  # next_action=1\n",
    "        # Q(s,a) = 0 + 0.5 * (-1 + 0.9 * 2.0 - 0) = 0.4\n",
    "        assert np.isclose(agent.q_table[state][0], 0.4), \\\n",
    "            f\"SARSA更新错误: {agent.q_table[state][0]}\"\n",
    "        \n",
    "        print(\"测试4通过: SARSA 更新正确\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试4失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 测试5: 训练收敛性\n",
    "    try:\n",
    "        env = CliffWalkingEnv()\n",
    "        agent = QLearningAgent(\n",
    "            n_actions=4, learning_rate=0.5,\n",
    "            epsilon=0.1, epsilon_decay=1.0, epsilon_min=0.1\n",
    "        )\n",
    "        \n",
    "        metrics = train_q_learning(env, agent, episodes=200, verbose=False)\n",
    "        avg_reward = np.mean(metrics.episode_rewards[-50:])\n",
    "        assert avg_reward > -100, f\"训练未收敛: {avg_reward}\"\n",
    "        \n",
    "        print(f\"测试5通过: 训练收敛 (最后50回合平均奖励: {avg_reward:.2f})\")\n",
    "        passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"测试5失败: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # 总结\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"测试完成: {passed} 通过, {failed} 失败\")\n",
    "    if failed == 0:\n",
    "        print(\"所有测试通过！\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return failed == 0\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## 5.3 练习题\n",
    "\n",
    "### 练习1: 调整超参数\n",
    "\n",
    "尝试修改学习率、折扣因子和探索率，观察对学习效果的影响。\n",
    "\n",
    "### 练习2: 实现 Expected SARSA\n",
    "\n",
    "Expected SARSA 使用期望而非采样：\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma \\mathbb{E}_\\pi[Q(S',A')] - Q(S,A)]$$\n",
    "\n",
    "### 练习3: 在 Gymnasium 环境中测试\n",
    "\n",
    "尝试在 Taxi-v3 或 FrozenLake-v1 环境中训练 Q-Learning 智能体。\n",
    "\n",
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Watkins, C.J.C.H. (1989). *Learning from Delayed Rewards*. PhD Thesis.\n",
    "2. Sutton, R.S. & Barto, A.G. (2018). *Reinforcement Learning: An Introduction*, 2nd ed. Chapter 6.\n",
    "3. Van Hasselt, H. (2010). *Double Q-learning*. NeurIPS.\n",
    "\n",
    "---\n",
    "\n",
    "[返回目录](README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
