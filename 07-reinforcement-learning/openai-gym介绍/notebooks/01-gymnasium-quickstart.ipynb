{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gymnasium 强化学习环境快速入门\n",
    "\n",
    "---\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "Gymnasium (原 OpenAI Gym) 提供了强化学习研究的**标准化接口**，定义了智能体与环境交互的统一 API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学原理\n",
    "\n",
    "强化学习建模为**马尔可夫决策过程 (MDP)**:\n",
    "\n",
    "$$MDP = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$$\n",
    "\n",
    "其中:\n",
    "- $\\mathcal{S}$: 状态空间\n",
    "- $\\mathcal{A}$: 动作空间  \n",
    "- $P(s'|s,a)$: 状态转移概率\n",
    "- $R(s,a,s')$: 奖励函数\n",
    "- $\\gamma \\in [0,1]$: 折扣因子\n",
    "\n",
    "智能体目标是最大化**期望累积折扣奖励**:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 环境安装与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖 (如果尚未安装)\n",
    "# !pip install gymnasium[classic-control] matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(f\"Gymnasium 版本: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 创建第一个环境: CartPole\n",
    "\n",
    "CartPole 是经典的控制问题：通过左右移动小车来平衡竖立的杆子。\n",
    "\n",
    "| 属性 | 描述 |\n",
    "|------|------|\n",
    "| **状态** | [小车位置, 小车速度, 杆角度, 杆角速度] |\n",
    "| **动作** | 0 (向左推) 或 1 (向右推) |\n",
    "| **奖励** | 每步 +1 |\n",
    "| **终止** | 杆角度 > 12° 或 小车位置 > 2.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 CartPole 环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"环境 ID: {env.spec.id}\")\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"最大步数: {env.spec.max_episode_steps}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观测空间详情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"观测空间详情:\")\n",
    "print(f\"  形状: {env.observation_space.shape}\")\n",
    "print(f\"  下界: {env.observation_space.low}\")\n",
    "print(f\"  上界: {env.observation_space.high}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 环境交互基础\n",
    "\n",
    "### 核心 API\n",
    "\n",
    "```python\n",
    "observation, info = env.reset()           # 重置环境\n",
    "observation, reward, terminated, truncated, info = env.step(action)  # 执行动作\n",
    "```\n",
    "\n",
    "- `terminated`: 任务完成（成功或失败）\n",
    "- `truncated`: 回合因时间限制等原因被截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置环境\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"初始观测:\")\n",
    "print(f\"  小车位置: {observation[0]:.4f}\")\n",
    "print(f\"  小车速度: {observation[1]:.4f}\")\n",
    "print(f\"  杆角度:   {observation[2]:.4f} rad\")\n",
    "print(f\"  杆角速度: {observation[3]:.4f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset(seed=42)\n",
    "\n",
    "# 执行一个动作\n",
    "action = 1  # 向右推\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"执行动作: {action} (向右推)\")\n",
    "print(f\"获得奖励: {reward}\")\n",
    "print(f\"终止: {terminated}, 截断: {truncated}\")\n",
    "print(f\"新观测: {next_obs}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 运行完整回合\n",
    "\n",
    "定义回合执行函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy_fn, seed=None):\n",
    "    \"\"\"\n",
    "    运行一个完整回合\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        环境实例\n",
    "    policy_fn : callable\n",
    "        策略函数，输入观测返回动作\n",
    "    seed : int, optional\n",
    "        随机种子\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_reward : float\n",
    "        回合总奖励\n",
    "    steps : int\n",
    "        回合步数\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = policy_fn(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return total_reward, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义不同策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(obs):\n",
    "    \"\"\"随机策略\"\"\"\n",
    "    return np.random.randint(2)\n",
    "\n",
    "def angle_policy(obs):\n",
    "    \"\"\"基于角度的简单策略: 杆往哪边倒就往哪边推\"\"\"\n",
    "    pole_angle = obs[2]\n",
    "    return 1 if pole_angle > 0 else 0\n",
    "\n",
    "def pid_policy(obs):\n",
    "    \"\"\"PID 控制策略\"\"\"\n",
    "    x, x_dot, theta, theta_dot = obs\n",
    "    u = 50 * theta + 10 * theta_dot + 0.5 * x + 1.0 * x_dot\n",
    "    return 1 if u > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估不同策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "policies = {\n",
    "    \"随机策略\": random_policy,\n",
    "    \"角度策略\": angle_policy,\n",
    "    \"PID策略\": pid_policy\n",
    "}\n",
    "\n",
    "n_episodes = 20\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"策略评估 ({n_episodes} 回合)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, policy in policies.items():\n",
    "    rewards = []\n",
    "    for i in range(n_episodes):\n",
    "        reward, steps = run_episode(env, policy, seed=i)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    results[name] = rewards\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  平均奖励: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "    print(f\"  最小/最大: {np.min(rewards):.0f} / {np.max(rewards):.0f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化策略比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 箱线图\n",
    "ax1 = axes[0]\n",
    "data = [results[name] for name in results]\n",
    "bp = ax1.boxplot(data, labels=list(results.keys()), patch_artist=True)\n",
    "colors = ['#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax1.set_ylabel('回合奖励')\n",
    "ax1.set_title('策略性能分布')\n",
    "ax1.axhline(y=475, color='r', linestyle='--', label='成功阈值 (475)')\n",
    "ax1.legend()\n",
    "\n",
    "# 条形图\n",
    "ax2 = axes[1]\n",
    "means = [np.mean(results[name]) for name in results]\n",
    "stds = [np.std(results[name]) for name in results]\n",
    "x = np.arange(len(results))\n",
    "bars = ax2.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(list(results.keys()))\n",
    "ax2.set_ylabel('平均奖励')\n",
    "ax2.set_title('策略平均性能')\n",
    "ax2.axhline(y=475, color='r', linestyle='--', label='成功阈值')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. MountainCar - 稀疏奖励环境\n",
    "\n",
    "**挑战**: 小车引擎不够强，无法直接爬上山顶，需要利用来回摆动积累动量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MountainCar-v0 环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"  位置范围: [{env.observation_space.low[0]:.2f}, {env.observation_space.high[0]:.2f}]\")\n",
    "print(f\"  速度范围: [{env.observation_space.low[1]:.3f}, {env.observation_space.high[1]:.3f}]\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"  0: 向左, 1: 不动, 2: 向右\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCar 地形可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.linspace(-1.2, 0.6, 200)\n",
    "y = np.sin(3 * x) * 0.45 + 0.55\n",
    "\n",
    "ax.plot(x, y, 'b-', linewidth=3, label='地形')\n",
    "ax.fill_between(x, 0, y, alpha=0.3, color='green')\n",
    "ax.axvline(x=-0.5, color='red', linestyle='--', alpha=0.7, label='起点')\n",
    "ax.axvline(x=0.5, color='gold', linestyle='--', linewidth=2, label='目标')\n",
    "\n",
    "# 绘制小车\n",
    "car_x = -0.5\n",
    "car_y = np.sin(3 * car_x) * 0.45 + 0.55\n",
    "ax.plot(car_x, car_y + 0.05, 'ro', markersize=15, label='小车')\n",
    "\n",
    "ax.set_xlabel('位置')\n",
    "ax.set_ylabel('高度')\n",
    "ax.set_title('MountainCar 环境地形')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-1.3, 0.7)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动量策略测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_policy(obs):\n",
    "    \"\"\"动量策略: 跟随当前速度方向加速\"\"\"\n",
    "    position, velocity = obs\n",
    "    return 2 if velocity > 0 else 0\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "print(\"测试动量策略:\")\n",
    "rewards = []\n",
    "for i in range(10):\n",
    "    reward, steps = run_episode(env, momentum_policy, seed=i)\n",
    "    rewards.append(reward)\n",
    "    print(f\"  回合 {i+1}: 奖励={reward:.0f}, 步数={steps}\")\n",
    "\n",
    "print(f\"\\n平均奖励: {np.mean(rewards):.1f} ± {np.std(rewards):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pendulum - 连续动作空间\n",
    "\n",
    "动作是连续的扭矩值 $u \\in [-2, 2]$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Pendulum-v1 环境信息\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"  观测: [cos(θ), sin(θ), θ̇]\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"  扭矩范围: [{env.action_space.low[0]:.1f}, {env.action_space.high[0]:.1f}]\")\n",
    "print(f\"\\n这是一个连续动作空间环境!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD 控制器演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_controller(obs):\n",
    "    \"\"\"PD 控制器策略\"\"\"\n",
    "    cos_theta, sin_theta, theta_dot = obs\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    Kp, Kd = 10.0, 2.0\n",
    "    torque = -Kp * theta - Kd * theta_dot\n",
    "    return np.clip([torque], -2.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "obs, _ = env.reset(seed=42)\n",
    "observations = [obs]\n",
    "actions = []\n",
    "rewards_list = []\n",
    "\n",
    "for _ in range(200):\n",
    "    action = pd_controller(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    observations.append(obs)\n",
    "    actions.append(action[0])\n",
    "    rewards_list.append(reward)\n",
    "\n",
    "print(f\"回合总奖励: {sum(rewards_list):.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "observations = np.array(observations)\n",
    "steps = np.arange(len(observations))\n",
    "\n",
    "# 角度\n",
    "ax1 = axes[0, 0]\n",
    "theta = np.arctan2(observations[:, 1], observations[:, 0])\n",
    "ax1.plot(steps, np.degrees(theta), 'b-', linewidth=2)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('步数')\n",
    "ax1.set_ylabel('角度 (度)')\n",
    "ax1.set_title('摆角变化')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 角速度\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(steps, observations[:, 2], 'g-', linewidth=2)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('步数')\n",
    "ax2.set_ylabel('角速度 (rad/s)')\n",
    "ax2.set_title('角速度变化')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 扭矩\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(range(len(actions)), actions, 'r-', linewidth=2)\n",
    "ax3.set_xlabel('步数')\n",
    "ax3.set_ylabel('扭矩')\n",
    "ax3.set_title('控制输入')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 奖励\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(range(len(rewards_list)), np.cumsum(rewards_list), 'purple', linewidth=2)\n",
    "ax4.set_xlabel('步数')\n",
    "ax4.set_ylabel('累积奖励')\n",
    "ax4.set_title('累积奖励')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 空间类型详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Gymnasium 空间类型\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Discrete\n",
    "discrete = spaces.Discrete(5)\n",
    "print(f\"\\n1. Discrete(5): {{0, 1, 2, 3, 4}}\")\n",
    "print(f\"   采样: {[discrete.sample() for _ in range(5)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Box\n",
    "box = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "print(f\"Box([-1,1]^3): 连续空间\")\n",
    "print(f\"  形状: {box.shape}\")\n",
    "print(f\"  采样: {box.sample()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MultiDiscrete\n",
    "multi_discrete = spaces.MultiDiscrete([3, 2, 4])\n",
    "print(f\"MultiDiscrete([3,2,4]): 多维离散空间\")\n",
    "print(f\"  每维范围: [0,3), [0,2), [0,4)\")\n",
    "print(f\"  采样: {multi_discrete.sample()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dict\n",
    "dict_space = spaces.Dict({\n",
    "    \"position\": spaces.Box(-10, 10, shape=(2,)),\n",
    "    \"velocity\": spaces.Box(-1, 1, shape=(2,)),\n",
    "    \"flag\": spaces.Discrete(2)\n",
    "})\n",
    "print(f\"Dict 空间:\")\n",
    "sample = dict_space.sample()\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 环境包装器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RecordEpisodeStatistics(env)\n",
    "\n",
    "print(\"使用 RecordEpisodeStatistics 包装器:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "if 'episode' in info:\n",
    "    print(f\"回合奖励: {info['episode']['r']}\")\n",
    "    print(f\"回合长度: {info['episode']['l']}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义归一化包装器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    \"\"\"在线观测归一化包装器\"\"\"\n",
    "    \n",
    "    def __init__(self, env, epsilon=1e-8):\n",
    "        super().__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = np.zeros(env.observation_space.shape)\n",
    "        self.var = np.ones(env.observation_space.shape)\n",
    "        self.count = 0\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        self.count += 1\n",
    "        delta = obs - self.mean\n",
    "        self.mean += delta / self.count\n",
    "        self.var += delta * (obs - self.mean)\n",
    "        std = np.sqrt(self.var / max(1, self.count) + self.epsilon)\n",
    "        return (obs - self.mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = NormalizeObservation(env)\n",
    "\n",
    "print(\"归一化前后观测对比:\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "print(f\"步骤 0 - 归一化观测: {obs}\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs, _, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "print(f\"步骤 50 - 归一化观测: {obs}\")\n",
    "print(f\"\\n估计的均值: {env.mean}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 向量化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.vector import SyncVectorEnv\n",
    "\n",
    "n_envs = 4\n",
    "\n",
    "def make_env():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "vec_env = SyncVectorEnv([make_env for _ in range(n_envs)])\n",
    "\n",
    "print(f\"向量化环境信息:\")\n",
    "print(f\"  环境数量: {vec_env.num_envs}\")\n",
    "print(f\"  单环境观测空间: {vec_env.single_observation_space}\")\n",
    "print(f\"  批量观测空间: {vec_env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行采样\n",
    "obs, info = vec_env.reset()\n",
    "print(f\"批量观测形状: {obs.shape}\")\n",
    "\n",
    "actions = vec_env.action_space.sample()\n",
    "print(f\"批量动作: {actions}\")\n",
    "\n",
    "obs, rewards, terminateds, truncateds, infos = vec_env.step(actions)\n",
    "print(f\"批量奖励: {rewards}\")\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 总结\n",
    "\n",
    "本教程涵盖了 Gymnasium 的核心概念:\n",
    "\n",
    "1. **环境创建**: `gym.make(env_id)`\n",
    "2. **基本交互**: `reset()`, `step()`, `close()`\n",
    "3. **空间类型**: Discrete, Box, MultiDiscrete, Dict\n",
    "4. **包装器**: 观测/动作/奖励预处理\n",
    "5. **向量化环境**: 并行采样\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- 学习 Q-Learning 和 SARSA 算法\n",
    "- 探索深度强化学习 (DQN, PPO)\n",
    "- 尝试更复杂的环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经典控制环境列表\n",
    "print(\"经典控制环境列表:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "classic_envs = [\n",
    "    (\"CartPole-v1\", \"倒立摆平衡\"),\n",
    "    (\"MountainCar-v0\", \"爬山车 (离散)\"),\n",
    "    (\"Acrobot-v1\", \"双摆控制\"),\n",
    "    (\"Pendulum-v1\", \"单摆控制 (连续)\"),\n",
    "]\n",
    "\n",
    "for env_id, desc in classic_envs:\n",
    "    try:\n",
    "        env = gym.make(env_id)\n",
    "        obs_dim = env.observation_space.shape\n",
    "        act_type = \"离散\" if isinstance(env.action_space, spaces.Discrete) else \"连续\"\n",
    "        print(f\"  {env_id:30s} | 观测: {str(obs_dim):10s} | 动作: {act_type}\")\n",
    "        env.close()\n",
    "    except:\n",
    "        print(f\"  {env_id:30s} | 未安装\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
