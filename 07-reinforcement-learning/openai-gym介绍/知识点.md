# OpenAI Gymnasium 强化学习环境 - 核心知识点

> 本文档提供了 Gymnasium 强化学习环境的关键知识点总结，帮助深度理解和复习。

---

## 目录

1. [马尔可夫决策过程 (MDP)](#1-马尔可夫决策过程-mdp)
2. [Gymnasium 核心 API](#2-gymnasium-核心-api)
3. [空间类型详解](#3-空间类型详解)
4. [经典控制环境](#4-经典控制环境)
5. [环境包装器](#5-环境包装器)
6. [控制理论基础](#6-控制理论基础)
7. [在线统计算法](#7-在线统计算法)
8. [最佳实践与设计模式](#8-最佳实践与设计模式)
9. [思考题与练习](#9-思考题与练习)

---

## 1. 马尔可夫决策过程 (MDP)

### 核心概念

**马尔可夫决策过程**是强化学习的数学基础，定义为五元组：

$$MDP = (\mathcal{S}, \mathcal{A}, P, R, \gamma)$$

| 符号 | 含义 | 说明 |
|------|------|------|
| $\mathcal{S}$ | 状态空间 | 环境所有可能状态的集合 |
| $\mathcal{A}$ | 动作空间 | 智能体可执行的所有动作集合 |
| $P(s'|s,a)$ | 转移概率 | 在状态 $s$ 执行动作 $a$ 后转移到 $s'$ 的概率 |
| $R(s,a,s')$ | 奖励函数 | 转移产生的即时奖励 |
| $\gamma \in [0,1]$ | 折扣因子 | 未来奖励的衰减系数 |

### 马尔可夫性质

> **核心思想**：未来只依赖于当前状态，与历史无关。

$$P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1}|S_t, A_t)$$

### 目标函数

智能体的目标是最大化**期望累积折扣奖励**（回报）：

$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

**直观理解**：
- $\gamma = 0$：只关注即时奖励（短视）
- $\gamma = 1$：同等重视所有未来奖励（远见但可能发散）
- $\gamma \in (0,1)$：平衡即时与长期收益

### 自测问题

1. 为什么需要折扣因子 $\gamma < 1$？
2. 哪些现实问题违反马尔可夫性质？如何处理？

---

## 2. Gymnasium 核心 API

### 环境生命周期

```
创建 → 重置 → [执行动作 → 获取反馈]* → 关闭
```

### 核心接口

```python
import gymnasium as gym

# 1. 创建环境
env = gym.make("CartPole-v1")

# 2. 重置环境（开始新回合）
observation, info = env.reset(seed=42)

# 3. 执行动作（交互循环）
observation, reward, terminated, truncated, info = env.step(action)

# 4. 关闭环境（释放资源）
env.close()
```

### 返回值详解

| 返回值 | 类型 | 含义 |
|--------|------|------|
| `observation` | np.ndarray | 环境当前状态 |
| `reward` | float | 本步奖励 |
| `terminated` | bool | 是否达到终止状态（成功/失败） |
| `truncated` | bool | 是否因时间限制等外部原因截断 |
| `info` | dict | 额外调试信息 |

### `terminated` vs `truncated` 的区别

| 类型 | 含义 | 示例 |
|------|------|------|
| `terminated=True` | 任务完成（成功或失败） | CartPole 杆子倒下 |
| `truncated=True` | 回合被截断（非自然结束） | 达到最大步数限制 |

**重要性**：在计算价值估计时，`terminated` 需要将未来回报置零，`truncated` 则需要进行 bootstrapping。

---

## 3. 空间类型详解

### Discrete（离散空间）

$$\mathcal{A} = \{0, 1, ..., n-1\}$$

```python
action_space = spaces.Discrete(4)  # {0, 1, 2, 3}
action_space.sample()  # 随机采样
```

**应用场景**：有限个选择的决策（上下左右、是否购买）

### Box（连续空间）

$$\mathcal{A} = [low, high]^{shape}$$

```python
action_space = spaces.Box(
    low=-1.0, high=1.0,
    shape=(3,),
    dtype=np.float32
)
```

**应用场景**：连续控制（机械臂角度、车辆速度）

### MultiDiscrete（多维离散空间）

$$\mathcal{A} = \{0,...,n_1-1\} \times \{0,...,n_2-1\} \times ...$$

```python
action_space = spaces.MultiDiscrete([3, 2, 4])
# 第一维: {0,1,2}, 第二维: {0,1}, 第三维: {0,1,2,3}
```

**应用场景**：多个独立的离散决策

### Dict（字典空间）

复合空间，组合多种类型：

```python
observation_space = spaces.Dict({
    "position": spaces.Box(-10, 10, shape=(2,)),
    "inventory": spaces.Discrete(10),
    "flags": spaces.MultiBinary(4)
})
```

---

## 4. 经典控制环境

### 4.1 CartPole - 倒立摆平衡

#### 问题描述

在移动的小车上平衡一根杆子，通过左右推动小车保持杆子直立。

#### 状态空间

| 变量 | 符号 | 范围 |
|------|------|------|
| 小车位置 | $x$ | $[-4.8, 4.8]$ |
| 小车速度 | $\dot{x}$ | $(-\infty, \infty)$ |
| 杆角度 | $\theta$ | $[-24°, 24°]$ |
| 杆角速度 | $\dot{\theta}$ | $(-\infty, \infty)$ |

#### 动力学方程

$$\ddot{\theta} = \frac{g\sin\theta + \cos\theta \cdot \frac{-F - m_p l \dot{\theta}^2 \sin\theta}{m_c + m_p}}{l\left(\frac{4}{3} - \frac{m_p \cos^2\theta}{m_c + m_p}\right)}$$

#### 控制策略

**简单策略**（基于角度）：
```python
action = 1 if theta > 0 else 0  # 杆往哪边倒就往哪边推
```

**PID 控制策略**：
$$u = K_p \theta + K_d \dot{\theta} + K_x x + K_v \dot{x}$$

---

### 4.2 MountainCar - 爬山车

#### 问题描述

小车引擎动力不足，无法直接爬上山顶，需要利用来回摆动积累动量。

#### 动力学方程

$$v_{t+1} = v_t + 0.001 \cdot a - 0.0025 \cdot \cos(3x_t)$$
$$x_{t+1} = x_t + v_{t+1}$$

#### 核心挑战：稀疏奖励

- 每步奖励：$-1$
- 到达目标：回合结束

**问题**：随机策略几乎无法到达目标，需要智能探索。

#### 能量视角

势能：$E_p = \sin(3x) \cdot 0.45 + 0.55$

**策略思想**：增加系统总能量，直到足以越过山顶。

---

### 4.3 Pendulum - 单摆控制

#### 问题描述

将初始下垂的单摆摆动到直立位置并保持平衡。

#### 动力学方程

$$\ddot{\theta} = -\frac{3g}{2l}\sin(\theta + \pi) + \frac{3}{ml^2}u$$

#### 奖励函数

$$r = -(\theta^2 + 0.1\dot{\theta}^2 + 0.001u^2)$$

**设计思想**：
- $-\theta^2$：惩罚偏离目标角度
- $-0.1\dot{\theta}^2$：惩罚高速摆动
- $-0.001u^2$：惩罚过大的控制输入（能量效率）

#### 连续控制

动作为扭矩 $u \in [-2, 2]$，需要使用连续动作算法（DDPG、SAC 等）。

---

### 4.4 Acrobot - 双摆控制

#### 问题描述

控制双连杆机器人末端到达目标高度。只能控制中间关节，底端关节自由摆动。

#### 系统特性

- **欠驱动系统**：自由度 > 控制输入数
- 需要利用动力学耦合实现控制

---

## 5. 环境包装器

### 包装器设计模式

```python
class MyWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        # 初始化

    def step(self, action):
        # 预处理 action
        obs, reward, term, trunc, info = self.env.step(action)
        # 后处理 obs, reward
        return obs, reward, term, trunc, info
```

### 常用包装器类型

#### 观测包装器 (ObservationWrapper)

```python
class NormalizeObservation(gym.ObservationWrapper):
    def observation(self, obs):
        return (obs - self.mean) / self.std
```

**应用**：归一化、帧堆叠、图像裁剪

#### 动作包装器 (ActionWrapper)

```python
class ClipAction(gym.ActionWrapper):
    def action(self, action):
        return np.clip(action, self.low, self.high)
```

**应用**：动作裁剪、重缩放、离散化

#### 奖励包装器 (RewardWrapper)

```python
class ClipReward(gym.RewardWrapper):
    def reward(self, reward):
        return np.clip(reward, -1, 1)
```

**应用**：奖励裁剪、归一化、塑形

### 包装器堆叠顺序

```python
env = gym.make("Pendulum-v1")
env = NormalizeObservation(env)  # 最内层
env = ClipAction(env)
env = NormalizeReward(env)        # 最外层
```

**执行顺序**：
- `step(action)`: 外 → 内（先处理动作）
- 返回值: 内 → 外（后处理观测和奖励）

---

## 6. 控制理论基础

### PID 控制器

$$u(t) = K_p e(t) + K_i \int_0^t e(\tau)d\tau + K_d \frac{de(t)}{dt}$$

| 项 | 作用 | 副作用 |
|----|------|--------|
| P (比例) | 减小当前误差 | 可能产生稳态误差 |
| I (积分) | 消除稳态误差 | 可能导致超调 |
| D (微分) | 预测并阻尼震荡 | 对噪声敏感 |

### 离散化 PD 控制

```python
def pd_controller(obs, Kp=10.0, Kd=2.0):
    theta = np.arctan2(obs[1], obs[0])  # sin, cos -> angle
    theta_dot = obs[2]
    torque = -Kp * theta - Kd * theta_dot
    return np.clip([torque], -2.0, 2.0)
```

### 能量控制

**思想**：将系统能量驱动到目标能量。

$$u = -k(E - E_{target}) \cdot \dot{\theta}$$

**优势**：可以将系统从任意初始状态摆动到目标状态。

---

## 7. 在线统计算法

### Welford 算法

在线计算均值和方差，避免数值不稳定：

$$\bar{x}_n = \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n}$$
$$M_{2,n} = M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n)$$
$$\sigma^2_n = \frac{M_{2,n}}{n}$$

**优势**：
- 单次遍历，$O(1)$ 空间
- 数值稳定（避免大数相减）

```python
class RunningStatistics:
    def update(self, x):
        self.count += 1
        delta = x - self.mean
        self.mean += delta / self.count
        delta2 = x - self.mean
        self.M2 += delta * delta2
```

### 指数移动平均 (EMA)

$$\bar{x}_t = \alpha \cdot x_t + (1-\alpha) \cdot \bar{x}_{t-1}$$

**特点**：近期数据权重更高，适合非平稳过程。

---

## 8. 最佳实践与设计模式

### 环境使用最佳实践

1. **总是设置随机种子**以保证可复现性
   ```python
   obs, info = env.reset(seed=42)
   ```

2. **使用 context manager 或显式关闭**
   ```python
   env.close()
   ```

3. **检查空间类型**后再处理
   ```python
   if isinstance(env.action_space, spaces.Discrete):
       action = np.random.randint(env.action_space.n)
   ```

### 策略设计模式

```python
class BasePolicy(ABC):
    @abstractmethod
    def __call__(self, obs: np.ndarray) -> Any:
        pass

class CartPolePolicy(BasePolicy):
    def __init__(self, method: str = "pid"):
        self.method = method

    def __call__(self, obs):
        if self.method == "pid":
            return self._pid_policy(obs)
        # ...
```

### 向量化环境

```python
from gymnasium.vector import SyncVectorEnv

vec_env = SyncVectorEnv([lambda: gym.make("CartPole-v1") for _ in range(4)])
obs, _ = vec_env.reset()  # shape: (4, obs_dim)
```

**优势**：并行采样，提高数据收集效率。

---

## 9. 思考题与练习

### 基础理解

1. **MDP 建模**：将"下棋"建模为 MDP，定义 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$。

2. **折扣因子**：在 CartPole 中，$\gamma = 0.99$ 和 $\gamma = 0.9$ 会导致什么不同的行为？

3. **空间类型选择**：设计一个"交通信号灯控制"的观测空间和动作空间。

### 进阶实践

4. **实现奖励塑形**：为 MountainCar 设计一个密集奖励函数，加速学习。

5. **包装器组合**：实现一个包装器，将连续动作空间离散化为 N 个离散动作。

6. **控制器调参**：调整 Pendulum 的 PD 控制器参数，观察对控制效果的影响。

### 思考题

7. 为什么 MountainCar 的随机策略表现很差？如何改进探索？

8. 能量控制器相比 PD 控制器有什么优势和劣势？

9. 为什么需要奖励归一化？不归一化会有什么问题？

---

## 参考资源

- [Gymnasium 官方文档](https://gymnasium.farama.org/)
- [Spinning Up in Deep RL](https://spinningup.openai.com/)
- Sutton & Barto, *Reinforcement Learning: An Introduction*

---

## 核心要点回顾

| 主题 | 关键点 |
|------|--------|
| MDP | 五元组定义、马尔可夫性质、回报定义 |
| API | reset/step/close 三步走 |
| 空间 | Discrete、Box、MultiDiscrete、Dict |
| CartPole | 欠驱动控制、PID 策略 |
| MountainCar | 稀疏奖励、动量积累 |
| Pendulum | 连续控制、能量塑形 |
| 包装器 | Obs/Action/Reward 三类、堆叠顺序 |
| 统计 | Welford 算法、EMA |

---

*最后更新: 2025-12*
