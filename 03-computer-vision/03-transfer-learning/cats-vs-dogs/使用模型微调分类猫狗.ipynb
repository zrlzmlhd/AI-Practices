{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-14T07:48:38.608328Z",
     "start_time": "2025-09-14T06:54:28.154933Z"
    }
   },
   "source": "\"\"\"\n模型微调(Fine-tuning)进阶技术\n\n模型微调是迁移学习的高级技巧:\n- 不仅使用预训练权重,还对其进行微调\n- 选择性解冻部分卷积层参与训练\n- 在目标数据集上进一步优化特征\n\n与特征提取的对比:\n1. 特征提取: 完全冻结卷积基,仅训练分类器\n2. 微调: 解冻顶层卷积块,同时训练\n\n微调的关键原则:\n- 只微调顶层(高级特征层)\n- 保持底层冻结(通用特征层)\n- 使用非常小的学习率\n- 先训练分类器后再微调\n\n技术要点:\n- Block5(VGG16最后一个卷积块)最适合微调\n- 学习率要比从头训练小10-100倍\n- 微调通常能提升2-5%的准确率\n\n作者: [Your Name]\n日期: 2024-01\n\"\"\"\n\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras  \nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# 环境配置\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# 随机种子\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\nprint(f\"TensorFlow版本: {tf.__version__}\")\nprint(f\"GPU可用: {len(tf.config.list_physical_devices('GPU')) > 0}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "xuhaer2k8o8",
   "source": "\"\"\"\n阶段2: 解冻Block5进行微调\n\n微调策略:\n1. 只解冻最后一个卷积块(block5)\n2. 保持前面的块冻结\n3. 使用很小的学习率(1e-5)\n\n为什么只解冻Block5?\n- Block1-4学习的是通用特征(边缘、纹理)\n- Block5学习的是任务相关的高级特征\n- 微调Block5可以适应新任务,同时保持底层特征\n\"\"\"\n\n# 解冻block5的所有层\nprint(\"=\" * 60)\nprint(\"解冻策略\")\nprint(\"=\" * 60)\n\nconv_base.trainable = True\nset_trainable = False\n\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        # 从block5_conv1开始解冻\n        set_trainable = True\n        print(f\"从 {layer.name} 开始解冻\")\n    \n    layer.trainable = set_trainable\n\n# 显示每层的冻结状态\nprint(\"\\n层冻结状态:\")\nfor i, layer in enumerate(conv_base.layers):\n    trainable_status = \"可训练\" if layer.trainable else \"冻结\"\n    print(f\"{i:2d}. {layer.name:20s} - {trainable_status}\")\n\nprint(\"=\" * 60)\n\n# 检查参数统计\ntrainable_params_ft = sum([np.prod(w.shape) for w in model.trainable_weights])\nnon_trainable_params_ft = sum([np.prod(w.shape) for w in model.non_trainable_weights])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"阶段2: 微调Block5\")\nprint(\"=\" * 60)\nprint(f\"可训练参数: {trainable_params_ft:,} ({trainable_params_ft/1e6:.2f}M)\")\nprint(f\"冻结参数: {non_trainable_params_ft:,} ({non_trainable_params_ft/1e6:.2f}M)\")\nprint(f\"与阶段1对比: 新增 {trainable_params_ft-trainable_params:,} 可训练参数\")\nprint(f\"学习率: {LR_FINETUNE} (降低10倍!)\")\nprint(\"=\" * 60 + \"\\n\")\n\n# 重新编译模型(使用更小的学习率!)\nmodel.compile(\n    optimizer=optimizers.RMSprop(learning_rate=LR_FINETUNE),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# 微调训练\nprint(\"开始微调...\")\nhistory_finetune = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS_FINETUNE,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"阶段2完成 - 微调效果对比\")\nprint(\"=\" * 60)\nfinal_acc_finetune = history_finetune.history['val_accuracy'][-1]\nprint(f\"阶段1(冻结)验证准确率: {final_acc_frozen:.4f}\")\nprint(f\"阶段2(微调)验证准确率: {final_acc_finetune:.4f}\")\nprint(f\"准确率提升: {final_acc_finetune - final_acc_frozen:+.4f}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "s5n54ypy0b",
   "source": "\"\"\"\n总结与最佳实践\n\n本notebook展示了完整的模型微调流程\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"迁移学习三种方法对比\")\nprint(\"=\" * 60)\nprint(\"\\n1. 从头训练CNN:\")\nprint(\"   ✓ 完全自定义\")\nprint(\"   ✗ 需要大量数据和训练时间\")\nprint(\"   ✗ 容易过拟合\")\nprint(\"\\n2. 特征提取(冻结卷积基):\")\nprint(\"   ✓ 训练快速\")\nprint(\"   ✓ 数据需求少\")\nprint(\"   ✗ 性能可能不是最优\")\nprint(\"\\n3. 模型微调(本notebook方法):\")\nprint(\"   ✓ 性能最佳\")\nprint(\"   ✓ 在目标数据集上优化特征\")\nprint(\"   ✗ 训练时间较长\")\nprint(\"   ✗ 需要仔细调整学习率\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"微调的关键要点\")\nprint(\"=\" * 60)\nprint(\"\\n✓ 分阶段训练:\")\nprint(\"  1. 先冻结卷积基,训练分类器\")\nprint(\"  2. 再解冻顶层,进行微调\")\nprint(\"\\n✓ 学习率策略:\")\nprint(\"  - 阶段1: 正常学习率(1e-4)\")\nprint(\"  - 阶段2: 降低10-100倍(1e-5或更小)\")\nprint(\"\\n✓ 解冻策略:\")\nprint(\"  - 仅解冻顶层卷积块\")\nprint(\"  - 保持底层特征冻结\")\nprint(\"\\n✓ 数据增强:\")\nprint(\"  - 提升模型泛化能力\")\nprint(\"  - 减少过拟合\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"生产环境建议\")\nprint(\"=\" * 60)\nprint(\"1. 使用更多训练轮数(阶段1: 10-20, 阶段2: 50-100)\")\nprint(\"2. 实现学习率衰减策略\")\nprint(\"3. 使用EarlyStopping防止过拟合\")\nprint(\"4. 保存最佳模型检查点\")\nprint(\"5. 在多个测试集上验证性能\")\nprint(\"6. 考虑模型集成提升鲁棒性\")\nprint(\"=\" * 60)\n\nprint(\"\\nNotebook执行完成! ✓\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5195r26n35m",
   "source": "\"\"\"\n在测试集上评估最终模型性能\n\n使用完全独立的测试集评估模型的泛化能力\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"测试集评估\")\nprint(\"=\" * 60)\nprint(\"正在评估模型...\")\n\ntest_loss, test_acc = model.evaluate(\n    test_generator,\n    steps=test_generator.samples // BATCH_SIZE,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"最终性能评估\")\nprint(\"=\" * 60)\nprint(f\"测试集损失: {test_loss:.4f}\")\nprint(f\"测试集准确率: {test_acc:.4f}\")\nprint(\"\\n性能对比:\")\nprint(f\"  验证集准确率: {val_acc_ft[-1]:.4f}\")\nprint(f\"  测试集准确率: {test_acc:.4f}\")\nprint(f\"  差异: {abs(val_acc_ft[-1] - test_acc):.4f}\")\n\nif abs(val_acc_ft[-1] - test_acc) < 0.02:\n    print(\"\\n✓ 优秀! 验证集和测试集性能接近,模型泛化良好\")\nelif abs(val_acc_ft[-1] - test_acc) < 0.05:\n    print(\"\\n✓ 良好,模型泛化可接受\")\nelse:\n    print(\"\\n⚠️  验证集和测试集性能差异较大,可能存在过拟合\")\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qouw56kcmzf",
   "source": "\"\"\"\n可视化完整训练过程(阶段1+阶段2)\n\n合并两个阶段的训练历史,展示完整的学习曲线\n\"\"\"\n\n# 定义平滑函数\ndef smooth_curve(points, factor=0.8):\n    \"\"\"\n    指数移动平均平滑曲线\n    \n    Args:\n        points: 原始数据点\n        factor: 平滑因子,越大越平滑\n    \"\"\"\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n\n# 合并两个阶段的历史\nacc_frozen = history_frozen.history['accuracy']\nval_acc_frozen = history_frozen.history['val_accuracy']\nloss_frozen = history_frozen.history['loss']\nval_loss_frozen = history_frozen.history['val_loss']\n\nacc_ft = history_finetune.history['accuracy']\nval_acc_ft = history_finetune.history['val_accuracy']\nloss_ft = history_finetune.history['loss']\nval_loss_ft = history_finetune.history['val_loss']\n\n# 合并数据\nacc_combined = acc_frozen + acc_ft\nval_acc_combined = val_acc_frozen + val_acc_ft\nloss_combined = loss_frozen + loss_ft\nval_loss_combined = val_loss_frozen + val_loss_ft\n\nepochs_combined = range(1, len(acc_combined) + 1)\nfinetune_start_epoch = len(acc_frozen)\n\n# 绘制原始曲线\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# 原始准确率曲线\nax1.plot(epochs_combined, acc_combined, 'bo-', label='训练准确率', alpha=0.6, markersize=5)\nax1.plot(epochs_combined, val_acc_combined, 'rs-', label='验证准确率', alpha=0.6, markersize=5)\nax1.axvline(x=finetune_start_epoch, color='g', linestyle='--', linewidth=2, label='开始微调')\nax1.set_title('训练和验证准确率', fontsize=14, fontweight='bold')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('准确率')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 原始损失曲线\nax2.plot(epochs_combined, loss_combined, 'bo-', label='训练损失', alpha=0.6, markersize=5)\nax2.plot(epochs_combined, val_loss_combined, 'rs-', label='验证损失', alpha=0.6, markersize=5)\nax2.axvline(x=finetune_start_epoch, color='g', linestyle='--', linewidth=2, label='开始微调')\nax2.set_title('训练和验证损失', fontsize=14, fontweight='bold')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('损失')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 平滑准确率曲线\nax3.plot(epochs_combined, smooth_curve(acc_combined), 'b-', label='训练准确率(平滑)', linewidth=2)\nax3.plot(epochs_combined, smooth_curve(val_acc_combined), 'r-', label='验证准确率(平滑)', linewidth=2)\nax3.axvline(x=finetune_start_epoch, color='g', linestyle='--', linewidth=2, label='开始微调')\nax3.set_title('训练和验证准确率(平滑)', fontsize=14, fontweight='bold')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('准确率')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 平滑损失曲线\nax4.plot(epochs_combined, smooth_curve(loss_combined), 'b-', label='训练损失(平滑)', linewidth=2)\nax4.plot(epochs_combined, smooth_curve(val_loss_combined), 'r-', label='验证损失(平滑)', linewidth=2)\nax4.axvline(x=finetune_start_epoch, color='g', linestyle='--', linewidth=2, label='开始微调')\nax4.set_title('训练和验证损失(平滑)', fontsize=14, fontweight='bold')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('损失')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"训练过程分析\")\nprint(\"=\" * 60)\nprint(f\"阶段1(冻结VGG16): Epoch 1-{finetune_start_epoch}\")\nprint(f\"  最终训练准确率: {acc_frozen[-1]:.4f}\")\nprint(f\"  最终验证准确率: {val_acc_frozen[-1]:.4f}\")\nprint(f\"\\n阶段2(微调Block5): Epoch {finetune_start_epoch+1}-{len(epochs_combined)}\")\nprint(f\"  最终训练准确率: {acc_ft[-1]:.4f}\")\nprint(f\"  最终验证准确率: {val_acc_ft[-1]:.4f}\")\nprint(f\"\\n总体提升:\")\nprint(f\"  验证准确率提升: {val_acc_ft[-1] - val_acc_frozen[-1]:+.4f}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T07:48:42.866581Z",
     "start_time": "2025-09-14T07:48:42.720047Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n配置参数和数据路径\n\"\"\"\n\n# 数据路径\nDATA_ROOT = Path(\"猫狗数据集/dataset\")\nTRAIN_DIR = DATA_ROOT / \"train\"\nVALIDATION_DIR = DATA_ROOT / \"validation\"\nTEST_DIR = DATA_ROOT / \"test\"\n\n# 图像参数\nIMG_HEIGHT = 150\nIMG_WIDTH = 150\nIMG_CHANNELS = 3\n\n# 训练参数\nBATCH_SIZE = 32\nEPOCHS_INITIAL = 2  # 初始训练分类器的轮数(测试用)\nEPOCHS_FINETUNE = 2  # 微调的轮数(测试用)\nLR_INITIAL = 1e-4    # 初始训练学习率\nLR_FINETUNE = 1e-5   # 微调学习率(比初始训练小10倍!)\n\n# VGG16权重路径\nVGG_WEIGHTS_PATH = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# 检查权重文件\nif VGG_WEIGHTS_PATH and Path(VGG_WEIGHTS_PATH).exists():\n    weights_source = VGG_WEIGHTS_PATH\n    print(f\"✓ 使用本地VGG16权重\")\nelse:\n    weights_source = 'imagenet'\n    print(f\"⚠️  使用在线下载的ImageNet权重\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"微调配置\")\nprint(\"=\" * 60)\nprint(f\"图像尺寸: {IMG_HEIGHT}x{IMG_WIDTH}\")\nprint(f\"批次大小: {BATCH_SIZE}\")\nprint(f\"初始训练轮数: {EPOCHS_INITIAL} (测试配置)\")\nprint(f\"微调轮数: {EPOCHS_FINETUNE} (测试配置)\")\nprint(f\"初始学习率: {LR_INITIAL}\")\nprint(f\"微调学习率: {LR_FINETUNE} (降低10倍)\")\nprint(\"=\" * 60)",
   "id": "5453bd43cd1d1fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T07:48:47.323471Z",
     "start_time": "2025-09-14T07:48:47.170624Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n加载VGG16卷积基并构建完整模型\n\"\"\"\n\n# 加载VGG16卷积基\nconv_base = VGG16(\n    weights=weights_source,\n    include_top=False,\n    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n)\n\n# 构建完整模型\nmodel = models.Sequential(name='VGG16_Finetuning')\nmodel.add(conv_base)\nmodel.add(layers.Flatten(name='flatten'))\nmodel.add(layers.Dense(256, activation='relu', name='fc1'))\nmodel.add(layers.Dropout(0.5, name='dropout'))\nmodel.add(layers.Dense(1, activation='sigmoid', name='output'))\n\nprint(\"=\" * 60)\nprint(\"完整模型架构\")\nprint(\"=\" * 60)\nmodel.summary()\nprint(\"=\" * 60)\n\n# 显示VGG16的层结构\nprint(\"\\nVGG16卷积基的层:\")\nprint(\"=\" * 60)\nfor i, layer in enumerate(conv_base.layers):\n    print(f\"{i:2d}. {layer.name:20s} - 输出形状: {str(layer.output_shape):30s}\")\nprint(\"=\" * 60)",
   "id": "b189063510f2148f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T07:58:45.398529Z",
     "start_time": "2025-09-14T07:58:42.012884Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n数据加载和预处理\n\n使用数据增强提升模型泛化能力\n\"\"\"\n\n# 创建数据增强生成器\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# 验证/测试集仅归一化\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\n# 创建数据生成器\ntrain_generator = train_datagen.flow_from_directory(\n    str(TRAIN_DIR),\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=True,\n    seed=RANDOM_SEED\n)\n\nvalidation_generator = val_test_datagen.flow_from_directory(\n    str(VALIDATION_DIR),\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    str(TEST_DIR),\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\n# 计算训练步数\nsteps_per_epoch = train_generator.samples // BATCH_SIZE\nvalidation_steps = validation_generator.samples // BATCH_SIZE\n\nprint(\"=\" * 60)\nprint(\"数据加载完成\")\nprint(\"=\" * 60)\nprint(f\"训练样本: {train_generator.samples}\")\nprint(f\"验证样本: {validation_generator.samples}\")\nprint(f\"测试样本: {test_generator.samples}\")\nprint(f\"每epoch步数: {steps_per_epoch}\")\nprint(\"=\" * 60)",
   "id": "269ec683c28ccb00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\"\"\"\n阶段1: 冻结卷积基,训练顶层分类器\n\n微调的最佳实践:\n1. 先完全冻结卷积基\n2. 训练新添加的分类器层\n3. 然后再解冻部分层进行微调\n\n为什么要分阶段?\n- 新添加的层权重是随机初始化的\n- 如果直接微调,大梯度会破坏预训练权重\n- 先训练分类器使其收敛,再微调更安全\n\"\"\"\n\n# 冻结整个卷积基\nconv_base.trainable = False\n\n# 编译模型\nmodel.compile(\n    optimizer=optimizers.RMSprop(learning_rate=LR_INITIAL),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# 检查可训练参数\ntrainable_params = sum([np.prod(w.shape) for w in model.trainable_weights])\ntotal_params = model.count_params()\n\nprint(\"=\" * 60)\nprint(\"阶段1: 训练顶层分类器(冻结VGG16)\")\nprint(\"=\" * 60)\nprint(f\"可训练参数: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\nprint(f\"冻结参数: {total_params-trainable_params:,} ({(total_params-trainable_params)/1e6:.2f}M)\")\nprint(f\"总参数: {total_params:,} ({total_params/1e6:.2f}M)\")\nprint(f\"学习率: {LR_INITIAL}\")\nprint(\"=\" * 60 + \"\\n\")\n\n# 训练分类器\nprint(\"开始训练分类器...\")\nhistory_frozen = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS_INITIAL,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"阶段1完成\")\nprint(\"=\" * 60)\nfinal_acc_frozen = history_frozen.history['val_accuracy'][-1]\nprint(f\"验证准确率: {final_acc_frozen:.4f}\")\nprint(\"=\" * 60)",
   "id": "e3312d2f021a6bae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}