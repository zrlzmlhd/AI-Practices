{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单词级别的One-Hot编码\n",
    "\n",
    "## 1. 概述\n",
    "\n",
    "One-Hot编码是自然语言处理中最基础的文本向量化方法。它将每个单词映射到一个高维稀疏向量，向量维度等于词汇表大小，只有对应单词的位置为1，其余位置都为0。\n",
    "\n",
    "### 核心特点：\n",
    "- **稀疏性**：向量中只有一个位置为1，其余全为0\n",
    "- **高维度**：维度等于词汇表大小，通常为几千到几万\n",
    "- **无序性**：不同单词之间没有相似度关系（正交向量）\n",
    "- **离散性**：每个单词被视为独立的符号\n",
    "\n",
    "### 优势：\n",
    "- 实现简单直观\n",
    "- 适合小规模文本处理\n",
    "- 可解释性强\n",
    "\n",
    "### 劣势：\n",
    "- 无法捕捉语义相似性（如\"猫\"和\"狗\"的向量完全正交）\n",
    "- 维度灾难（词汇表越大，向量越稀疏）\n",
    "- 内存占用大\n",
    "- 无法处理未登录词（OOV, Out-of-Vocabulary）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 方法一：手动实现One-Hot编码\n",
    "\n",
    "### 实现步骤：\n",
    "1. 构建词汇表：为每个唯一单词分配一个整数索引\n",
    "2. 创建零矩阵：形状为(样本数, 最大序列长度, 词汇表大小)\n",
    "3. 填充矩阵：将对应位置设置为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 示例文本数据\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "# 步骤1：构建词汇表\n",
    "# 为每个唯一单词分配一个整数索引（从1开始，0保留用于填充）\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            # 索引从1开始，0通常保留用于填充或特殊标记\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "print(\"词汇表索引映射：\")\n",
    "for word, idx in sorted(token_index.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {word:12s} -> {idx}\")\n",
    "print(f\"\\n词汇表大小: {len(token_index)}\")\n",
    "\n",
    "# 步骤2：设置序列最大长度并创建零矩阵\n",
    "max_length = 10  # 只考虑每个样本的前10个单词\n",
    "vocab_size = max(token_index.values()) + 1  # +1是因为索引0保留\n",
    "\n",
    "# 创建三维零矩阵：(样本数, 序列长度, 词汇表大小)\n",
    "results = np.zeros(shape=(len(samples), max_length, vocab_size))\n",
    "\n",
    "print(f\"结果矩阵形状: {results.shape}\")\n",
    "print(f\"  - {results.shape[0]} 个样本\")\n",
    "print(f\"  - 每个样本最多 {results.shape[1]} 个单词\")\n",
    "print(f\"  - 每个单词编码为 {results.shape[2]} 维向量\\n\")\n",
    "\n",
    "# 步骤3：填充One-Hot向量\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        # 将对应位置设置为1\n",
    "        results[i, j, index] = 1\n",
    "\n",
    "# 输出结果分析\n",
    "print(\"=\"*60)\n",
    "print(\"第一个样本的One-Hot编码结果：\")\n",
    "print(\"=\"*60)\n",
    "for i, word in enumerate(samples[0].split()[:max_length]):\n",
    "    word_vector = results[0, i, :]\n",
    "    non_zero_idx = np.where(word_vector == 1)[0]\n",
    "    print(f\"位置 {i}: '{word:12s}' -> 索引 {non_zero_idx[0] if len(non_zero_idx) > 0 else 'None'}\")\n",
    "\n",
    "print(f\"\\n完整的One-Hot矩阵（第一个样本）：\")\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 方法二：使用Keras Tokenizer实现\n",
    "\n",
    "Keras提供了`Tokenizer`类来简化文本预处理流程。\n",
    "\n",
    "### Tokenizer的核心功能：\n",
    "- **fit_on_texts()**: 根据文本构建词汇表\n",
    "- **texts_to_sequences()**: 将文本转换为整数序列\n",
    "- **texts_to_matrix()**: 直接生成One-Hot矩阵\n",
    "\n",
    "### 参数说明：\n",
    "- **num_words**: 只保留最常见的N个单词\n",
    "- **mode**: 编码模式（binary, count, tfidf, freq）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "# 创建Tokenizer实例\n",
    "# num_words参数指定只考虑最常见的N个单词（按词频排序）\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# 根据文本数据构建词汇表\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# 将文本转换为整数序列\n",
    "# 例如: \"The cat sat\" -> [1, 2, 3]\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "print(\"整数序列表示：\")\n",
    "for i, (sample, seq) in enumerate(zip(samples, sequences)):\n",
    "    print(f\"  样本{i+1}: {sample}\")\n",
    "    print(f\"  序列: {seq}\\n\")\n",
    "\n",
    "# 直接生成One-Hot矩阵\n",
    "# mode='binary': 使用二进制编码（0或1）\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "print(f\"One-Hot矩阵形状: {one_hot_results.shape}\")\n",
    "print(f\"  - {one_hot_results.shape[0]} 个样本\")\n",
    "print(f\"  - 每个样本表示为 {one_hot_results.shape[1]} 维向量\\n\")\n",
    "\n",
    "# 获取词汇表\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"词汇表大小: {len(word_index)}\")\n",
    "print(\"\\n词汇表索引映射：\")\n",
    "for word, idx in sorted(word_index.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {word:12s} -> {idx}\")\n",
    "\n",
    "print(\"\\nOne-Hot矩阵内容：\")\n",
    "print(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 方法三：散列技巧（Hashing Trick）\n",
    "\n",
    "### 散列技巧的原理：\n",
    "散列技巧通过哈希函数将单词映射到固定大小的向量空间，无需显式构建词汇表。\n",
    "\n",
    "### 核心思想：\n",
    "```\n",
    "index = hash(word) % dimensionality\n",
    "```\n",
    "\n",
    "### 优势：\n",
    "1. **内存高效**：不需要存储完整的词汇表\n",
    "2. **可扩展性强**：可以处理任意大小的词汇表\n",
    "3. **支持在线学习**：新词可以直接编码\n",
    "4. **实现简单**：只需要一个哈希函数\n",
    "\n",
    "### 劣势：\n",
    "1. **哈希碰撞**：不同单词可能映射到同一索引\n",
    "2. **不可逆**：无法从索引还原单词\n",
    "3. **碰撞率**：维度越小，碰撞概率越高\n",
    "\n",
    "### 碰撞概率估算：\n",
    "根据生日悖论，当词汇表大小为V，哈希空间维度为D时：\n",
    "- 碰撞概率 ≈ 1 - exp(-V²/(2D))\n",
    "- 建议：D ≥ 2V 以保持较低碰撞率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "# 设置哈希空间维度\n",
    "# 维度越大，碰撞概率越低，但内存占用也越大\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "\n",
    "# 创建零矩阵\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "# 记录哈希映射以检测碰撞\n",
    "hash_mapping = {}\n",
    "\n",
    "print(\"散列编码过程：\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"\\n样本 {i+1}: {sample}\")\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # 计算哈希值并映射到[0, dimensionality)范围\n",
    "        # abs()确保哈希值为正数\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        \n",
    "        # 记录映射关系\n",
    "        if index not in hash_mapping:\n",
    "            hash_mapping[index] = []\n",
    "        hash_mapping[index].append(word)\n",
    "        \n",
    "        results[i, j, index] = 1\n",
    "        print(f\"  位置 {j}: '{word:12s}' -> 哈希索引 {index}\")\n",
    "\n",
    "# 检测哈希碰撞\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"哈希碰撞分析：\")\n",
    "print(\"=\"*60)\n",
    "collisions = {idx: words for idx, words in hash_mapping.items() if len(words) > 1}\n",
    "\n",
    "if collisions:\n",
    "    print(f\"发现 {len(collisions)} 个碰撞：\")\n",
    "    for idx, words in collisions.items():\n",
    "        print(f\"  索引 {idx}: {words}\")\n",
    "else:\n",
    "    print(\"未发现碰撞\")\n",
    "\n",
    "unique_words = len(set(word for sample in samples for word in sample.split()))\n",
    "collision_rate = len(collisions) / unique_words if unique_words > 0 else 0\n",
    "print(f\"\\n碰撞率: {collision_rate:.2%}\")\n",
    "print(f\"唯一单词数: {unique_words}\")\n",
    "print(f\"哈希空间维度: {dimensionality}\")\n",
    "print(f\"空间利用率: {unique_words/dimensionality:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 三种方法对比\n",
    "\n",
    "| 特性 | 手动实现 | Keras Tokenizer | 散列技巧 |\n",
    "|------|----------|-----------------|----------|\n",
    "| **实现难度** | 简单 | 非常简单 | 简单 |\n",
    "| **内存占用** | 高 | 高 | 中 |\n",
    "| **需要词汇表** | 是 | 是 | 否 |\n",
    "| **处理OOV** | 不支持 | 不支持 | 支持 |\n",
    "| **可解释性** | 高 | 高 | 低 |\n",
    "| **碰撞风险** | 无 | 无 | 有 |\n",
    "| **适用场景** | 小规模教学 | 一般NLP任务 | 大规模在线学习 |\n",
    "\n",
    "## 6. 实际应用建议\n",
    "\n",
    "### 选择标准：\n",
    "\n",
    "1. **小规模任务（词汇表 < 10K）**:\n",
    "   - 推荐使用 Keras Tokenizer\n",
    "   - 优势：API简洁，功能完善\n",
    "\n",
    "2. **大规模任务（词汇表 > 100K）**:\n",
    "   - 考虑散列技巧或词嵌入（Word2Vec, GloVe）\n",
    "   - One-Hot编码维度过高，效率低下\n",
    "\n",
    "3. **在线学习场景**:\n",
    "   - 使用散列技巧\n",
    "   - 无需预先构建词汇表\n",
    "\n",
    "4. **生产环境**:\n",
    "   - 首选词嵌入方法（Embedding层、预训练模型）\n",
    "   - One-Hot编码仅用于简单基线模型\n",
    "\n",
    "### One-Hot编码的局限性：\n",
    "\n",
    "在现代NLP中，One-Hot编码已逐渐被更高级的表示方法取代：\n",
    "- **词嵌入**（Word Embeddings）：Word2Vec, GloVe, FastText\n",
    "- **上下文嵌入**（Contextual Embeddings）：ELMo, BERT, GPT\n",
    "\n",
    "这些方法能够：\n",
    "1. 捕捉语义相似性\n",
    "2. 降低维度（通常为100-300维）\n",
    "3. 共享统计信息\n",
    "4. 提升模型泛化能力"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
