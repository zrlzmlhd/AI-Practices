{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词嵌入（Word Embedding）基础\n",
    "\n",
    "## 1. 从One-Hot到词嵌入的演进\n",
    "\n",
    "### 1.1 One-Hot编码的局限性\n",
    "\n",
    "回顾One-Hot编码的问题：\n",
    "\n",
    "```python\n",
    "\"cat\"  -> [0, 0, 1, 0, 0, 0, ..., 0]  # 10,000维，只有1个非零\n",
    "\"dog\"  -> [0, 0, 0, 0, 1, 0, ..., 0]  # 10,000维，只有1个非零\n",
    "\"kitten\" -> [0, 1, 0, 0, 0, 0, ..., 0]  # 10,000维，只有1个非零\n",
    "```\n",
    "\n",
    "**核心问题**：\n",
    "1. **维度灾难**：词汇表10,000个词 = 10,000维向量\n",
    "2. **语义缺失**：\"cat\"和\"kitten\"语义相近，但向量完全正交（内积=0）\n",
    "3. **稀疏性**：99.99%的元素都是0，浪费存储和计算\n",
    "4. **泛化能力差**：模型无法利用词之间的相似性\n",
    "\n",
    "### 1.2 词嵌入的核心思想\n",
    "\n",
    "**词嵌入（Word Embedding）**将高维稀疏的One-Hot向量映射到低维稠密的连续向量空间：\n",
    "\n",
    "```python\n",
    "\"cat\"    -> [0.2, -0.4, 0.7, ..., 0.1]  # 300维，全是实数\n",
    "\"dog\"    -> [0.3, -0.3, 0.8, ..., 0.2]  # 与cat相近\n",
    "\"kitten\" -> [0.25, -0.35, 0.65, ..., 0.15]  # 更接近cat\n",
    "\"car\"    -> [-0.8, 0.6, -0.2, ..., 0.9]  # 距离cat很远\n",
    "```\n",
    "\n",
    "**关键特性**：\n",
    "1. **低维稠密**：通常为100-300维，远小于词汇表大小\n",
    "2. **语义相似性**：相似词在向量空间中距离近\n",
    "3. **可学习**：向量值通过训练学习得到\n",
    "4. **分布式表示**：每个维度捕捉词的某个语义特征\n",
    "\n",
    "## 2. 词嵌入的理论基础\n",
    "\n",
    "### 2.1 分布式假设（Distributional Hypothesis）\n",
    "\n",
    "> \"You shall know a word by the company it keeps.\"  \n",
    "> — J.R. Firth (1957)\n",
    "\n",
    "**核心观点**：词的含义由其上下文决定。\n",
    "\n",
    "**例子**：\n",
    "- \"猫\"经常出现在：\"宠物\", \"喵\", \"抓老鼠\", \"猫粮\" 附近\n",
    "- \"狗\"经常出现在：\"宠物\", \"汪\", \"看家\", \"狗粮\" 附近\n",
    "- 因此\"猫\"和\"狗\"应该有相似的向量表示\n",
    "\n",
    "### 2.2 词嵌入的数学本质\n",
    "\n",
    "词嵌入本质上是一个**查找表（Lookup Table）**：\n",
    "\n",
    "```\n",
    "Embedding矩阵: E ∈ R^(V × d)\n",
    "- V: 词汇表大小（如10,000）\n",
    "- d: 嵌入维度（如300）\n",
    "\n",
    "查找操作:\n",
    "word_id = 42  # \"cat\"的索引\n",
    "word_vector = E[42, :]  # 取出第42行，得到300维向量\n",
    "```\n",
    "\n",
    "**等价视角**：\n",
    "```\n",
    "One-Hot × Embedding矩阵 = 词向量\n",
    "[0, 0, 1, ..., 0] × E = E的第3行\n",
    "    (V维)         (V×d)   (d维)\n",
    "```\n",
    "\n",
    "### 2.3 词嵌入的几何解释\n",
    "\n",
    "在嵌入空间中：\n",
    "- **距离**：相似词距离近（欧氏距离或余弦距离）\n",
    "- **方向**：捕捉语义关系\n",
    "- **类比**：向量运算可表示类比关系\n",
    "\n",
    "**著名例子**：\n",
    "```\n",
    "King - Man + Woman ≈ Queen\n",
    "Paris - France + Italy ≈ Rome\n",
    "```\n",
    "\n",
    "## 3. Keras中的Embedding层\n",
    "\n",
    "Keras提供了`Embedding`层来实现词嵌入。\n",
    "\n",
    "### 3.1 Embedding层的参数\n",
    "\n",
    "```python\n",
    "Embedding(input_dim, output_dim, input_length)\n",
    "```\n",
    "\n",
    "- **input_dim**：词汇表大小（整数索引的最大值+1）\n",
    "- **output_dim**：嵌入维度（词向量的长度）\n",
    "- **input_length**：输入序列的长度（固定长度序列）\n",
    "\n",
    "### 3.2 Embedding层的工作流程\n",
    "\n",
    "```\n",
    "输入：整数序列  [5, 12, 3, 8]        形状: (batch_size, seq_length)\n",
    "       ↓\n",
    "Embedding层：查找嵌入矩阵\n",
    "       ↓\n",
    "输出：词向量序列  [[v5], [v12], [v3], [v8]]  形状: (batch_size, seq_length, embedding_dim)\n",
    "```\n",
    "\n",
    "### 3.3 关键特性\n",
    "\n",
    "1. **可训练**：嵌入矩阵的权重可以通过反向传播学习\n",
    "2. **参数共享**：相同的词总是映射到相同的向量\n",
    "3. **高效**：只是查找操作，不涉及矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实践：在IMDB数据集上使用词嵌入\n",
    "\n",
    "### 4.1 任务说明\n",
    "\n",
    "- **数据集**：IMDB电影评论情感分类\n",
    "- **任务**：根据评论文本判断情感（正面/负面）\n",
    "- **方法**：使用Embedding层 + 全连接层\n",
    "\n",
    "### 4.2 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# 设置超参数\n",
    "max_features = 10000  # 词汇表大小：只考虑最常出现的10,000个单词\n",
    "maxlen = 20          # 序列最大长度：每条评论只取前20个单词\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"加载IMDB数据集\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 加载数据集\n",
    "# num_words参数确保只保留最常见的max_features个词\n",
    "# 每个词被映射为一个整数索引（按词频排序）\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(f\"训练集大小: {len(x_train)}\")\n",
    "print(f\"测试集大小: {len(x_test)}\")\n",
    "print(f\"\\n第一条评论（整数序列）: {x_train[0][:10]}...\")  # 只显示前10个词\n",
    "print(f\"第一条评论长度: {len(x_train[0])}\")\n",
    "print(f\"第一条评论标签: {y_train[0]} ({'正面' if y_train[0] == 1 else '负面'})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"序列填充\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 序列填充：将所有序列统一为相同长度\n",
    "# - 短于maxlen的序列：在前面填充0\n",
    "# - 长于maxlen的序列：截断\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(f\"填充后的训练集形状: {x_train.shape}\")\n",
    "print(f\"填充后的测试集形状: {x_test.shape}\")\n",
    "print(f\"\\n第一条评论（填充后）: {x_train[0]}\")\n",
    "print(f\"\\n解释：\")\n",
    "print(f\"  - 形状 (25000, 20) 表示 25,000条评论，每条20个词\")\n",
    "print(f\"  - 0表示填充位置（padding）\")\n",
    "print(f\"  - 非零数字是词的索引（1-9999）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 模型架构\n",
    "\n",
    "我们构建一个简单的模型：\n",
    "\n",
    "```\n",
    "输入: (batch_size, 20)  # 20个词的整数序列\n",
    "   ↓\n",
    "Embedding: (batch_size, 20, 8)  # 每个词变成8维向量\n",
    "   ↓\n",
    "Flatten: (batch_size, 160)  # 展平为一维向量\n",
    "   ↓\n",
    "Dense: (batch_size, 1)  # 输出单个概率值\n",
    "```\n",
    "\n",
    "**设计说明**：\n",
    "1. **Embedding维度选择8**：通常为50-300，这里为演示用较小值\n",
    "2. **Flatten层**：将3D张量展平为2D，方便全连接层处理\n",
    "3. **无循环结构**：这个模型不考虑词序，类似bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"构建模型\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding层：将整数索引映射为稠密向量\n",
    "    # input_dim=10000: 词汇表大小\n",
    "    # output_dim=8: 每个词的嵌入维度\n",
    "    # input_length=20: 输入序列长度\n",
    "    Embedding(input_dim=max_features, output_dim=8, input_length=maxlen, name='embedding'),\n",
    "    \n",
    "    # Flatten层：将3D张量(batch, 20, 8)展平为2D张量(batch, 160)\n",
    "    Flatten(name='flatten'),\n",
    "    \n",
    "    # 全连接层：输出单个概率值用于二分类\n",
    "    Dense(1, activation='sigmoid', name='output')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer='adam',                # Adam优化器\n",
    "    loss='binary_crossentropy',      # 二分类交叉熵损失\n",
    "    metrics=['accuracy']              # 监控准确率\n",
    ")\n",
    "\n",
    "# 显示模型结构\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n模型参数分析：\")\n",
    "print(f\"  Embedding层参数: {max_features} × 8 = {max_features * 8:,}\")\n",
    "print(f\"  Dense层参数: 160 × 1 + 1(bias) = 161\")\n",
    "print(f\"  总参数: {max_features * 8 + 161:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 训练模型\n",
    "\n",
    "训练过程中，Embedding层的权重会自动学习，使得：\n",
    "- 有助于正面情感的词向量朝一个方向移动\n",
    "- 有助于负面情感的词向量朝另一个方向移动\n",
    "- 语义相近的词向量距离变近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"训练模型\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,                    # 训练10个epoch\n",
    "    batch_size=32,                # 批次大小32\n",
    "    validation_split=0.2          # 20%数据用于验证\n",
    ")\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 分析学习到的词嵌入\n",
    "\n",
    "训练完成后，我们可以提取和分析学到的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"提取词嵌入矩阵\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 获取Embedding层的权重（即词嵌入矩阵）\n",
    "embedding_layer = model.get_layer('embedding')\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"嵌入矩阵形状: {embedding_weights.shape}\")\n",
    "print(f\"  - {embedding_weights.shape[0]} 个词\")\n",
    "print(f\"  - 每个词 {embedding_weights.shape[1]} 维向量\")\n",
    "\n",
    "# 查看几个词的嵌入向量\n",
    "print(\"\\n示例词向量：\")\n",
    "for word_id in [1, 2, 3, 4, 5]:  # IMDB中1=\"the\", 2=\"and\", 3=\"a\", 4=\"of\", 5=\"to\"\n",
    "    word_vector = embedding_weights[word_id]\n",
    "    print(f\"  词索引 {word_id}: {word_vector}\")\n",
    "\n",
    "# 计算词向量之间的余弦相似度\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"计算两个向量的余弦相似度\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(\"\\n词向量相似度分析：\")\n",
    "vec1 = embedding_weights[1]  # \"the\"\n",
    "vec2 = embedding_weights[2]  # \"and\"\n",
    "vec3 = embedding_weights[3]  # \"a\"\n",
    "\n",
    "print(f\"  词1(the) vs 词2(and): {cosine_similarity(vec1, vec2):.4f}\")\n",
    "print(f\"  词1(the) vs 词3(a):   {cosine_similarity(vec1, vec3):.4f}\")\n",
    "print(f\"  词2(and) vs 词3(a):   {cosine_similarity(vec2, vec3):.4f}\")\n",
    "\n",
    "print(\"\\n注意：\")\n",
    "print(\"  - 相似度在[-1, 1]之间\")\n",
    "print(\"  - 值越接近1，词越相似\")\n",
    "print(\"  - 功能词（the, and, a）通常具有相似的向量\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 词嵌入的两种学习方式\n",
    "\n",
    "### 5.1 任务相关的嵌入（Task-Specific Embedding）\n",
    "\n",
    "**特点**：\n",
    "- 随机初始化嵌入矩阵\n",
    "- 在特定任务上从头训练\n",
    "- 嵌入向量针对当前任务优化\n",
    "\n",
    "**优势**：\n",
    "- 简单直接，无需额外数据\n",
    "- 嵌入表示与任务完美匹配\n",
    "\n",
    "**劣势**：\n",
    "- 需要大量标注数据\n",
    "- 训练时间长\n",
    "- 无法利用外部语言知识\n",
    "\n",
    "**适用场景**：\n",
    "- 有大量任务相关数据\n",
    "- 领域特殊，通用词向量不适用\n",
    "\n",
    "### 5.2 预训练嵌入（Pre-trained Embedding）\n",
    "\n",
    "**特点**：\n",
    "- 使用在大规模语料上预训练的词向量\n",
    "- 常见的预训练模型：Word2Vec, GloVe, FastText\n",
    "- 可以固定或微调\n",
    "\n",
    "**优势**：\n",
    "- 包含丰富的语言知识\n",
    "- 数据量小时效果更好\n",
    "- 训练速度快\n",
    "\n",
    "**劣势**：\n",
    "- 可能不完全适配特定任务\n",
    "- 需要下载和管理大文件\n",
    "\n",
    "**适用场景**：\n",
    "- 标注数据有限\n",
    "- 通用NLP任务\n",
    "- 需要快速原型\n",
    "\n",
    "## 6. Embedding层 vs 全连接层\n",
    "\n",
    "### 从数学角度看，Embedding等价于特殊的全连接层：\n",
    "\n",
    "```python\n",
    "# Embedding层的实现\n",
    "output = embedding_matrix[word_id]  # 直接查找\n",
    "\n",
    "# 等价于全连接层的实现\n",
    "one_hot = to_one_hot(word_id, vocab_size)  # [0, 0, 1, 0, ...]\n",
    "output = one_hot @ embedding_matrix  # 矩阵乘法\n",
    "```\n",
    "\n",
    "### 为什么使用Embedding层而不是全连接层？\n",
    "\n",
    "1. **效率**：\n",
    "   - Embedding：O(1)查找操作\n",
    "   - 全连接：O(V)矩阵乘法，V为词汇表大小\n",
    "\n",
    "2. **内存**：\n",
    "   - Embedding：只存储嵌入矩阵\n",
    "   - 全连接：需要先创建One-Hot向量（稀疏且大）\n",
    "\n",
    "3. **语义**：\n",
    "   - Embedding：明确表达\"词查找\"的语义\n",
    "   - 全连接：通用层，语义不明确\n",
    "\n",
    "## 7. 实践建议\n",
    "\n",
    "### 7.1 嵌入维度选择\n",
    "\n",
    "| 词汇表大小 | 推荐维度 | 说明 |\n",
    "|----------|---------|------|\n",
    "| < 10K | 50-100 | 小词汇表，低维度足够 |\n",
    "| 10K-100K | 100-300 | 中等词汇表，标准选择 |\n",
    "| > 100K | 300-500 | 大词汇表，需要更高维度 |\n",
    "\n",
    "**经验法则**：embedding_dim ≈ 4 × log2(vocab_size)\n",
    "\n",
    "### 7.2 训练策略\n",
    "\n",
    "1. **数据充足**（>100K样本）：\n",
    "   - 从头训练词嵌入\n",
    "   - 或使用预训练嵌入并微调\n",
    "\n",
    "2. **数据有限**（<10K样本）：\n",
    "   - 使用预训练嵌入\n",
    "   - 冻结嵌入层（不训练）\n",
    "\n",
    "3. **数据中等**（10K-100K样本）：\n",
    "   - 使用预训练嵌入\n",
    "   - 微调嵌入层（较小学习率）\n",
    "\n",
    "### 7.3 常见陷阱\n",
    "\n",
    "1. **嵌入维度过高**：\n",
    "   - 增加过拟合风险\n",
    "   - 训练速度变慢\n",
    "   - 收益递减\n",
    "\n",
    "2. **忽略padding**：\n",
    "   - 索引0应该保留给padding\n",
    "   - 设置`mask_zero=True`可以忽略padding位置\n",
    "\n",
    "3. **不考虑OOV**：\n",
    "   - 预留特殊索引给未知词（UNK）\n",
    "   - 或使用字符级/子词级模型\n",
    "\n",
    "## 8. 总结\n",
    "\n",
    "### 核心要点：\n",
    "\n",
    "1. **词嵌入解决了One-Hot编码的主要问题**：\n",
    "   - 降维：10,000维 → 300维\n",
    "   - 捕捉语义：相似词距离近\n",
    "   - 可学习：通过数据自动学习\n",
    "\n",
    "2. **Keras的Embedding层**：\n",
    "   - 本质是可训练的查找表\n",
    "   - 输入整数序列，输出稠密向量序列\n",
    "   - 可以随机初始化或加载预训练权重\n",
    "\n",
    "3. **下一步学习**：\n",
    "   - 预训练词向量（Word2Vec, GloVe, FastText）\n",
    "   - 上下文嵌入（ELMo, BERT, GPT）\n",
    "   - 子词级模型（BPE, WordPiece）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
