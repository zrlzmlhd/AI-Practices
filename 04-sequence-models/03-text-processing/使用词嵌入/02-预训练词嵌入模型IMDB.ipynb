{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用预训练词嵌入模型\n",
    "\n",
    "## 1. 为什么使用预训练词嵌入？\n",
    "\n",
    "### 1.1 从头训练 vs 预训练\n",
    "\n",
    "| 维度 | 从头训练 | 预训练词嵌入 |\n",
    "|------|---------|-------------|\n",
    "| **训练数据** | 任务相关的标注数据 | 大规模无标注语料（数十亿词）|\n",
    "| **语义知识** | 仅学到任务相关的模式 | 包含丰富的通用语义知识 |\n",
    "| **数据需求** | 需要大量标注数据（>100K） | 标注数据要求低（<10K）|\n",
    "| **训练时间** | 较长 | 较短（只需微调）|\n",
    "| **小数据集表现** | 差（过拟合） | 好（迁移学习）|\n",
    "| **专业领域** | 好（领域适配） | 一般（可能不适配）|\n",
    "\n",
    "### 1.2 常见的预训练词嵌入\n",
    "\n",
    "#### Word2Vec (2013, Google)\n",
    "- **训练方法**：Skip-gram / CBOW\n",
    "- **训练语料**：Google News (1000亿词)\n",
    "- **维度**：通常300维\n",
    "- **特点**：捕捉语义和句法关系\n",
    "\n",
    "#### GloVe (2014, Stanford)\n",
    "- **全称**：Global Vectors for Word Representation\n",
    "- **训练方法**：基于全局词共现统计\n",
    "- **训练语料**：\n",
    "  - Wikipedia + Gigaword (60亿词)\n",
    "  - Common Crawl (420亿 / 840亿词)\n",
    "- **维度**：50, 100, 200, 300维\n",
    "- **优势**：结合了全局统计和局部上下文\n",
    "\n",
    "#### FastText (2016, Facebook)\n",
    "- **创新**：利用子词信息（n-gram）\n",
    "- **优势**：\n",
    "  - 可以为OOV词生成向量\n",
    "  - 对拼写错误鲁棒\n",
    "  - 适合形态丰富的语言\n",
    "\n",
    "### 1.3 本教程使用GloVe\n",
    "\n",
    "我们将使用GloVe 840B（8400亿token，300维）预训练向量，原因：\n",
    "1. 训练语料规模大，覆盖词汇广\n",
    "2. 性能优秀，广泛使用\n",
    "3. 容易获取和使用\n",
    "\n",
    "## 2. 任务说明\n",
    "\n",
    "**任务**：IMDB电影评论情感分类（二分类）\n",
    "\n",
    "**挑战**：\n",
    "- 使用原始IMDB数据集（而非Keras预处理版本）\n",
    "- 从头构建tokenizer和词汇表\n",
    "- 加载GloVe向量并映射到词汇表\n",
    "- 对比预训练嵌入 vs 随机初始化的效果\n",
    "\n",
    "**数据集说明**：\n",
    "- 50,000条电影评论（25,000训练 + 25,000测试）\n",
    "- 二分类：正面(pos)评论 vs 负面(neg)评论\n",
    "- 每个评论是一段英文文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据加载与预处理\n",
    "\n",
    "### 3.1 读取原始文本文件\n",
    "\n",
    "IMDB数据集通常组织为以下结构：\n",
    "```\n",
    "imdb/\n",
    "  train/\n",
    "    pos/  # 正面评论\n",
    "      0.txt\n",
    "      1.txt\n",
    "      ...\n",
    "    neg/  # 负面评论\n",
    "      0.txt\n",
    "      1.txt\n",
    "      ...\n",
    "  test/\n",
    "    pos/\n",
    "    neg/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"使用Keras内置IMDB数据集\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n注意：本示例使用Keras内置的IMDB数据集\")\n",
    "print(\"如果需要使用自定义数据集，请修改数据加载部分\\n\")\n",
    "\n",
    "# 从Keras加载IMDB数据集的索引\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# 获取词汇表映射（word -> index）\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Keras的IMDB数据集索引从3开始\n",
    "# 0: padding, 1: start token, 2: unknown token\n",
    "# 我们需要调整索引\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "\n",
    "# 创建反向映射（index -> word）\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "# 加载数据（这里加载已经编码为整数的数据）\n",
    "(x_train_sequences, y_train), (x_test_sequences, y_test) = imdb.load_data()\n",
    "\n",
    "print(f\"训练集大小: {len(x_train_sequences)}\")\n",
    "print(f\"测试集大小: {len(x_test_sequences)}\")\n",
    "print(f\"词汇表大小: {len(word_index)}\")\n",
    "\n",
    "# 将整数序列转换回文本（用于后续tokenizer）\n",
    "def decode_review(encoded_review):\n",
    "    \"\"\"将整数序列解码为文本\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
    "\n",
    "# 重构文本列表\n",
    "texts_train = [decode_review(seq) for seq in x_train_sequences]\n",
    "texts_test = [decode_review(seq) for seq in x_test_sequences]\n",
    "\n",
    "print(f\"\\n第一条评论示例：\")\n",
    "print(f\"原始长度: {len(x_train_sequences[0])} 个词\")\n",
    "print(f\"文本内容: {texts_train[0][:200]}...\")  # 显示前200个字符\n",
    "print(f\"标签: {y_train[0]} ({'正面' if y_train[0] == 1 else '负面'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 文本分词和序列化\n",
    "\n",
    "**核心步骤**：\n",
    "1. **Tokenization**：将文本分词并构建词汇表\n",
    "2. **Sequence Encoding**：将文本转换为整数序列\n",
    "3. **Padding**：统一序列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"文本分词和序列化\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 设置超参数\n",
    "max_words = 10000  # 只保留最常见的10,000个词\n",
    "maxlen = 100       # 每条评论最多100个词\n",
    "\n",
    "# 创建Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "# 在训练数据上拟合tokenizer（构建词汇表）\n",
    "print(\"\\n正在构建词汇表...\")\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "\n",
    "# 将文本转换为整数序列\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "# 获取词汇表\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"完整词汇表大小: {len(word_index)}\")\n",
    "print(f\"保留的词汇表大小: {min(max_words, len(word_index))}\")\n",
    "\n",
    "# 显示前10个最常见的词\n",
    "print(\"\\n最常见的10个词：\")\n",
    "sorted_words = sorted(word_index.items(), key=lambda x: x[1])[:10]\n",
    "for word, idx in sorted_words:\n",
    "    print(f\"  {word:15s} -> 索引 {idx}\")\n",
    "\n",
    "# 序列填充\n",
    "print(f\"\\n正在进行序列填充（maxlen={maxlen}）...\")\n",
    "x_train = pad_sequences(sequences_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "print(f\"\\n训练数据形状: {x_train.shape}\")\n",
    "print(f\"测试数据形状: {x_test.shape}\")\n",
    "print(f\"  - {x_train.shape[0]} 条训练评论\")\n",
    "print(f\"  - 每条评论 {x_train.shape[1]} 个词\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 划分训练集和验证集\n",
    "\n",
    "从训练集中划分出一部分作为验证集，用于：\n",
    "- 监控训练过程\n",
    "- 早停（Early Stopping）\n",
    "- 超参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"数据集划分\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 设置训练集和验证集的大小\n",
    "training_samples = 20000   # 用于训练的样本数\n",
    "validation_samples = 5000  # 用于验证的样本数\n",
    "\n",
    "# 打乱数据顺序（保证随机性）\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# 划分训练集和验证集\n",
    "x_train_final = x_train[:training_samples]\n",
    "y_train_final = y_train[:training_samples]\n",
    "\n",
    "x_val = x_train[training_samples:training_samples + validation_samples]\n",
    "y_val = y_train[training_samples:training_samples + validation_samples]\n",
    "\n",
    "print(f\"训练集: {len(x_train_final)} 条\")\n",
    "print(f\"验证集: {len(x_val)} 条\")\n",
    "print(f\"测试集: {len(x_test)} 条\")\n",
    "\n",
    "# 统计标签分布\n",
    "print(f\"\\n训练集标签分布：\")\n",
    "print(f\"  正面: {np.sum(y_train_final == 1)} ({np.mean(y_train_final == 1):.1%})\")\n",
    "print(f\"  负面: {np.sum(y_train_final == 0)} ({np.mean(y_train_final == 0):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载GloVe预训练词向量\n",
    "\n",
    "### 4.1 GloVe文件格式\n",
    "\n",
    "GloVe文件是一个文本文件，每行包含一个词和它的向量：\n",
    "```\n",
    "the -0.038194 -0.24487 0.72812 ... (300个数字)\n",
    "of -0.071953 0.23127 0.023731 ... (300个数字)\n",
    "and -0.026165 -0.12684 0.053322 ... (300个数字)\n",
    "```\n",
    "\n",
    "### 4.2 加载策略\n",
    "\n",
    "1. 读取GloVe文件\n",
    "2. 构建字典：{word: vector}\n",
    "3. 只保留我们词汇表中出现的词\n",
    "4. 未在GloVe中出现的词使用零向量或随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"加载GloVe预训练词向量\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# GloVe文件路径（相对于当前notebook）\n",
    "glove_dir = \"GloVe\"\n",
    "glove_file = \"glove.840B.300d.txt\"\n",
    "glove_path = os.path.join(glove_dir, glove_file)\n",
    "\n",
    "print(f\"\\n正在加载GloVe向量: {glove_path}\")\n",
    "print(\"这可能需要几分钟时间...\\n\")\n",
    "\n",
    "# 存储词向量的字典\n",
    "embeddings_index = {}\n",
    "\n",
    "# 统计信息\n",
    "line_count = 0\n",
    "error_count = 0\n",
    "\n",
    "try:\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            \n",
    "            # 每10万行打印一次进度\n",
    "            if line_count % 100000 == 0:\n",
    "                print(f\"  已处理 {line_count:,} 行...\")\n",
    "            \n",
    "            # 解析每一行\n",
    "            values = line.split()\n",
    "            word = values[0]  # 第一个元素是词\n",
    "            \n",
    "            try:\n",
    "                # 剩余元素是词向量（转换为float32以节省内存）\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                \n",
    "                # 验证向量维度（应该是300）\n",
    "                if len(coefs) == 300:\n",
    "                    embeddings_index[word] = coefs\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                    \n",
    "            except (ValueError, IndexError) as e:\n",
    "                error_count += 1\n",
    "                if error_count <= 5:  # 只打印前5个错误\n",
    "                    print(f\"  警告：第 {line_count} 行解析失败: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"\\n加载完成！\")\n",
    "    print(f\"  总行数: {line_count:,}\")\n",
    "    print(f\"  成功加载: {len(embeddings_index):,} 个词向量\")\n",
    "    print(f\"  错误行数: {error_count:,}\")\n",
    "    print(f\"  向量维度: 300\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n错误：找不到GloVe文件: {glove_path}\")\n",
    "    print(\"\\n请确保GloVe文件已下载并放置在正确的路径。\")\n",
    "    print(\"下载地址: https://nlp.stanford.edu/projects/glove/\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 构建嵌入矩阵\n",
    "\n",
    "**目标**：创建一个嵌入矩阵 E ∈ R^(V × d)，其中：\n",
    "- V = 词汇表大小（max_words）\n",
    "- d = 嵌入维度（300）\n",
    "\n",
    "**步骤**：\n",
    "1. 初始化零矩阵 (max_words × 300)\n",
    "2. 对于词汇表中的每个词：\n",
    "   - 如果词在GloVe中 → 使用GloVe向量\n",
    "   - 如果词不在GloVe中 → 保持零向量（或随机初始化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"构建嵌入矩阵\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "embedding_dim = 300  # GloVe向量维度\n",
    "\n",
    "# 初始化嵌入矩阵（全零）\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "# 统计信息\n",
    "found_count = 0\n",
    "missing_count = 0\n",
    "missing_words = []\n",
    "\n",
    "print(f\"\\n正在构建嵌入矩阵 ({max_words} × {embedding_dim})...\\n\")\n",
    "\n",
    "# 遍历词汇表中的每个词\n",
    "for word, i in word_index.items():\n",
    "    # 只处理前max_words个词\n",
    "    if i < max_words:\n",
    "        # 从GloVe中查找该词的向量\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # 找到了，使用GloVe向量\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found_count += 1\n",
    "        else:\n",
    "            # 未找到，保持零向量\n",
    "            missing_count += 1\n",
    "            if len(missing_words) < 10:  # 记录前10个缺失的词\n",
    "                missing_words.append(word)\n",
    "\n",
    "# 统计报告\n",
    "total_words = min(max_words, len(word_index))\n",
    "coverage = found_count / total_words * 100\n",
    "\n",
    "print(f\"嵌入矩阵构建完成！\")\n",
    "print(f\"  词汇表大小: {total_words:,}\")\n",
    "print(f\"  找到GloVe向量: {found_count:,} ({coverage:.1f}%)\")\n",
    "print(f\"  未找到GloVe向量: {missing_count:,} ({100-coverage:.1f}%)\")\n",
    "\n",
    "if missing_words:\n",
    "    print(f\"\\n未找到向量的词示例：{', '.join(missing_words[:10])}\")\n",
    "\n",
    "print(f\"\\n嵌入矩阵形状: {embedding_matrix.shape}\")\n",
    "print(f\"嵌入矩阵内存占用: {embedding_matrix.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 构建模型\n",
    "\n",
    "### 5.1 模型架构\n",
    "\n",
    "我们构建一个简单的分类模型：\n",
    "\n",
    "```\n",
    "输入 (batch_size, 100)\n",
    "  ↓\n",
    "Embedding (batch_size, 100, 300)  ← 加载GloVe权重\n",
    "  ↓\n",
    "Flatten (batch_size, 30000)\n",
    "  ↓\n",
    "Dense(32, relu) (batch_size, 32)\n",
    "  ↓\n",
    "Dense(1, sigmoid) (batch_size, 1)\n",
    "```\n",
    "\n",
    "### 5.2 关键设计决策\n",
    "\n",
    "**是否冻结Embedding层？**\n",
    "\n",
    "| 策略 | 适用场景 | 优势 | 劣势 |\n",
    "|------|---------|------|------|\n",
    "| **冻结**（trainable=False） | 数据量少(<10K) | 防止过拟合；训练快 | 无法适配任务 |\n",
    "| **微调**（trainable=True） | 数据量中等(10K-100K) | 任务适配；性能更好 | 可能过拟合 |\n",
    "\n",
    "本例中我们**冻结**Embedding层，因为：\n",
    "1. GloVe向量已经包含丰富的语义信息\n",
    "2. 防止过拟合（训练样本有限）\n",
    "3. 训练速度更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"构建模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding层：使用GloVe预训练权重\n",
    "    Embedding(\n",
    "        input_dim=max_words,      # 词汇表大小\n",
    "        output_dim=embedding_dim,  # 嵌入维度(300)\n",
    "        input_length=maxlen,       # 输入序列长度(100)\n",
    "        name='embedding'\n",
    "    ),\n",
    "    \n",
    "    # Flatten层：展平为1D向量\n",
    "    Flatten(name='flatten'),\n",
    "    \n",
    "    # 全连接层\n",
    "    Dense(32, activation='relu', name='dense1'),\n",
    "    \n",
    "    # 输出层\n",
    "    Dense(1, activation='sigmoid', name='output')\n",
    "])\n",
    "\n",
    "# 加载GloVe权重到Embedding层\n",
    "print(\"\\n正在加载GloVe权重到Embedding层...\")\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# 冻结Embedding层（不允许训练）\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "print(\"Embedding层已冻结（trainable=False）\")\n",
    "print(\"\\n模型结构：\")\n",
    "model.summary()\n",
    "\n",
    "# 分析可训练参数\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\n参数统计：\")\n",
    "print(f\"  总参数: {total_params:,}\")\n",
    "print(f\"  可训练参数: {trainable_params:,}\")\n",
    "print(f\"  不可训练参数: {non_trainable_params:,} (Embedding层)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"编译模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n模型编译完成！\")\n",
    "print(\"  优化器: Adam\")\n",
    "print(\"  损失函数: Binary Crossentropy\")\n",
    "print(\"  评估指标: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型\n",
    "\n",
    "### 训练配置\n",
    "\n",
    "- **epochs**: 10（为快速演示，实际可能需要更多）\n",
    "- **batch_size**: 32\n",
    "- **validation_data**: 使用验证集监控性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"训练模型\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_final, y_train_final,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"模型评估\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 在测试集上评估\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\n测试集性能：\")\n",
    "print(f\"  损失: {test_loss:.4f}\")\n",
    "print(f\"  准确率: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# 在验证集上的最终性能\n",
    "val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(f\"\\n验证集性能：\")\n",
    "print(f\"  损失: {val_loss:.4f}\")\n",
    "print(f\"  准确率: {val_acc:.4f} ({val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"可视化训练曲线\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 提取训练历史\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "# 创建图表\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 准确率曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, 'bo-', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(epochs_range, val_acc, 'ro-', label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 损失曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, 'bo-', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs_range, val_loss, 'ro-', label='Validation Loss', linewidth=2)\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印最佳性能\n",
    "best_val_acc = max(val_acc)\n",
    "best_epoch = val_acc.index(best_val_acc) + 1\n",
    "\n",
    "print(f\"\\n最佳验证准确率: {best_val_acc:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 预训练嵌入 vs 随机初始化对比\n",
    "\n",
    "### 实验设计\n",
    "\n",
    "为了展示预训练嵌入的优势，我们训练一个随机初始化的对比模型：\n",
    "\n",
    "| 特性 | 预训练GloVe | 随机初始化 |\n",
    "|------|------------|----------|\n",
    "| 初始权重 | GloVe向量 | 随机值 |\n",
    "| trainable | False | True |\n",
    "| 语义知识 | 包含 | 需要学习 |\n",
    "\n",
    "**预期结果**：\n",
    "- 预训练模型收敛更快\n",
    "- 预训练模型泛化性能更好（验证集准确率更高）\n",
    "- 随机初始化可能过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"对比实验：随机初始化 vs 预训练GloVe\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 构建随机初始化模型\n",
    "model_random = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=max_words,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen,\n",
    "        name='embedding_random'\n",
    "    ),\n",
    "    Flatten(name='flatten_random'),\n",
    "    Dense(32, activation='relu', name='dense1_random'),\n",
    "    Dense(1, activation='sigmoid', name='output_random')\n",
    "])\n",
    "\n",
    "# 注意：这里不加载GloVe权重，也不冻结Embedding层\n",
    "# Embedding层会从随机初始化开始训练\n",
    "\n",
    "model_random.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n随机初始化模型结构：\")\n",
    "model_random.summary()\n",
    "\n",
    "print(\"\\n开始训练随机初始化模型...\")\n",
    "history_random = model_random.fit(\n",
    "    x_train_final, y_train_final,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"性能对比\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 在测试集上评估两个模型\n",
    "test_loss_glove, test_acc_glove = model.evaluate(x_test, y_test, verbose=0)\n",
    "test_loss_random, test_acc_random = model_random.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\n测试集性能对比：\")\n",
    "print(f\"{'':<20s} {'GloVe预训练':<15s} {'随机初始化':<15s} {'差异':<10s}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'准确率':<20s} {test_acc_glove:< 15.4f} {test_acc_random:<15.4f} {test_acc_glove - test_acc_random:+.4f}\")\n",
    "print(f\"{'损失':<20s} {test_loss_glove:<15.4f} {test_loss_random:<15.4f} {test_loss_glove - test_loss_random:+.4f}\")\n",
    "\n",
    "# 可视化对比\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 验证准确率对比\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['val_accuracy'], 'b-', label='GloVe Pre-trained', linewidth=2)\n",
    "plt.plot(history_random.history['val_accuracy'], 'r--', label='Random Init', linewidth=2)\n",
    "plt.title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 验证损失对比\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_loss'], 'b-', label='GloVe Pre-trained', linewidth=2)\n",
    "plt.plot(history_random.history['val_loss'], 'r--', label='Random Init', linewidth=2)\n",
    "plt.title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n关键观察：\")\n",
    "if test_acc_glove > test_acc_random:\n",
    "    improvement = (test_acc_glove - test_acc_random) / test_acc_random * 100\n",
    "    print(f\"  ✓ GloVe预训练模型性能更优，提升 {improvement:.1f}%\")\n",
    "    print(f\"  ✓ 预训练嵌入包含的语义知识有助于泛化\")\n",
    "else:\n",
    "    print(f\"  ⚠ 在此次运行中，随机初始化表现更好\")\n",
    "    print(f\"  ⚠ 可能原因：训练数据充足，模型容量足够\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结与最佳实践\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "1. **预训练词嵌入的优势**：\n",
    "   - 包含在大规模语料上学到的通用语义知识\n",
    "   - 提升小数据集上的性能\n",
    "   - 加快训练收敛速度\n",
    "   - 提高模型泛化能力\n",
    "\n",
    "2. **何时使用预训练嵌入**：\n",
    "   - 标注数据有限（<10K样本）\n",
    "   - 通用NLP任务（非专业领域）\n",
    "   - 需要快速原型开发\n",
    "   - 词汇覆盖率高（>80%）\n",
    "\n",
    "3. **何时从头训练**：\n",
    "   - 标注数据充足（>100K样本）\n",
    "   - 专业领域（医疗、法律等）\n",
    "   - 预训练嵌入覆盖率低\n",
    "   - 需要特定任务适配\n",
    "\n",
    "### 实践建议\n",
    "\n",
    "#### 选择合适的预训练模型\n",
    "\n",
    "| 模型 | 优势 | 适用场景 |\n",
    "|------|------|----------|\n",
    "| **Word2Vec** | 训练快，效果好 | 通用任务 |\n",
    "| **GloVe** | 全局统计，性能稳定 | 通用任务，本教程推荐 |\n",
    "| **FastText** | 支持OOV，子词信息 | 形态丰富的语言，专业术语多 |\n",
    "| **BERT/GPT嵌入** | 上下文相关，SOTA性能 | 对性能要求高的任务 |\n",
    "\n",
    "#### 调优策略\n",
    "\n",
    "1. **嵌入层是否可训练**：\n",
    "   ```python\n",
    "   # 小数据集：冻结\n",
    "   embedding_layer.trainable = False\n",
    "   \n",
    "   # 中等数据集：微调（小学习率）\n",
    "   embedding_layer.trainable = True\n",
    "   optimizer = Adam(learning_rate=1e-4)  # 降低学习率\n",
    "   \n",
    "   # 大数据集：自由训练\n",
    "   embedding_layer.trainable = True\n",
    "   ```\n",
    "\n",
    "2. **处理OOV词**：\n",
    "   - 使用`<UNK>`标记\n",
    "   - 为OOV词随机初始化向量\n",
    "   - 使用FastText（利用子词信息）\n",
    "   - 使用字符级或子词级模型\n",
    "\n",
    "3. **降低内存占用**：\n",
    "   - 使用float16代替float32\n",
    "   - 只加载需要的词向量\n",
    "   - 降低嵌入维度（如300→100）\n",
    "\n",
    "### 进阶方向\n",
    "\n",
    "1. **上下文相关嵌入**：\n",
    "   - ELMo：双向LSTM\n",
    "   - BERT：Transformer编码器\n",
    "   - GPT：Transformer解码器\n",
    "\n",
    "2. **多语言嵌入**：\n",
    "   - MUSE：跨语言词嵌入\n",
    "   - XLM-R：多语言预训练模型\n",
    "\n",
    "3. **领域适配**：\n",
    "   - 在领域语料上微调预训练嵌入\n",
    "   - 使用领域特定的预训练模型（BioBERT、SciBERT等）\n",
    "\n",
    "### 常见问题\n",
    "\n",
    "**Q: GloVe向量覆盖率低怎么办？**\n",
    "- A: 尝试FastText（子词级），或在领域语料上训练Word2Vec\n",
    "\n",
    "**Q: 如何选择嵌入维度？**\n",
    "- A: 标准选择300维；小数据集可用100维；大词汇表可用500维\n",
    "\n",
    "**Q: 预训练嵌入效果不好？**\n",
    "- A: 检查词汇覆盖率；考虑领域差异；尝试微调或从头训练\n",
    "\n",
    "**Q: 如何更新嵌入层？**\n",
    "- A: 设置`trainable=True`并使用较小的学习率\n",
    "\n",
    "## 参考资源\n",
    "\n",
    "- **GloVe**: https://nlp.stanford.edu/projects/glove/\n",
    "- **Word2Vec**: https://code.google.com/archive/p/word2vec/\n",
    "- **FastText**: https://fasttext.cc/\n",
    "- **论文**:\n",
    "  - Pennington et al. (2014): \"GloVe: Global Vectors for Word Representation\"\n",
    "  - Mikolov et al. (2013): \"Efficient Estimation of Word Representations in Vector Space\"\n",
    "  - Bojanowski et al. (2017): \"Enriching Word Vectors with Subword Information\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
