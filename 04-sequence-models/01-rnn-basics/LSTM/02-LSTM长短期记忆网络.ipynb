{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-31T08:46:17.205605Z",
     "start_time": "2025-10-31T08:46:17.151866Z"
    }
   },
   "source": "## 一、为什么需要LSTM？\n\n### SimpleRNN的局限性\n\n传统RNN在处理长序列时存在以下问题：\n\n1. **梯度消失(Vanishing Gradient)**\n   - 梯度在反向传播时呈指数级衰减\n   - 网络难以学习长期依赖关系\n   - 只能记住最近几个时间步的信息\n\n2. **梯度爆炸(Exploding Gradient)**\n   - 梯度可能呈指数级增长\n   - 导致权重更新不稳定\n   - 训练过程难以收敛\n\n### LSTM的解决方案\n\nLSTM通过引入**门控机制(Gating Mechanism)**和**细胞状态(Cell State)**来解决这些问题：\n\n- **细胞状态**：作为信息高速公路，可以让信息不经过非线性变换直接传递\n- **三个门**：控制信息的流动，决定保留、遗忘或输出哪些信息",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "yvyhrlvzyp",
   "source": "## 二、LSTM的结构详解\n\n### LSTM的核心组件\n\nLSTM单元包含以下关键组件：\n\n#### 1. 遗忘门(Forget Gate)\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n\n作用：决定从细胞状态中丢弃哪些信息\n- 输出范围：[0, 1]\n- 0 = 完全遗忘，1 = 完全保留\n\n#### 2. 输入门(Input Gate)\n$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n\n作用：决定向细胞状态中添加哪些新信息\n\n#### 3. 细胞状态更新\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n\n作用：结合遗忘门和输入门的结果，更新细胞状态\n\n#### 4. 输出门(Output Gate)\n$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(C_t)$$\n\n作用：决定输出细胞状态的哪些部分\n\n### 符号说明\n- $\\sigma$: sigmoid函数\n- $\\odot$: 逐元素乘法(Hadamard product)\n- $[h_{t-1}, x_t]$: 拼接上一时刻隐藏状态和当前输入",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ludgydmm92",
   "source": "\"\"\"\n三、Keras中的LSTM实现\n演示不同的LSTM构建方式\n\"\"\"\nfrom tensorflow import keras\nimport numpy as np\n\nprint(\"=\" * 70)\nprint(\"方法1: 使用Sequential API构建LSTM模型\")\nprint(\"=\" * 70)\n\n# 构建基础LSTM模型\nmodel_simple = keras.models.Sequential([\n    # 显式指定输入形状\n    keras.Input(shape=[None, 1]),  # [timesteps, features]\n    \n    # LSTM层\n    keras.layers.LSTM(50, return_sequences=True),\n    keras.layers.LSTM(50),\n    keras.layers.Dense(1)\n])\n\nmodel_simple.summary()\nprint(f\"\\n总参数量: {model_simple.count_params():,}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"方法2: 使用LSTMCell构建自定义循环\")\nprint(\"=\" * 70)\n\n# 使用LSTMCell提供更细粒度的控制\nmodel_cell = keras.models.Sequential([\n    keras.layers.RNN(\n        keras.layers.LSTMCell(20),\n        return_sequences=True,\n        input_shape=[None, 1]\n    ),\n    keras.layers.RNN(\n        keras.layers.LSTMCell(20),\n        return_sequences=True\n    ),\n    # TimeDistributed: 对每个时间步应用相同的Dense层\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\nmodel_cell.summary()\nprint(f\"\\n总参数量: {model_cell.count_params():,}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"LSTM参数计算公式\")\nprint(\"=\" * 70)\nprint(\"LSTM单元有4组权重矩阵(遗忘门、输入门、输出门、候选值)\")\nprint(\"参数量 = 4 × (units × (units + input_dim + 1))\")\nprint(\"\\n示例：units=50, input_dim=1\")\nprint(f\"参数量 = 4 × (50 × (50 + 1 + 1)) = {4 * (50 * (50 + 1 + 1)):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bwun93irqoj",
   "source": "## 四、实战：LSTM用于时间序列预测\n\n我们将创建一个合成的正弦波数据集，并使用LSTM进行预测。这是一个经典的时间序列任务，可以很好地展示LSTM的能力。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jje5tps8ae",
   "source": "\"\"\"\n生成时间序列数据\n创建正弦波叠加噪声的数据集\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 设置随机种子\nnp.random.seed(42)\n\ndef generate_time_series(batch_size, n_steps):\n    \"\"\"\n    生成多个时间序列样本\n    \n    参数:\n        batch_size: 生成的样本数量\n        n_steps: 每个序列的时间步数\n    \n    返回:\n        形状为 (batch_size, n_steps, 1) 的数组\n    \"\"\"\n    # 生成基础频率\n    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n    \n    # 生成时间点\n    time = np.linspace(0, 1, n_steps)\n    \n    # 生成正弦波叠加\n    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # 第一个正弦波\n    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # 第二个正弦波\n    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # 添加噪声\n    \n    return series[..., np.newaxis].astype(np.float32)\n\n# 生成数据集\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 1)\n\n# 划分输入和目标\n# 使用前n_steps个点预测第n_steps+1个点\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n\nprint(\"=\" * 70)\nprint(\"数据集信息\")\nprint(\"=\" * 70)\nprint(f\"训练集形状: X={X_train.shape}, y={y_train.shape}\")\nprint(f\"验证集形状: X={X_valid.shape}, y={y_valid.shape}\")\nprint(f\"测试集形状: X={X_test.shape}, y={y_test.shape}\")\n\n# 可视化几个样本\nfig, axes = plt.subplots(2, 3, figsize=(15, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.plot(X_train[i, :, 0], 'b-', linewidth=2, label='输入序列')\n    ax.axhline(y=y_train[i, 0], color='r', linestyle='--', linewidth=2, label='目标值')\n    ax.set_title(f'样本 {i+1}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('时间步', fontsize=10)\n    ax.set_ylabel('值', fontsize=10)\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n数据生成完成！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5lhsx499d6i",
   "source": "\"\"\"\n构建并训练LSTM模型\n使用SimpleRNN和LSTM进行对比\n\"\"\"\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, SimpleRNN, Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\nprint(\"=\" * 70)\nprint(\"构建并训练SimpleRNN模型（作为基准）\")\nprint(\"=\" * 70)\n\n# 构建SimpleRNN模型\nmodel_rnn = Sequential([\n    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    SimpleRNN(20),\n    Dense(1)\n])\n\nmodel_rnn.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\n# 训练SimpleRNN（快速测试，少量epoch）\nhistory_rnn = model_rnn.fit(\n    X_train, y_train,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid, y_valid),\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"构建并训练LSTM模型\")\nprint(\"=\" * 70)\n\n# 构建LSTM模型\nmodel_lstm = Sequential([\n    LSTM(20, return_sequences=True, input_shape=[None, 1]),\n    LSTM(20),\n    Dense(1)\n])\n\nmodel_lstm.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\n# 训练LSTM\nhistory_lstm = model_lstm.fit(\n    X_train, y_train,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid, y_valid),\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"模型性能对比\")\nprint(\"=\" * 70)\n\n# 在测试集上评估\nrnn_loss, rnn_mae = model_rnn.evaluate(X_test, y_test, verbose=0)\nlstm_loss, lstm_mae = model_lstm.evaluate(X_test, y_test, verbose=0)\n\nprint(f\"\\nSimpleRNN - 测试集MSE: {rnn_loss:.6f}, MAE: {rnn_mae:.6f}\")\nprint(f\"LSTM      - 测试集MSE: {lstm_loss:.6f}, MAE: {lstm_mae:.6f}\")\nprint(f\"\\n性能提升: {((rnn_mae - lstm_mae) / rnn_mae * 100):.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "66wktoqeav",
   "source": "\"\"\"\n可视化预测结果\n对比SimpleRNN和LSTM的预测效果\n\"\"\"\nimport matplotlib.pyplot as plt\n\n# 在测试集上进行预测\ny_pred_rnn = model_rnn.predict(X_test, verbose=0)\ny_pred_lstm = model_lstm.predict(X_test, verbose=0)\n\n# 选择几个样本进行可视化\nn_samples = 3\nfig, axes = plt.subplots(n_samples, 1, figsize=(14, 4*n_samples))\n\nfor i in range(n_samples):\n    ax = axes[i] if n_samples > 1 else axes\n    \n    # 绘制输入序列\n    ax.plot(range(n_steps), X_test[i, :, 0], 'b-', \n            linewidth=2, label='输入序列', alpha=0.7)\n    \n    # 绘制真实值\n    ax.plot(n_steps, y_test[i, 0], 'go', \n            markersize=12, label='真实值')\n    \n    # 绘制SimpleRNN预测\n    ax.plot(n_steps, y_pred_rnn[i, 0], 'rs', \n            markersize=12, label=f'SimpleRNN预测')\n    \n    # 绘制LSTM预测\n    ax.plot(n_steps, y_pred_lstm[i, 0], 'md', \n            markersize=12, label=f'LSTM预测')\n    \n    ax.set_title(f'测试样本 {i+1}', fontsize=13, fontweight='bold')\n    ax.set_xlabel('时间步', fontsize=11)\n    ax.set_ylabel('值', fontsize=11)\n    ax.legend(fontsize=10, loc='best')\n    ax.grid(True, alpha=0.3)\n    \n    # 添加误差标注\n    rnn_error = abs(y_test[i, 0] - y_pred_rnn[i, 0])\n    lstm_error = abs(y_test[i, 0] - y_pred_lstm[i, 0])\n    ax.text(0.02, 0.98, \n            f'SimpleRNN误差: {rnn_error:.4f}\\nLSTM误差: {lstm_error:.4f}',\n            transform=ax.transAxes,\n            verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n            fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# 绘制训练历史对比\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# 损失曲线\nepochs_range = range(1, len(history_rnn.history['loss']) + 1)\nax1.plot(epochs_range, history_rnn.history['loss'], 'b-o', \n         label='SimpleRNN训练', linewidth=2)\nax1.plot(epochs_range, history_rnn.history['val_loss'], 'b--o', \n         label='SimpleRNN验证', linewidth=2)\nax1.plot(epochs_range, history_lstm.history['loss'], 'r-s', \n         label='LSTM训练', linewidth=2)\nax1.plot(epochs_range, history_lstm.history['val_loss'], 'r--s', \n         label='LSTM验证', linewidth=2)\nax1.set_title('训练损失对比', fontsize=14, fontweight='bold')\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('MSE Loss', fontsize=12)\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\n# MAE曲线\nax2.plot(epochs_range, history_rnn.history['mae'], 'b-o', \n         label='SimpleRNN训练', linewidth=2)\nax2.plot(epochs_range, history_rnn.history['val_mae'], 'b--o', \n         label='SimpleRNN验证', linewidth=2)\nax2.plot(epochs_range, history_lstm.history['mae'], 'r-s', \n         label='LSTM训练', linewidth=2)\nax2.plot(epochs_range, history_lstm.history['val_mae'], 'r--s', \n         label='LSTM验证', linewidth=2)\nax2.set_title('MAE对比', fontsize=14, fontweight='bold')\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('MAE', fontsize=12)\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"结论\")\nprint(\"=\" * 70)\nprint(\"LSTM通过其门控机制能够:\")\nprint(\"1. 更好地捕捉长期依赖关系\")\nprint(\"2. 减少梯度消失问题\")\nprint(\"3. 在时间序列预测任务上通常优于SimpleRNN\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}