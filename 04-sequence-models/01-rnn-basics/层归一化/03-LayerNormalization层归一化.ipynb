{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# 层归一化(Layer Normalization)在RNN中的应用\n\n## 教程概览\n\n层归一化(Layer Normalization)是一种正则化技术，特别适用于循环神经网络(RNN)。与批归一化(Batch Normalization)不同，层归一化在特征维度上进行归一化，而不是在batch维度上。\n\n### 核心内容\n1. **归一化技术对比**：Batch Norm vs Layer Norm\n2. **Layer Norm原理**：数学公式和直觉理解\n3. **自定义RNN Cell**：实现带Layer Norm的RNN\n4. **性能对比**：验证Layer Norm的效果\n\n### 知识点\n- 层归一化的数学原理\n- 自定义Keras RNN Cell\n- 训练稳定性提升\n- 收敛速度对比"
  },
  {
   "cell_type": "markdown",
   "id": "nyhx7t2hnb8",
   "source": "## 一、归一化技术对比\n\n### Batch Normalization (批归一化)\n- **归一化维度**：在batch维度上进行归一化\n- **适用场景**：前馈网络、CNN\n- **RNN中的问题**：不同时间步的统计特性可能差异很大\n\n$$\\text{BN}(x) = \\gamma \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$$\n\n其中 $\\mu_B$ 和 $\\sigma_B$ 是batch内的均值和标准差\n\n### Layer Normalization (层归一化)\n- **归一化维度**：在特征维度上进行归一化\n- **适用场景**：RNN、Transformer\n- **优势**：不依赖batch统计，每个样本独立归一化\n\n$$\\text{LN}(x) = \\gamma \\frac{x - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}} + \\beta$$\n\n其中 $\\mu_L$ 和 $\\sigma_L$ 是单个样本特征维度的均值和标准差\n\n### 为什么RNN需要Layer Norm？\n1. **稳定训练**：减少内部协变量偏移(Internal Covariate Shift)\n2. **加速收敛**：通过归一化激活值加快训练速度\n3. **减轻梯度消失/爆炸**：保持激活值在合理范围内",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9gn3nr5q6l",
   "source": "\"\"\"\n二、实现带Layer Normalization的RNN Cell\n自定义RNN单元，集成层归一化\n\"\"\"\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\n\nclass LNSimpleRNNCell(keras.layers.Layer):\n    \"\"\"\n    带层归一化的SimpleRNN单元\n    \n    在标准SimpleRNN的基础上，对输出应用层归一化\n    \"\"\"\n    \n    def __init__(self, units, activation='tanh', **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = keras.activations.get(activation)\n        self.state_size = units\n        self.output_size = units\n        \n    def build(self, input_shape):\n        \"\"\"构建层的权重\"\"\"\n        input_dim = input_shape[-1]\n        \n        # 输入到隐藏状态的权重\n        self.kernel = self.add_weight(\n            shape=(input_dim, self.units),\n            initializer='glorot_uniform',\n            name='kernel'\n        )\n        \n        # 循环权重（隐藏状态到隐藏状态）\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            initializer='orthogonal',\n            name='recurrent_kernel'\n        )\n        \n        # 偏置\n        self.bias = self.add_weight(\n            shape=(self.units,),\n            initializer='zeros',\n            name='bias'\n        )\n        \n        # Layer Normalization层\n        self.layer_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        super().build(input_shape)\n    \n    def call(self, inputs, states):\n        \"\"\"\n        前向传播\n        \n        参数:\n            inputs: 当前时间步的输入，形状 (batch_size, input_dim)\n            states: 上一时间步的隐藏状态，列表形式 [h_{t-1}]\n        \n        返回:\n            outputs: 当前时间步的输出\n            new_states: 新的隐藏状态列表\n        \"\"\"\n        prev_output = states[0]\n        \n        # 标准RNN计算：h = activation(W*x + U*h_{t-1} + b)\n        h = tf.matmul(inputs, self.kernel)\n        h += tf.matmul(prev_output, self.recurrent_kernel)\n        h = tf.nn.bias_add(h, self.bias)\n        \n        # 应用Layer Normalization\n        h = self.layer_norm(h)\n        \n        # 应用激活函数\n        output = self.activation(h)\n        \n        return output, [output]\n    \n    def get_config(self):\n        \"\"\"保存配置以便序列化\"\"\"\n        config = super().get_config()\n        config.update({\n            'units': self.units,\n            'activation': keras.activations.serialize(self.activation)\n        })\n        return config\n\nprint(\"=\" * 70)\nprint(\"自定义LNSimpleRNNCell类定义完成\")\nprint(\"=\" * 70)\nprint(\"特点:\")\nprint(\"1. 继承自keras.layers.Layer\")\nprint(\"2. 实现了build()和call()方法\")\nprint(\"3. 在激活函数前应用Layer Normalization\")\nprint(\"4. 支持序列化和反序列化\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ec0wo751aya",
   "source": "\"\"\"\n三、构建使用自定义Cell的RNN模型\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"构建带Layer Norm的RNN模型\")\nprint(\"=\" * 70)\n\n# 构建使用LNSimpleRNNCell的模型\nmodel_ln = keras.models.Sequential([\n    keras.layers.RNN(\n        LNSimpleRNNCell(32),\n        return_sequences=True,\n        input_shape=(None, 10)\n    ),\n    keras.layers.RNN(\n        LNSimpleRNNCell(32),\n        return_sequences=True\n    ),\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\nmodel_ln.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\nmodel_ln.summary()\nprint(f\"\\n总参数量: {model_ln.count_params():,}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"构建标准SimpleRNN模型（对比基准）\")\nprint(\"=\" * 70)\n\n# 构建标准SimpleRNN模型作为对比\nmodel_standard = keras.models.Sequential([\n    keras.layers.SimpleRNN(32, return_sequences=True, input_shape=(None, 10)),\n    keras.layers.SimpleRNN(32, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\n\nmodel_standard.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\nmodel_standard.summary()\nprint(f\"\\n总参数量: {model_standard.count_params():,}\")\n\nprint(\"\\n✓ 两个模型构建完成\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uqobbikm1g",
   "source": "\"\"\"\n四、生成训练数据并对比性能\n\"\"\"\n\n# 生成合成序列数据\nnp.random.seed(42)\n\ndef generate_sequences(n_samples, n_steps, n_features):\n    \"\"\"\n    生成随机序列数据用于训练\n    \"\"\"\n    X = np.random.randn(n_samples, n_steps, n_features).astype(np.float32)\n    # 目标是输入的变换版本（简单的学习任务）\n    y = np.roll(X, shift=1, axis=1)\n    return X, y\n\n# 生成数据集\nn_samples = 5000\nn_steps = 20\nn_features = 10\n\nX_train, y_train = generate_sequences(n_samples, n_steps, n_features)\nX_val, y_val = generate_sequences(1000, n_steps, n_features)\n\nprint(\"=\" * 70)\nprint(\"数据集生成完成\")\nprint(\"=\" * 70)\nprint(f\"训练集: X={X_train.shape}, y={y_train.shape}\")\nprint(f\"验证集: X={X_val.shape}, y={y_val.shape}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"训练标准SimpleRNN模型\")\nprint(\"=\" * 70)\n\nhistory_standard = model_standard.fit(\n    X_train, y_train,\n    epochs=3,                    # 快速测试，使用较少epoch\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"训练带Layer Norm的RNN模型\")\nprint(\"=\" * 70)\n\nhistory_ln = model_ln.fit(\n    X_train, y_train,\n    epochs=3,                    # 快速测试，使用较少epoch\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"性能对比\")\nprint(\"=\" * 70)\n\nfinal_loss_std = history_standard.history['val_loss'][-1]\nfinal_loss_ln = history_ln.history['val_loss'][-1]\n\nprint(f\"\\n标准SimpleRNN - 最终验证损失: {final_loss_std:.6f}\")\nprint(f\"LayerNorm RNN  - 最终验证损失: {final_loss_ln:.6f}\")\nprint(f\"\\n改进: {((final_loss_std - final_loss_ln) / final_loss_std * 100):.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9a7hhg82xge",
   "source": "\"\"\"\n五、可视化训练过程\n\"\"\"\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs_range = range(1, len(history_standard.history['loss']) + 1)\n\n# 绘制损失曲线\nax1.plot(epochs_range, history_standard.history['loss'], \n         'b-o', label='标准RNN - 训练', linewidth=2)\nax1.plot(epochs_range, history_standard.history['val_loss'], \n         'b--o', label='标准RNN - 验证', linewidth=2)\nax1.plot(epochs_range, history_ln.history['loss'], \n         'r-s', label='LayerNorm RNN - 训练', linewidth=2)\nax1.plot(epochs_range, history_ln.history['val_loss'], \n         'r--s', label='LayerNorm RNN - 验证', linewidth=2)\nax1.set_title('训练损失对比', fontsize=14, fontweight='bold')\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('MSE Loss', fontsize=12)\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\n# 绘制MAE曲线\nax2.plot(epochs_range, history_standard.history['mae'], \n         'b-o', label='标准RNN - 训练', linewidth=2)\nax2.plot(epochs_range, history_standard.history['val_mae'], \n         'b--o', label='标准RNN - 验证', linewidth=2)\nax2.plot(epochs_range, history_ln.history['mae'], \n         'r-s', label='LayerNorm RNN - 训练', linewidth=2)\nax2.plot(epochs_range, history_ln.history['val_mae'], \n         'r--s', label='LayerNorm RNN - 验证', linewidth=2)\nax2.set_title('MAE对比', fontsize=14, fontweight='bold')\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('MAE', fontsize=12)\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"总结\")\nprint(\"=\" * 70)\nprint(\"Layer Normalization的优势:\")\nprint(\"1. 训练更稳定：减少内部协变量偏移\")\nprint(\"2. 收敛更快：通过标准化激活值加速学习\")\nprint(\"3. 泛化更好：作为正则化手段提升模型性能\")\nprint(\"4. 不依赖batch：适合RNN等序列模型\")\nprint(\"\\n应用场景:\")\nprint(\"- 深层RNN网络\")\nprint(\"- Transformer架构\")\nprint(\"- 小batch size训练\")\nprint(\"- 在线学习场景\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}