{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# RNN时间序列预测完整指南\n\n## 教程概览\n\n本教程全面介绍如何使用RNN进行时间序列预测，涵盖从基础到高级的多种预测策略。\n\n### 核心内容\n1. **基准模型**：简单线性回归baseline\n2. **单步预测**：使用RNN预测下一个时间步\n3. **深层RNN**：堆叠多层RNN提升性能\n4. **多步预测**：预测未来多个时间步\n5. **序列到序列**：Seq2Seq架构详解\n\n### 预测任务类型\n- **Sequence-to-One**：输入序列 → 单个值\n- **Sequence-to-Sequence**：输入序列 → 输出序列\n- **多步预测**：自回归预测vs一次性预测\n\n### 学习目标\n- 掌握时间序列预测的基本流程\n- 理解不同RNN架构的适用场景\n- 学会评估和对比模型性能\n- 了解多步预测的不同策略",
   "id": "32c0c49cb1e960a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:41:51.421947Z",
     "start_time": "2025-10-31T07:41:50.111349Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n数据生成：合成时间序列\n生成包含多个正弦波和噪声的时间序列数据\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# 设置随机种子确保可复现\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\ndef generate_time_series(batch_size, n_steps):\n    \"\"\"\n    生成合成的时间序列数据\n    \n    数据特点:\n    - 由两个不同频率的正弦波叠加而成\n    - 添加随机噪声增加真实性\n    - 每个样本的频率和相位随机\n    \n    参数:\n        batch_size: 生成的序列数量\n        n_steps: 每个序列的时间步数\n        \n    返回:\n        形状为 (batch_size, n_steps, 1) 的数组\n    \"\"\"\n    # 生成随机频率和相位偏移\n    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n    \n    # 创建时间轴\n    time = np.linspace(0, 1, n_steps)\n    \n    # 第一个正弦波：振幅0.5，频率10-20Hz\n    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n    \n    # 第二个正弦波：振幅0.2，频率20-40Hz\n    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n    \n    # 添加高斯噪声\n    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n    \n    return series[..., np.newaxis].astype(np.float32)\n\n# 可视化几个样本\nprint(\"=\" * 70)\nprint(\"生成并可视化时间序列样本\")\nprint(\"=\" * 70)\n\nsample_series = generate_time_series(4, 100)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\nfor i, ax in enumerate(axes.flat):\n    ax.plot(sample_series[i, :, 0], linewidth=2)\n    ax.set_title(f'时间序列样本 {i+1}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('时间步', fontsize=10)\n    ax.set_ylabel('值', fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \nplt.tight_layout()\nplt.show()\n\nprint(\"✓ 数据生成函数定义完成\")",
   "id": "1b95f0ccc10c978a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:41:51.444565Z",
     "start_time": "2025-10-31T07:41:51.426271Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n准备训练数据集\n任务：给定前n_steps个点，预测第n_steps+1个点（单步预测）\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"准备数据集\")\nprint(\"=\" * 70)\n\n# 生成数据\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 1)\n\n# 划分输入和目标\n# X: 前50个时间步，y: 第51个时间步\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n\nprint(f\"训练集: X={X_train.shape}, y={y_train.shape}\")\nprint(f\"验证集: X={X_valid.shape}, y={y_valid.shape}\")\nprint(f\"测试集: X={X_test.shape}, y={y_test.shape}\")\nprint(f\"\\n任务类型: Sequence-to-One (序列到单值)\")\nprint(f\"输入: {n_steps}个时间步\")\nprint(f\"输出: 1个值（下一时间步的预测）\")\nprint(\"=\" * 70)",
   "id": "6712f52413efa388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 一、基准模型：线性回归\n\n建立一个简单的baseline模型，用于对比RNN的性能提升。\n\n**模型架构：**\n- Flatten层：将序列展平为一维向量\n- Dense层：线性回归输出\n\n**优点：** 简单快速\n**缺点：** 丢失了时序信息，无法捕捉序列中的模式",
   "id": "cb797c933f7cdc99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:41:57.549435Z",
     "start_time": "2025-10-31T07:41:51.472827Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n训练基准模型\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"基准模型：线性回归\")\nprint(\"=\" * 70)\n\n# 构建模型\nmodel_baseline = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[n_steps, 1]),\n    keras.layers.Dense(1)\n])\n\nmodel_baseline.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"模型结构：\")\nmodel_baseline.summary()\n\n# 训练模型（使用较少epoch进行快速测试）\nprint(\"\\n开始训练...\")\nhistory_baseline = model_baseline.fit(\n    X_train, y_train,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid, y_valid),\n    verbose=1\n)\n\n# 评估\ntest_loss, test_mae = model_baseline.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n基准模型性能:\")\nprint(f\"  测试集MSE: {test_loss:.6f}\")\nprint(f\"  测试集MAE: {test_mae:.6f}\")\nprint(\"=\" * 70)",
   "id": "264495ae45f61968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 二、单层SimpleRNN模型\n\n使用单层SimpleRNN来捕捉时序信息。\n\n**模型架构：**\n- SimpleRNN层：50个隐藏单元，只返回最后一个时间步的输出\n- Dense层：将RNN输出映射到最终预测值\n\n**优势：**\n- 能够捕捉序列中的时间依赖关系\n- 参数量相对较少\n- 训练速度快",
   "id": "83db58b369dce801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:42:20.384088Z",
     "start_time": "2025-10-31T07:41:57.598267Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n训练单层SimpleRNN模型\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"单层SimpleRNN模型\")\nprint(\"=\" * 70)\n\n# 构建模型\nmodel_rnn = keras.models.Sequential([\n    # SimpleRNN层：处理序列数据\n    # return_sequences=False（默认）：只返回最后一个时间步\n    keras.layers.SimpleRNN(50, input_shape=[n_steps, 1]),\n    \n    # 输出层\n    keras.layers.Dense(1)\n])\n\nmodel_rnn.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"模型结构：\")\nmodel_rnn.summary()\n\n# 训练模型\nprint(\"\\n开始训练...\")\nhistory_rnn = model_rnn.fit(\n    X_train, y_train,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid, y_valid),\n    verbose=1\n)\n\n# 评估\ntest_loss_rnn, test_mae_rnn = model_rnn.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nSimpleRNN模型性能:\")\nprint(f\"  测试集MSE: {test_loss_rnn:.6f}\")\nprint(f\"  测试集MAE: {test_mae_rnn:.6f}\")\n\n# 对比基准模型\nimprovement = (test_loss - test_loss_rnn) / test_loss * 100\nprint(f\"\\n相比基准模型，MSE改进: {improvement:.2f}%\")\nprint(\"=\" * 70)",
   "id": "dc71113acbc586ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:26:15.532795Z",
     "start_time": "2025-10-31T07:26:15.531135Z"
    }
   },
   "cell_type": "markdown",
   "source": "## 三、深层RNN模型\n\n通过堆叠多层RNN来增加模型的表达能力。\n\n**模型架构：**\n- 第1层SimpleRNN：50单元，return_sequences=True\n- 第2层SimpleRNN：50单元，return_sequences=True\n- 第3层SimpleRNN：20单元，return_sequences=False\n- Dense输出层\n\n**设计要点：**\n1. 前面的RNN层必须设置`return_sequences=True`才能将序列传递给下一层\n2. 最后一层RNN只需返回最后时间步的输出\n3. 层数递减可以逐步提取更抽象的特征",
   "id": "4bb8a94a30c3916"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:42:20.459276Z",
     "start_time": "2025-10-31T07:42:20.434992Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n训练深层RNN模型\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"深层RNN模型（3层堆叠）\")\nprint(\"=\" * 70)\n\nmodel_deep = keras.models.Sequential([\n    # 第1层：返回完整序列\n    keras.layers.SimpleRNN(50, return_sequences=True, input_shape=[n_steps, 1]),\n    \n    # 第2层：继续返回序列\n    keras.layers.SimpleRNN(50, return_sequences=True),\n    \n    # 第3层：只返回最后一个时间步\n    keras.layers.SimpleRNN(20),\n    \n    # 输出层\n    keras.layers.Dense(1)\n])\n\nmodel_deep.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"模型结构：\")\nmodel_deep.summary()\n\n# 训练模型\nprint(\"\\n开始训练...\")\nhistory_deep = model_deep.fit(\n    X_train, y_train,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid, y_valid),\n    verbose=1\n)\n\n# 评估\ntest_loss_deep, test_mae_deep = model_deep.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n深层RNN模型性能:\")\nprint(f\"  测试集MSE: {test_loss_deep:.6f}\")\nprint(f\"  测试集MAE: {test_mae_deep:.6f}\")\n\nprint(f\"\\n模型对比:\")\nprint(f\"  基准模型MSE: {test_loss:.6f}\")\nprint(f\"  单层RNN MSE: {test_loss_rnn:.6f}\")\nprint(f\"  深层RNN MSE: {test_loss_deep:.6f}\")\nprint(\"=\" * 70)",
   "id": "adb68c303d1ae4df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 四、多步预测策略\n\n预测未来多个时间步有两种主要策略：\n\n### 1. 自回归预测(Autoregressive)\n- 一次预测一个时间步\n- 将预测值作为下一次的输入\n- 优点：使用单输出模型\n- 缺点：误差会累积\n\n### 2. 序列到序列(Sequence-to-Sequence)\n- 一次性预测所有未来时间步\n- 使用TimeDistributed层\n- 优点：避免误差累积\n- 缺点：模型更复杂\n\n下面演示自回归预测方法：",
   "id": "3f0d4ae9a21045bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:44:42.274275Z",
     "start_time": "2025-10-31T07:44:41.622603Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n自回归多步预测示例\n使用已训练的RNN模型预测未来10个时间步\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"多步预测：自回归方法\")\nprint(\"=\" * 70)\n\n# 生成一个测试序列\nn_predict = 10\ntest_series = generate_time_series(1, n_steps + n_predict)\nX_test_seq = test_series[:, :n_steps]\ny_true = test_series[:, n_steps:]\n\n# 自回归预测\nX_current = X_test_seq.copy()\npredictions = []\n\nprint(f\"输入序列长度: {n_steps}\")\nprint(f\"预测步数: {n_predict}\")\nprint(\"\\n开始逐步预测...\")\n\nfor step in range(n_predict):\n    # 预测下一个时间步\n    y_pred = model_rnn.predict(X_current, verbose=0)\n    predictions.append(y_pred[0, 0])\n    \n    # 将预测值添加到输入序列中\n    X_current = np.concatenate([X_current, y_pred.reshape(1, 1, 1)], axis=1)\n    \n    # 移除最旧的时间步，保持固定长度（滑动窗口）\n    X_current = X_current[:, 1:, :]\n\npredictions = np.array(predictions)\n\n# 计算误差\nmse = np.mean((y_true[0, :, 0] - predictions) ** 2)\nprint(f\"\\n多步预测MSE: {mse:.6f}\")\n\n# 可视化\nplt.figure(figsize=(14, 5))\n\n# 绘制输入序列\nplt.plot(range(n_steps), X_test_seq[0, :, 0], \n         'b-', linewidth=2, label='输入序列')\n\n# 绘制真实未来值\nplt.plot(range(n_steps, n_steps + n_predict), y_true[0, :, 0], \n         'g-o', linewidth=2, markersize=6, label='真实值')\n\n# 绘制预测值\nplt.plot(range(n_steps, n_steps + n_predict), predictions, \n         'r--s', linewidth=2, markersize=6, label='预测值')\n\nplt.axvline(x=n_steps, color='gray', linestyle=':', linewidth=2, alpha=0.5)\nplt.text(n_steps, plt.ylim()[1]*0.9, '预测起点', \n         ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat'))\n\nplt.title('自回归多步预测', fontsize=14, fontweight='bold')\nplt.xlabel('时间步', fontsize=12)\nplt.ylabel('值', fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 70)",
   "id": "35dd5735e6c8cae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 五、序列到序列模型(Seq2Seq)\n\nSeq2Seq模型可以一次性预测多个未来时间步，避免自回归方法的误差累积。\n\n**模型架构：**\n- 多层SimpleRNN，全部设置`return_sequences=True`\n- TimeDistributed层：对每个时间步应用相同的Dense层\n- 输出：与输入相同长度的序列\n\n**应用场景：**\n- 机器翻译\n- 视频帧预测\n- 语音合成\n- 多步时间序列预测",
   "id": "b04c6ea453fdb57b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:50:56.131957Z",
     "start_time": "2025-10-31T07:50:56.096479Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\"\n构建并训练Seq2Seq模型\n\"\"\"\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Seq2Seq模型：序列到序列预测\")\nprint(\"=\" * 70)\n\n# 准备Seq2Seq数据\n# 输入：前n_steps个点，输出：后10个点的序列\nn_output_steps = 10\nseries_seq2seq = generate_time_series(10000, n_steps + n_output_steps)\nX_train_seq = series_seq2seq[:7000, :n_steps]\ny_train_seq = series_seq2seq[:7000, -n_output_steps:, np.newaxis]\nX_valid_seq = series_seq2seq[7000:9000, :n_steps]\ny_valid_seq = series_seq2seq[7000:9000, -n_output_steps:, np.newaxis]\n\nprint(f\"训练数据形状: X={X_train_seq.shape}, y={y_train_seq.shape}\")\n\n# 构建模型\nmodel_seq2seq = keras.models.Sequential([\n    # 第1层RNN：保持序列输出\n    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n    \n    # 第2层RNN：继续保持序列\n    keras.layers.SimpleRNN(20, return_sequences=True),\n    \n    # TimeDistributed：对每个时间步应用Dense层\n    # 输出形状：(batch_size, n_output_steps, 1)\n    keras.layers.TimeDistributed(keras.layers.Dense(1))\n])\n\nmodel_seq2seq.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='mse',\n    metrics=['mae']\n)\n\nprint(\"\\n模型结构：\")\nmodel_seq2seq.summary()\n\n# 训练（注意：y现在是序列形状，不是单值）\nprint(\"\\n开始训练...\")\nhistory_seq2seq = model_seq2seq.fit(\n    X_train_seq, y_train_seq,\n    epochs=5,                    # 测试时使用较少epoch\n    batch_size=32,\n    validation_data=(X_valid_seq, y_valid_seq),\n    verbose=1\n)\n\nprint(\"\\n✓ Seq2Seq模型训练完成\")\nprint(\"=\" * 70)",
   "id": "3499b41de64c5c6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "outputs": [],
   "execution_count": null,
   "source": "## 总结与最佳实践\n\n### 模型选择指南\n\n| 模型类型 | 适用场景 | 优点 | 缺点 |\n|---------|---------|------|------|\n| 线性回归 | 简单基准 | 快速、易解释 | 无法捕捉时序模式 |\n| 单层RNN | 短序列预测 | 参数少、训练快 | 长期依赖能力弱 |\n| 深层RNN | 复杂时序模式 | 表达能力强 | 易过拟合、训练慢 |\n| LSTM/GRU | 长序列、长期依赖 | 解决梯度消失 | 计算开销大 |\n| Seq2Seq | 多步预测 | 避免误差累积 | 模型复杂度高 |\n\n### 关键技术点\n\n1. **return_sequences参数**\n   - False（默认）：只返回最后时间步，用于Seq2One\n   - True：返回所有时间步，用于堆叠RNN或Seq2Seq\n\n2. **多步预测策略**\n   - 自回归：简单但误差累积\n   - Seq2Seq：复杂但更准确\n\n3. **数据准备**\n   - 归一化：提升训练稳定性\n   - 滑动窗口：构造监督学习样本\n   - 时间步长选择：权衡信息量和计算成本\n\n### 性能优化建议\n\n- 使用LSTM/GRU替代SimpleRNN\n- 添加Dropout防止过拟合\n- 使用双向RNN捕捉双向依赖\n- 结合注意力机制提升长序列性能\n- 考虑使用Transformer处理超长序列",
   "id": "51ddcd4f45062ebb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}