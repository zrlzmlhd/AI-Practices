{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM/GRU高级应用：时间序列预测\n\n本notebook展示了如何使用LSTM和GRU模型进行时间序列预测，以Jena气候数据集为例。\n\n## 1. 数据加载与初步分析",
   "id": "a68bec1f5874e70d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:23.017831Z",
     "start_time": "2025-09-24T10:30:22.957628Z"
    }
   },
   "source": "import os\nimport numpy as np\nimport tensorflow as tf\nfrom pathlib import Path\n\n# 设置全局随机种子以确保结果可复现\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# 数据路径配置 - 请根据实际情况修改\nDATA_DIR = Path.home() / \"下载\" / \"jena_climate\"\nDATA_FILE = DATA_DIR / \"jena_climate_2009_2016.csv\"\n\n# 检查数据文件是否存在\nif not DATA_FILE.exists():\n    raise FileNotFoundError(\n        f\"数据文件未找到: {DATA_FILE}\\n\"\n        f\"请下载Jena气候数据集并放置在正确的位置。\\n\"\n        f\"下载地址: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\n    )\n\n# 读取数据文件\nwith open(DATA_FILE, 'r', encoding='utf-8') as f:\n    data = f.read()\n\n# 解析CSV数据\nlines = data.split(\"\\n\")\nheader = lines[0].split(\",\")\nlines = lines[1:]\n\nprint(f\"数据列名: {header}\")\nprint(f\"数据记录数: {len(lines)}\")\nprint(f\"采样频率: 每10分钟一次\")\nprint(f\"时间跨度: 2009-2016年，约{len(lines) / (6 * 24 * 365):.1f}年\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 数据解析与预处理\n\n将原始CSV数据转换为NumPy数组，方便后续处理。",
   "id": "53302abc832bcb4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:23.718161Z",
     "start_time": "2025-09-24T10:30:23.024621Z"
    }
   },
   "cell_type": "code",
   "source": "# 将数据转换为浮点数数组\n# 跳过第一列（日期时间），只保留数值列\nfloat_data = np.zeros((len(lines), len(header) - 1))\n\nfor i, line in enumerate(lines):\n    # 分割每行，跳过日期时间列，提取数值\n    values = [float(x) for x in line.split(\",\")[1:]]\n    float_data[i, :] = values\n\nprint(f\"数据矩阵形状: {float_data.shape}\")\nprint(f\"特征数量: {float_data.shape[1]}\")\nprint(f\"时间步数: {float_data.shape[0]}\")",
   "id": "1a7dcadc6cd676e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:23.723759Z",
     "start_time": "2025-09-24T10:30:23.722575Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3309d452a7d2d384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. 数据可视化分析\n\n可视化温度变化趋势，帮助理解数据特征。",
   "id": "3b5a641442b951a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:24.025175Z",
     "start_time": "2025-09-24T10:30:23.767003Z"
    }
   },
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# 温度数据在第2列（索引为1）\ntemp = float_data[:, 1]\n\n# 绘制完整时间序列\nplt.figure(figsize=(15, 5))\nplt.plot(range(len(temp)), temp, linewidth=0.5)\nplt.title(\"Temperature Time Series (2009-2016)\", fontsize=14, pad=15)\nplt.xlabel(\"Time Steps (10-minute intervals)\", fontsize=12)\nplt.ylabel(\"Temperature (°C)\", fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "id": "4314ae5bfe27371b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:24.071893Z",
     "start_time": "2025-09-24T10:30:24.030741Z"
    }
   },
   "cell_type": "code",
   "source": "# 绘制前10天的温度数据（1天 = 6 * 24 = 144个时间步）\n# 10天 = 1440个时间步\nplt.figure(figsize=(15, 5))\nplt.plot(range(1440), temp[:1440], linewidth=1.5)\nplt.title(\"Temperature Time Series (First 10 Days)\", fontsize=14, pad=15)\nplt.xlabel(\"Time Steps (10-minute intervals)\", fontsize=12)\nplt.ylabel(\"Temperature (°C)\", fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "id": "963fa717d1206e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. 数据标准化\n\n使用Z-score标准化方法对数据进行归一化处理。\n\n**注意**: \n- 只使用训练集（前200,000个样本）的统计量进行标准化\n- 这样可以避免数据泄露，确保模型不会利用验证集和测试集的信息",
   "id": "d727c2d9e6522259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:24.096201Z",
     "start_time": "2025-09-24T10:30:24.083430Z"
    }
   },
   "cell_type": "code",
   "source": "# 使用训练集的统计量进行标准化\nTRAIN_SIZE = 200000\nmean = float_data[:TRAIN_SIZE].mean(axis=0)\nstd = float_data[:TRAIN_SIZE].std(axis=0)\n\n# 标准化整个数据集\nfloat_data -= mean\nfloat_data /= std\n\nprint(f\"训练集大小: {TRAIN_SIZE}\")\nprint(f\"均值: {mean[:3]}...\")  # 显示前3个特征的均值\nprint(f\"标准差: {std[:3]}...\")  # 显示前3个特征的标准差",
   "id": "181d95b54631c92c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. 时间序列数据生成器\n\n实现一个高效的数据生成器，用于按批次生成时间序列样本。\n\n**生成器的作用**：\n- 避免一次性将所有数据加载到内存\n- 支持数据增强（如随机采样）\n- 提高训练效率",
   "id": "fb42f9cf984563a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:24.132471Z",
     "start_time": "2025-09-24T10:30:24.128367Z"
    }
   },
   "cell_type": "code",
   "source": "def generator(data, lookback, delay, min_index, max_index, \n              shuffle=False, batch_size=128, step=6):\n    \"\"\"\n    时间序列数据生成器\n    \n    Parameters:\n    -----------\n    data : ndarray\n        完整的时间序列数据，形状为 (timesteps, features)\n    lookback : int\n        输入序列的时间窗口长度（回溯多少个时间步）\n    delay : int\n        目标值相对于当前时间的延迟（预测未来多少个时间步）\n    min_index : int\n        数据索引的最小值（用于划分训练/验证/测试集）\n    max_index : int or None\n        数据索引的最大值，None表示使用数据末尾\n    shuffle : bool\n        是否随机采样（训练时True，验证/测试时False）\n    batch_size : int\n        每批次的样本数量\n    step : int\n        时间步采样间隔（用于降采样，节省计算资源）\n        \n    Yields:\n    -------\n    samples : ndarray\n        输入序列，形状为 (batch_size, lookback//step, features)\n    targets : ndarray\n        目标值，形状为 (batch_size,)\n    \"\"\"\n    if max_index is None:\n        max_index = len(data) - delay - 1\n    \n    i = min_index + lookback\n    \n    while True:\n        if shuffle:\n            # 训练时随机采样\n            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n        else:\n            # 验证/测试时顺序采样\n            if i + batch_size >= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n        \n        # 初始化样本和目标数组\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        \n        # 填充数据\n        for j, row in enumerate(rows):\n            indices = range(row - lookback, row, step)\n            samples[j] = data[indices]\n            targets[j] = data[row + delay][1]  # 预测温度（第2列）\n        \n        yield samples, targets",
   "id": "18eb5859fe00aad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. 创建训练、验证和测试数据生成器\n\n**数据集划分**：\n- 训练集: 0 - 200,000 (约4.75年)\n- 验证集: 200,001 - 300,000 (约2.38年)\n- 测试集: 300,001 - 结束 (约2.87年)\n\n**预测任务**：\n- 使用过去10天的数据（lookback=1440，即144×10个时间步）\n- 预测未来24小时后的温度（delay=144，即24小时）\n- 采样间隔为1小时（step=6，即每小时采样一次）",
   "id": "3c6442bb642472f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:24.179933Z",
     "start_time": "2025-09-24T10:30:24.176724Z"
    }
   },
   "cell_type": "code",
   "source": "# 配置参数\nlookback = 1440  # 回溯1440个时间步（10天）\ndelay = 144      # 预测144个时间步后的温度（24小时后）\nstep = 6         # 采样间隔为6（每小时采样一次）\nbatch_size = 128\n\n# 创建训练数据生成器（使用随机采样）\ntrain_gen = generator(\n    float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=200000,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size\n)\n\n# 创建验证数据生成器（顺序采样）\nval_gen = generator(\n    float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=200001,\n    max_index=300000,\n    step=step,\n    batch_size=batch_size\n)\n\n# 创建测试数据生成器（顺序采样）\ntest_gen = generator(\n    float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=300001,\n    max_index=None,\n    step=step,\n    batch_size=batch_size\n)\n\n# 计算验证和测试所需的步数\nval_steps = (300000 - 200001 - lookback) // batch_size\ntest_steps = (len(float_data) - 300001 - lookback) // batch_size\n\nprint(f\"验证步数: {val_steps}\")\nprint(f\"测试步数: {test_steps}\")",
   "id": "16e279490f82838f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T10:40:52.298071Z",
     "start_time": "2025-09-23T10:40:52.296686Z"
    }
   },
   "cell_type": "markdown",
   "source": "## 7. 建立性能基准\n\n在训练复杂模型之前，先建立一个简单的基准模型。\n\n**朴素方法（Naive Method）**：\n- 假设未来24小时的温度等于当前温度\n- 计算平均绝对误差（MAE）作为基准\n- 后续模型需要超越这个基准才有意义",
   "id": "bdfb26c0fbbdfba2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:30:25.389326Z",
     "start_time": "2025-09-24T10:30:24.226216Z"
    }
   },
   "cell_type": "code",
   "source": "def evaluate_naive_method():\n    \"\"\"\n    评估朴素方法的性能\n    \n    朴素方法假设：未来温度 = 当前温度\n    这是最简单的预测策略，作为性能基准\n    \n    Returns:\n    --------\n    float : 平均绝对误差（MAE）\n    \"\"\"\n    batch_maes = []\n    for step in range(val_steps):\n        samples, targets = next(val_gen)\n        # 使用序列最后一个时间步的温度作为预测值\n        preds = samples[:, -1, 1]  # 温度特征在索引1\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    return np.mean(batch_maes)\n\nnaive_mae = evaluate_naive_method()\nprint(f\"朴素方法的MAE: {naive_mae:.4f}\")\nprint(f\"转换为摄氏度: {naive_mae * std[1]:.2f}°C\")",
   "id": "8f04ece330ac8616",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. 全连接网络（Baseline）\n\n使用简单的全连接网络作为第一个学习模型。\n\n**模型架构**：\n- Flatten层：将3D输入展平为2D\n- Dense层：32个神经元，ReLU激活\n- Dense层：1个输出神经元（温度预测）\n\n**特点**：\n- 不考虑时间顺序，只是简单的模式匹配\n- 作为深度学习的基准对比",
   "id": "8913027174d58b0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:31:11.076455Z",
     "start_time": "2025-09-24T10:30:27.012814Z"
    }
   },
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建全连接网络模型\nmodel = Sequential([\n    layers.Input(shape=(lookback // step, float_data.shape[-1])),\n    layers.Flatten(),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1)\n], name='DenseNet_Baseline')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型（使用较少的epochs进行测试）\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=200,\n    epochs=20,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "53a34d2dd5db693e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:31:11.147069Z",
     "start_time": "2025-09-24T10:31:11.080821Z"
    }
   },
   "cell_type": "code",
   "source": "def plot_training_history(history, title='Training and Validation Loss', ylim=(0, 0.9)):\n    \"\"\"\n    绘制训练和验证损失曲线\n    \n    Parameters:\n    -----------\n    history : History object\n        Keras训练历史对象\n    title : str\n        图表标题\n    ylim : tuple\n        Y轴范围\n    \"\"\"\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss) + 1)\n    \n    plt.figure(figsize=(12, 5))\n    plt.plot(epochs, loss, 'bo-', label='Training loss', alpha=0.7)\n    plt.plot(epochs, val_loss, 'rs-', label='Validation loss', alpha=0.7)\n    plt.title(title, fontsize=14, pad=15)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Loss (MAE)', fontsize=12)\n    plt.legend(fontsize=11)\n    plt.ylim(ylim)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# 绘制全连接网络的训练历史\nplot_training_history(history, title='Dense Network: Training and Validation Loss')",
   "id": "94c4512dab1d0aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. GRU循环网络\n\nGRU（Gated Recurrent Unit）是一种循环神经网络，能够捕捉时间序列中的长期依赖关系。\n\n**模型架构**：\n- GRU层：32个单元\n- Dense层：1个输出神经元\n\n**优势**：\n- 能够学习时间模式和趋势\n- 比全连接网络更适合序列数据\n- 参数量相对较少，训练效率高",
   "id": "9f1be0c28f080208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:31:16.873251Z",
     "start_time": "2025-09-24T10:31:11.152847Z"
    }
   },
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建GRU模型\nmodel = Sequential([\n    layers.Input(shape=(None, float_data.shape[-1])),\n    layers.GRU(32),\n    layers.Dense(1)\n], name='GRU_Model')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=500,\n    epochs=20,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "f353c3e50804fa5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# 绘制GRU模型的训练历史\nplot_training_history(history, title='GRU Model: Training and Validation Loss')",
   "id": "1d484d9e63d79930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建带Dropout的GRU模型\nmodel = Sequential([\n    layers.Input(shape=(None, float_data.shape[-1])),\n    layers.GRU(32, dropout=0.1, recurrent_dropout=0.2),\n    layers.Dense(1)\n], name='GRU_Dropout_Model')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=500,\n    epochs=20,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "ce2158de413389ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "iunwjiteoel",
   "source": "## 10. 使用Dropout正则化的GRU模型\n\n为了降低过拟合，引入Dropout正则化技术。\n\n**Dropout类型**：\n- **dropout**: 输入单元的dropout率\n- **recurrent_dropout**: 循环连接的dropout率\n\n**效果**：\n- 防止模型过度依赖某些特征\n- 提高模型的泛化能力",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# 绘制带Dropout的GRU模型训练历史\nplot_training_history(history, title='GRU with Dropout: Training and Validation Loss')",
   "id": "a5f01f9262c6771a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建堆叠GRU模型\nmodel = Sequential([\n    layers.Input(shape=(None, float_data.shape[-1])),\n    layers.GRU(32, dropout=0.1, recurrent_dropout=0.2, return_sequences=True),\n    layers.GRU(64, dropout=0.1, recurrent_dropout=0.5, activation='relu'),\n    layers.Dense(1)\n], name='Stacked_GRU_Model')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=500,\n    epochs=40,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "6d94f9f2de5b7813",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "xk4juz6hs2",
   "source": "## 11. 堆叠GRU网络\n\n通过堆叠多层GRU来增强模型的表示能力。\n\n**模型架构**：\n- GRU层1：32个单元，return_sequences=True（返回完整序列）\n- GRU层2：64个单元\n- Dense层：1个输出神经元\n\n**关键点**：\n- 第一层GRU需要设置`return_sequences=True`以传递序列给下一层\n- 更深的网络可以学习更复杂的模式\n- 增加dropout防止过拟合",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# 绘制堆叠GRU模型训练历史\nplot_training_history(history, title='Stacked GRU Model: Training and Validation Loss')",
   "id": "ff671f159ff23968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:43:11.447191Z",
     "start_time": "2025-09-24T10:43:11.286857Z"
    }
   },
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建LSTM模型\nmodel = Sequential([\n    layers.Input(shape=(None, float_data.shape[-1])),\n    layers.LSTM(32, dropout=0.1, recurrent_dropout=0.2),\n    layers.Dense(1)\n], name='LSTM_Model')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=500,\n    epochs=20,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "b0751878fb30ea24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "yurk3qeq2ne",
   "source": "## 12. LSTM网络用于温度预测\n\nLSTM（Long Short-Term Memory）是另一种强大的循环神经网络，专门设计用于处理长期依赖问题。\n\n**LSTM vs GRU**：\n- LSTM有更复杂的门控机制（输入门、遗忘门、输出门）\n- GRU更简单，参数更少，训练更快\n- 在某些任务上LSTM可能表现更好，需要实验对比\n\n**模型架构**：\n- LSTM层：32个单元\n- Dense层：1个输出神经元",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "outputs": [],
   "execution_count": null,
   "source": "## 13. 双向GRU网络\n\n双向RNN能够同时从两个方向处理序列：从过去到未来，以及从未来到过去。\n\n**双向网络的优势**：\n- 对于某些模式，未来的信息也很重要\n- 能够捕获更全面的时间依赖关系\n- 在序列标注、翻译等任务中效果显著\n\n**注意**：\n- 双向网络不能用于实时预测（需要看到完整序列）\n- 参数量是单向网络的两倍\n- 训练时间更长\n\n**模型架构**：\n- 双向GRU层：32个单元（正向和反向各32个）\n- Dense层：1个输出神经元",
   "id": "d17250897f4a6d78"
  },
  {
   "cell_type": "code",
   "id": "exiva9efh6t",
   "source": "# 绘制LSTM模型训练历史\nplot_training_history(history, title='LSTM Model: Training and Validation Loss')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T10:51:27.411762Z",
     "start_time": "2025-09-24T10:51:09.520309Z"
    }
   },
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop\n\n# 构建双向GRU模型\nmodel = Sequential([\n    layers.Input(shape=(None, float_data.shape[-1])),\n    layers.Bidirectional(layers.GRU(32, dropout=0.1, recurrent_dropout=0.2)),\n    layers.Dense(1)\n], name='Bidirectional_GRU_Model')\n\n# 编译模型\nmodel.compile(\n    optimizer=RMSprop(learning_rate=0.001),\n    loss='mae',\n    metrics=['mae']\n)\n\n# 显示模型结构\nmodel.summary()\n\n# 训练模型\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=500,\n    epochs=40,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    verbose=1\n)",
   "id": "47b221541b8434e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 绘制双向GRU模型训练历史\nplot_training_history(history, title='Bidirectional GRU Model: Training and Validation Loss')",
   "id": "c762f2b85960892d"
  },
  {
   "cell_type": "markdown",
   "id": "vxw138mucrh",
   "source": "## 14. 总结与最佳实践\n\n### 模型性能对比\n\n通过本notebook，我们实现并比较了多种时间序列预测模型：\n\n1. **朴素方法**: 基准模型，MAE约为0.34\n2. **全连接网络**: 简单的深度学习baseline\n3. **GRU网络**: 单层循环网络\n4. **GRU + Dropout**: 添加正则化，减少过拟合\n5. **堆叠GRU**: 多层网络，增强表示能力\n6. **LSTM网络**: 另一种循环网络架构\n7. **双向GRU**: 同时利用过去和未来信息\n\n### 关键要点\n\n**数据预处理**：\n- 使用训练集的统计量进行标准化，避免数据泄露\n- 合理划分训练/验证/测试集\n- 使用生成器处理大规模数据\n\n**模型设计**：\n- 从简单模型开始，逐步增加复杂度\n- 使用Dropout防止过拟合\n- 堆叠多层可以提升性能，但要注意过拟合风险\n- 双向网络适合离线分析，不适合实时预测\n\n**训练技巧**：\n- 监控验证集性能，及时停止训练\n- 使用合适的学习率\n- 根据任务选择合适的批次大小\n\n### 进一步改进方向\n\n1. **早停机制**: 使用EarlyStopping回调防止过拟合\n2. **学习率调度**: 动态调整学习率\n3. **更多特征工程**: 添加时间特征（星期、月份等）\n4. **注意力机制**: 让模型关注重要的时间步\n5. **集成方法**: 结合多个模型的预测结果",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}