{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 主成分分析（PCA）\n",
    "\n",
    "## 概述\n",
    "\n",
    "主成分分析（Principal Component Analysis, PCA）是最常用的线性降维方法，通过正交变换将原始特征映射到一组线性不相关的新特征上，这些新特征称为主成分。\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "- **几何视角**：在所有 $k$ 维线性子空间中，寻找使投影后方差最大的子空间\n",
    "- **统计视角**：在均值为0的数据上，PCA寻找协方差矩阵的主特征向量\n",
    "\n",
    "$$\\Sigma = \\frac{1}{m} X^\\top X, \\quad \\Sigma u_i = \\lambda_i u_i$$\n",
    "\n",
    "其中 $u_i$ 为第 $i$ 个主成分方向，$\\lambda_i$ 为对应的方差（特征值）\n",
    "\n",
    "## 本节内容\n",
    "\n",
    "1. PCA 基本使用\n",
    "2. 解释方差比与成分选择\n",
    "3. 累计方差可视化\n",
    "4. 降维效果可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 设置随机种子确保可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理\n",
    "\n",
    "使用经典的鸢尾花（Iris）数据集进行演示：\n",
    "- 150个样本，4个特征\n",
    "- 3个类别：setosa, versicolor, virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"数据集形状: {X.shape}\")\n",
    "print(f\"特征名称: {iris.feature_names}\")\n",
    "print(f\"类别名称: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standardize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化处理（PCA对数据尺度敏感，标准化是必要的预处理步骤）\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"标准化后均值: {X_scaled.mean(axis=0).round(10)}\")\n",
    "print(f\"标准化后标准差: {X_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca-basic-section",
   "metadata": {},
   "source": [
    "## 3. PCA 基本使用\n",
    "\n",
    "### 3.1 按方差比例选择成分\n",
    "\n",
    "通过设置 `n_components` 为 (0, 1] 之间的浮点数，可以自动选择保留指定比例方差所需的最少主成分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-variance-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 PCA 模型，保留 95% 的方差\n",
    "pca_95 = PCA(n_components=0.95)\n",
    "X_pca_95 = pca_95.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"原始维度: {X_scaled.shape[1]}\")\n",
    "print(f\"降维后维度: {X_pca_95.shape[1]}\")\n",
    "print(f\"保留的成分数: {pca_95.n_components_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explained-variance-section",
   "metadata": {},
   "source": [
    "### 3.2 查看主成分矩阵\n",
    "\n",
    "`pca.components_` 返回主成分矩阵（$W_d^\\top$），每行是一个主成分方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看主成分矩阵\n",
    "print(\"主成分矩阵形状:\", pca_95.components_.shape)\n",
    "print(\"\\n主成分矩阵:\")\n",
    "print(pca_95.components_)\n",
    "\n",
    "# 展示各特征对主成分的贡献\n",
    "print(\"\\n各特征对主成分的贡献:\")\n",
    "for i, component in enumerate(pca_95.components_):\n",
    "    print(f\"\\n主成分 {i+1}:\")\n",
    "    for feature_name, weight in zip(iris.feature_names, component):\n",
    "        print(f\"  {feature_name}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variance-ratio-section",
   "metadata": {},
   "source": [
    "## 4. 解释方差比\n",
    "\n",
    "解释方差比（Explained Variance Ratio）表示每个主成分所解释的数据方差占总方差的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variance-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先对完整数据进行 PCA，查看所有成分的方差贡献\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "print(\"各主成分的解释方差比:\")\n",
    "for i, ratio in enumerate(pca_full.explained_variance_ratio_):\n",
    "    print(f\"  主成分 {i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n总解释方差比: {pca_full.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cumsum-section",
   "metadata": {},
   "source": [
    "## 5. 累计方差可视化\n",
    "\n",
    "累计解释方差曲线用于确定保留多少主成分可以保留足够的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cumsum-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算累计方差\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# 找到满足 95% 方差的最小成分数\n",
    "d_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# 绘制累计方差曲线\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 左图：单独方差贡献\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "        pca_full.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Individual Explained Variance')\n",
    "ax1.set_xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "\n",
    "# 右图：累计方差\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, len(cumsum) + 1), cumsum, 'o-', color='steelblue', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=0.95, color='red', linestyle='--', label='95% threshold')\n",
    "ax2.axvline(x=d_95, color='green', linestyle='--', label=f'{d_95} components')\n",
    "ax2.fill_between(range(1, d_95 + 1), 0, cumsum[:d_95], alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Number of Principal Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.set_xticks(range(1, len(cumsum) + 1))\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"保留 95% 方差所需的最少主成分数: {d_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-section",
   "metadata": {},
   "source": [
    "## 6. 降维效果可视化\n",
    "\n",
    "将4维数据降至2维进行可视化，观察类别分布情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降至2维用于可视化\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# 创建可视化\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (target_name, color, marker) in enumerate(zip(iris.target_names, colors, markers)):\n",
    "    mask = y == i\n",
    "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "               c=color, marker=marker, s=60, alpha=0.7,\n",
    "               label=target_name, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "ax.set_title('PCA Projection of Iris Dataset')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"2D投影保留的方差比: {pca_2d.explained_variance_ratio_.sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biplot-section",
   "metadata": {},
   "source": [
    "## 7. 双标图（Biplot）\n",
    "\n",
    "双标图同时展示样本点和特征向量的投影，有助于理解主成分的含义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# 绘制样本点\n",
    "for i, (target_name, color, marker) in enumerate(zip(iris.target_names, colors, markers)):\n",
    "    mask = y == i\n",
    "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
    "               c=color, marker=marker, s=50, alpha=0.6,\n",
    "               label=target_name, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# 绘制特征向量\n",
    "scale = 3  # 缩放因子，便于可视化\n",
    "for i, (feature_name, component) in enumerate(zip(iris.feature_names, pca_2d.components_.T)):\n",
    "    ax.arrow(0, 0, component[0] * scale, component[1] * scale,\n",
    "             head_width=0.1, head_length=0.05, fc='red', ec='red', linewidth=2)\n",
    "    ax.text(component[0] * scale * 1.15, component[1] * scale * 1.15,\n",
    "            feature_name, fontsize=10, color='red', ha='center', va='center')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "ax.set_title('Biplot: PCA of Iris Dataset')\n",
    "ax.legend(loc='upper left')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 8. 总结\n",
    "\n",
    "### 关键要点\n",
    "\n",
    "1. **数据预处理**：PCA对数据尺度敏感，通常需要先进行标准化\n",
    "\n",
    "2. **成分数选择**：\n",
    "   - 指定具体数值：`PCA(n_components=2)`\n",
    "   - 指定方差比例：`PCA(n_components=0.95)`\n",
    "   - 通过累计方差曲线分析\n",
    "\n",
    "3. **重要属性**：\n",
    "   - `components_`：主成分矩阵（投影方向）\n",
    "   - `explained_variance_ratio_`：各成分解释方差比\n",
    "   - `n_components_`：实际使用的成分数\n",
    "\n",
    "4. **应用场景**：\n",
    "   - 数据可视化（降至2D/3D）\n",
    "   - 特征压缩与去噪\n",
    "   - 加速后续模型训练\n",
    "   - 消除多重共线性\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "- PCA是线性方法，对于非线性结构可能效果有限\n",
    "- 主成分是原始特征的线性组合，可解释性降低\n",
    "- 对异常值敏感，考虑使用稳健PCA变体"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
