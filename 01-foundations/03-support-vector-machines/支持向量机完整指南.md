# 第五章深度笔记：支持向量机 (SVM) - 从几何直觉到数学本质

> **核心主题**：大间隔分类、核技巧、凸优化、对偶与 KKT
> **前置知识**：线性代数（向量、范数、投影）、概率与统计、凸优化（拉格朗日乘子法）、核方法

---

## 📋 本章知识图谱

```
支持向量机 (SVM)
├── 核心思想
│   ├── 大间隔分类 (Maximum Margin)
│   ├── 支持向量 (Support Vectors)
│   ├── 间隔与泛化误差界
│   └── 核技巧 (Kernel Trick)
├── 线性SVM (5.1)
│   ├── 硬间隔与软间隔
│   ├── C 参数：正则化强度
│   ├── 损失函数：Hinge / Squared Hinge
│   └── 求解器：Primal / Dual / SGD
├── 非线性SVM (5.2)
│   ├── 显式特征映射与多项式特征
│   ├── 多项式核 (Polynomial)
│   ├── RBF 核 (高斯核) ⭐
│   ├── 其它核：线性、Sigmoid、自定义核
│   └── 核函数性质与选择策略
├── SVM 回归 (5.3)
│   ├── ε-不敏感损失
│   ├── LinearSVR vs SVR
│   └── 应用场景与调参
├── 数学原理 (5.4)
│   ├── 决策函数与几何意义
│   ├── 原始问题与软间隔
│   ├── 对偶问题、拉格朗日乘子与 KKT 条件
│   ├── 支持向量与表示定理
│   └── 核技巧的数学本质
├── 实战与工程细节 (5.5)
│   ├── 多分类策略 (OvR / OvO)
│   ├── 类别不平衡与 class_weight
│   ├── 概率输出与校准 (Platt Scaling)
│   └── 调参与常见陷阱
└── 模型比较与选型 (5.6)
    ├── 与逻辑回归的比较
    ├── 与树模型 / 集成方法比较
    └── 典型适用与不适用场景
```

---

## 第一部分：SVM 的几何直觉与核心思想

### 🎯 两个核心直觉

1. 在类之间拟合尽可能**宽的街道**（大间隔分类，maximum margin），只让少数关键样本（支持向量）决定决策边界。
2. SVM 对**特征缩放高度敏感**，必须在训练前进行合理的标准化或归一化。

### 1. 大间隔分类 (Maximum Margin)

- **问题设置**：在一堆苹果点和一堆香蕉点之间可以画出无数条分割线，SVM 要找的是那条**最鲁棒**的线——它不仅能分开两类，还要让两类样本距离这条线都尽可能“远”。
- **几何图像**：决策边界两侧平行地各向外推一条线，形成一条“街道”。街道越宽，模型对小扰动越不敏感，泛化能力越强。
- **支持向量**：真正“顶住”街道两侧的，是处在边界上的那些样本点，它们就是 **支持向量 (support vectors)**。优化的结果满足：
  - 只有支持向量的拉格朗日乘子 $\alpha_i > 0$；
  - 决策函数可以写成
    $$f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + b.$$
- **泛化直觉**：在统计学习理论里，较大的间隔对应**更小的理论泛化误差上界**（VC 维、margin bound），这也是 SVM 强调“大间隔”的理论依据。

### 2. 对特征缩放敏感

- SVM 的目标与约束都依赖于**欧氏距离与内积**。如果一个特征量级远大于其它特征，它会在距离计算中占据压倒性权重，导致模型几乎只在这一维上做决策。
- 实践中几乎总是对输入做缩放：
  - 密集特征：常用 `StandardScaler`（零均值、单位方差），或 `MinMaxScaler` 映射到 `[0,1]`。
  - 稀疏高维特征（如文本）：常用 `MaxAbsScaler` 或直接使用 TF‑IDF 后的归一化向量。
- 在 `sklearn` 中，推荐使用 `Pipeline` 把缩放和模型绑定，避免数据泄露：

```python
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

svm_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svm_clf", SVC(kernel="linear"))  # 线性 SVM
])

# svm_pipeline.fit(X_train, y_train)
# y_pred = svm_pipeline.predict(X_test)
```

---

## 5.1 线性 SVM 分类

线性 SVM 假设在原始特征空间中存在一个线性可分（或近似可分）的超平面。

### 5.1.0 硬间隔分类 (Hard Margin，理想情形)

在线性**完全可分**时，硬间隔 SVM 的原始优化问题：

$$
\begin{aligned}
\min_{w, b} \quad & \frac{1}{2}\|w\|^2 \\
\text{s.t.} \quad & y_i (w^\top x_i + b) \ge 1,\quad i=1,\dots,m .
\end{aligned}
$$

- 目标：最小化 $\|w\|$，等价于**最大化几何间隔** $M = \frac{2}{\|w\|}$。
- 局限：对噪声点和异常值极其敏感；只要有一点点线性不可分，就不存在可行解。

### 5.1.1 软间隔分类 (Soft Margin) 与 C 参数

为解决现实数据中普遍存在的噪声和少量错标，引入松弛变量 $\zeta_i \ge 0$：

$$
\begin{aligned}
\min_{w, b, \zeta} \quad & \frac{1}{2}\|w\|^2 + C \sum_{i=1}^m \zeta_i \\
\text{s.t.} \quad & y_i (w^\top x_i + b) \ge 1 - \zeta_i, \quad i=1,\dots,m, \\
& \zeta_i \ge 0.
\end{aligned}
$$

- $C$ 控制“宽街道”与“少违规点”之间的权衡：
  - **小 $C$**：强正则化，允许更多间隔违规，街道更宽，欠拟合风险较大但泛化更稳健。
  - **大 $C$**：弱正则化，强烈惩罚违规点，街道变窄甚至过度弯折，容易过拟合。

示例（`LinearSVC`）：

```python
from sklearn.svm import LinearSVC

soft_svm = LinearSVC(C=0.1, loss="hinge", random_state=42)   # 强正则
harder_svm = LinearSVC(C=100, loss="hinge", random_state=42) # 弱正则
```

### 5.1.2 Hinge Loss（合页损失）

合页损失是软间隔 SVM 在经验风险项上的具体形式：

$$
\ell_{\text{hinge}}(y, f(x)) = \max(0, 1 - y f(x)), \quad f(x) = w^\top x + b.
$$

- 若 $y f(x) \ge 1$：点在“街道外”，损失为 0。
- 若 $0 < y f(x) < 1$：点在“街道内但分类正确”，损失随距离边界的远近线性增加。
- 若 $y f(x) \le 0$：点被误分，损失更大。

在 `sklearn` 中：

- `LinearSVC` 默认使用 `squared_hinge`（平方合页损失），梯度更平滑。
- `SGDClassifier(loss="hinge")` 显式使用合页损失，并配合随机梯度下降优化。

### 5.1.3 线性 SVM 的几种实现

1. **`LinearSVC`（liblinear）**
   - 适合**大样本、特征数中等**的场景（如 m > 10^5）。
   - 求解线性 SVM 的原始或对偶问题，训练非常快。
   - 支持 `dual` 参数：`dual=False` 时在 $m \gg n$ 时更高效。

2. **`SGDClassifier(loss="hinge")`**
   - 使用随机梯度下降近似优化目标。
   - 适合**超大规模数据**（out-of-core）和**在线学习**场景：
     ```python
     from sklearn.linear_model import SGDClassifier

     sgd_svm = SGDClassifier(
         loss="hinge",  # 线性 SVM
         penalty="l2",
         alpha=1e-3,    # 约等于 1/C
         max_iter=1000,
         tol=1e-3,
         random_state=42
     )
     # 支持 partial_fit 进行流式增量训练
     ```

3. **`SVC(kernel="linear")`（libsvm）**
   - 通过核 SVM 的通用求解器实现线性核，训练复杂度通常为 $O(m^2)$ 甚至 $O(m^3)$。
   - 在样本数很大时远慢于 `LinearSVC`，但在中小数据集上表现可靠。

### 5.1.4 偏置项与数据中心化

- `LinearSVC` 默认**同时正则化权重 $w$ 和偏置 $b$**，这在许多模型中并不常见。
- 若特征均值远离 0，偏置项的正则化会带来额外偏差，影响性能。
- 实践上几乎总是先做标准化（零均值、单位方差），再使用 `LinearSVC`。

相比之下，`SVC(kernel="linear")` 默认不正则化偏置，对是否中心化不那么敏感，但仍然需要缩放以平衡各特征的量纲。

### 5.1.5 `dual` 参数与 Primal / Dual 视角

- 线性 SVM 的原始问题变量维度为特征数 $n$，对偶问题变量维度为样本数 $m$。
- 经验规则：
  - 当 **$m \gg n$**（常见）时，设置 `dual=False`，直接解原始问题更高效。
  - 当 **$n \gg m$**（如小样本高维文本）时，解对偶问题更划算，保持 `dual=True`。
- 非线性 `SVC` 必须在对偶空间中求解，因为核技巧需要显式地在对偶形式中出现 $K(x_i, x_j)$。

### 5.1.6 多分类线性 SVM

SVM 本质是**二分类**方法，多分类需要构造组合策略：

- **一对多 (One‑vs‑Rest, OvR)**：
  - 训练 $K$ 个分类器，第 $k$ 个把第 $k$ 类视为正类，其余合并为负类。
  - `LinearSVC` 与 `SGDClassifier` 默认采用 OvR。
- **一对一 (One‑vs‑One, OvO)**：
  - 为每一对类别训练一个二分类器，总共 $K(K-1)/2$ 个。
  - `SVC` 默认使用 OvO，多数投票决定预测类别。
- 选择：
  - 高维稀疏特征 + 大样本：更偏向 `LinearSVC` + OvR。
  - 类别数不太大、样本规模中等：`SVC` + OvO 通常效果很好。

---

## 5.2 非线性 SVM 分类与核方法

当数据在原空间线性不可分时，SVM 通过**升维 + 线性分割**的思路解决问题。

### 5.2.1 显式特征映射 vs 核技巧

- 显式映射：构造高阶多项式特征、交叉特征等，把原始 $x$ 映射到高维空间 $\phi(x)$，在其中训练线性 SVM。
- 问题：特征数在高阶多项式下会指数级爆炸，既耗内存又难以优化。
- 关键观察：SVM 的对偶形式只依赖于**样本之间的内积** $\langle \phi(x_i), \phi(x_j) \rangle$，不需要显式地知道 $\phi(x)$ 本身。
- 因此可以用核函数 $K(x_i, x_j)$ 直接替代内积，这就是**核技巧**。

### 5.2.2 多项式核 (Polynomial Kernel)

- 形式：
  $$
  K(x, z) = (\gamma x^\top z + r)^d,
  $$
  其中 $d$ 是多项式阶数，$\gamma, r$ 为可调参数。
- 直观上等价于在原始特征上自动加入所有不超过 $d$ 阶的多项式组合。
- 适用场景：决策边界形状与多项式关系较为接近时（如抛物线、圆锥分割等）。

示例：

```python
from sklearn.svm import SVC

poly_svm = SVC(kernel="poly", degree=3, coef0=1, C=5)

# 关键参数：
# - degree: 多项式阶数，越大模型越复杂，越容易过拟合
# - coef0: 控制高阶项与低阶项的相对权重
# - C: 控制正则化强度
```

### 5.2.3 RBF 核与“相似度特征”

RBF（径向基函数）核的直观可以理解为“与每个训练样本的相似度”：

1. 将每个训练样本视为一个“地标”。
2. 对于新样本 $x$，计算它与每个地标 $x_i$ 的相似度：
   $$
   K(x, x_i) = \exp(-\gamma \|x - x_i\|^2).
   $$
3. 相似度越高，$x$ 在“特征空间”中越靠近 $x_i$ 对应的方向。
4. 在这种新特征空间中，原本非线性可分的数据往往变得线性可分。

在实现上，`SVC(kernel="rbf")` 并不显式构造这些特征，而是只在对偶形式中使用 $K(x_i, x_j)$，从而避免高维计算。

```python
rbf_svm = SVC(kernel="rbf", gamma=0.1, C=1.0)

# gamma (γ)：控制高斯核的“宽度”
# - γ 小：核函数很“胖”，每个点影响范围大，决策边界更平滑（偏向欠拟合）
# - γ 大：核函数很“窄”，模型可以绕过每个点，决策边界高度弯曲（偏向过拟合）
# C：与线性 SVM 中含义相同，控制对间隔违规的惩罚强度
```

常见经验：`C` 和 `gamma` 通常需要**同时在对数尺度上搜索**（如 $C \in \{10^{-2},\dots,10^3\}$，$\gamma \in \{10^{-4},\dots,10^1\}$）。

### 5.2.4 核函数的一般性质与选择

- 理论上，一个函数 $K(x,z)$ 要成为合法核函数，需要是**对称正定核**（对应某个 Hilbert 空间中的内积），这与 Mercer 定理相关。
- 常见核：
  - 线性核：`kernel="linear"`。
  - 多项式核：`kernel="poly"`。
  - RBF 核：`kernel="rbf"`。
  - Sigmoid 核：`kernel="sigmoid"`（类似于两层神经网络的激活，但在实践中不如 RBF 稳定）。
- 可以通过自定义函数或组合现有核（加和、乘积）构造更复杂的核，只要整体仍保持正定性。

### 5.2.5 核函数的选择策略（实用版）

1. **先试线性核**：
   - 样本数大、特征维度高（文本、稀疏特征）：首选 `LinearSVC`。
2. **数据规模中等时试 RBF 核**：
   - 若线性核欠拟合且数据量不大（如 $m < 10^5$），`SVC(kernel="rbf")` 是“瑞士军刀”。
3. **有明显多项式结构时试多项式核**：
   - 如物理建模中已知变量间是低阶多项式关系。
4. **谨慎使用 Sigmoid 核**：
   - 参数敏感、数值不稳定，通常只有在特定理论背景下才使用。

---

## 5.3 SVM 回归 (SVR)

### 5.3.1 ε-不敏感损失

SVR 将“大间隔”思想应用于回归问题，通过 **ε‑不敏感损失** 定义“街道”：

$$
\ell_\epsilon(y, f(x)) =
\begin{cases}
0, & |y - f(x)| \le \epsilon, \\
|y - f(x)| - \epsilon, & \text{否则}.
\end{cases}
$$

- 预测值在 $[y-\epsilon, y+\epsilon]$ 区间内不计入损失。
- 只有超出“街道”宽度的误差才被线性惩罚。

### 5.3.2 线性 SVR 与核 SVR

```python
from sklearn.svm import LinearSVR, SVR

# 线性 SVR：适合大样本、特征数中等的线性回归任务
svr_linear = LinearSVR(epsilon=1.5, C=1.0, random_state=42)

# 非线性 SVR：使用 RBF 核拟合复杂非线性关系
svr_rbf = SVR(kernel="rbf", C=10.0, gamma="auto", epsilon=0.1)
```

- `LinearSVR`：复杂度与线性 SVM 类似，可扩展到大规模数据。
- `SVR`：与 `SVC` 类似，训练复杂度约为 $O(m^2)$，更适合中小规模数据。

### 5.3.3 典型应用

- 时间序列的短期预测（需注意不要打乱时间顺序做交叉验证）。
- 具有中等特征维度、样本量不大的非线性回归问题。
- 对异常值不太敏感的场景（通过调节 $C$ 和 $\epsilon$ 控制鲁棒性）。

---

## 5.4 工作原理：原始问题、对偶问题与核技巧

### 5.4.1 决策函数与间隔

- 线性 SVM 的决策函数：
  $$
  f(x) = w^\top x + b.
  $$
- 预测规则：
  - $\hat{y} = \mathrm{sign}(f(x))$。
- 函数间隔与几何间隔：
  - 函数间隔：$\gamma_i = y_i f(x_i)$。
  - 几何间隔：$\hat{\gamma}_i = \dfrac{y_i f(x_i)}{\|w\|}$。
- SVM 的目标是**最大化最小几何间隔**，在软间隔情形下则是在最大间隔与有限错误之间折中。

在 `sklearn` 中，可以使用 `decision_function` 获得 $f(x)$：

```python
scores = svm_pipeline.decision_function(X_test)
# scores 的绝对值越大，模型“信心”越高
```

### 5.4.2 原始问题回顾：硬间隔与软间隔

硬间隔原始问题前面已经给出，这里再强调软间隔形式：

$$
\begin{aligned}
\min_{w, b, \zeta} \quad & \frac{1}{2}\|w\|^2 + C \sum_{i=1}^m \zeta_i \\
\text{s.t.} \quad & y_i (w^\top x_i + b) \ge 1 - \zeta_i, \\
& \zeta_i \ge 0,\quad i=1,\dots,m.
\end{aligned}
$$

- 这是一个标准的**凸二次规划（QP）**问题。
- 直接在原空间中求解时，优化变量为 $w \in \mathbb{R}^n$、$b \in \mathbb{R}$ 以及 $\zeta \in \mathbb{R}^m$。

### 5.4.3 对偶问题、拉格朗日乘子与 KKT 条件

引入拉格朗日乘子 $\alpha_i \ge 0$（对应间隔约束）和 $\mu_i \ge 0$（对应 $\zeta_i \ge 0$），构造拉格朗日函数并消去原始变量，可得到对偶问题：

$$
\begin{aligned}
\max_{\alpha} \quad &
\sum_{i=1}^m \alpha_i
- \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j) \\
\text{s.t.} \quad &
0 \le \alpha_i \le C,\quad i=1,\dots,m, \\
& \sum_{i=1}^m \alpha_i y_i = 0 .
\end{aligned}
$$

关键结论：

- **表示定理（Representer Theorem）**：
  - 最优解 $w^\star$ 总可以写成
    $$
    w^\star = \sum_{i=1}^m \alpha_i^\star y_i \phi(x_i),
    $$
    其中 $\phi$ 是隐式特征映射。
  - 决策函数：
    $$
    f(x) = \sum_{i=1}^m \alpha_i^\star y_i K(x_i, x) + b^\star.
    $$
- **支持向量的判别**：
  - 若 $\alpha_i^\star = 0$：该样本对 $w$ 没有贡献，不是支持向量。
  - 若 $0 < \alpha_i^\star < C$：样本恰好在“街道边界”上，是典型支持向量。
  - 若 $\alpha_i^\star = C$：样本为“难点”或“异常点”，在街道内甚至被误分。
- **KKT 条件**（直观版）：
  - 最优解同时满足原始约束、对偶约束和互补松弛条件；
  - 这些条件解释了为什么只有少数样本（支持向量）会对决策边界产生影响。

### 5.4.4 核技巧的数学本质

- 在线性 SVM 的对偶形式中，$x$ 只以内积 $\langle x_i, x_j \rangle$ 的形式出现。
- 若用非线性映射 $\phi(x)$ 将数据送入高维特征空间，则对偶形式只依赖于 $\langle \phi(x_i), \phi(x_j) \rangle$。
- 核函数 $K(x_i, x_j)$ 正是这个内积：
  $$
  K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle .
  $$
- 优点：
  - 无需显式计算 $\phi(x)$，尤其是当 $\phi$ 对应的空间维度极高甚至无限维（如 RBF 核）时；
  - 只要给定满足正定性的核函数，就隐式定义了一个特征空间和其内积。

### 5.4.5 SMO 算法与复杂度

- **SMO（Sequential Minimal Optimization）** 是求解 SVM 对偶问题的经典算法：
  - 每次只选取一对拉格朗日乘子 $(\alpha_i, \alpha_j)$，在保持其它 $\alpha_k$ 固定的情况下对这两个变量做解析更新；
  - 利用约束 $\sum_i \alpha_i y_i = 0$，可以将问题降到二维上，保证每步更新都有封闭解。
- `libsvm`（`SVC` 的后端）使用的是 SMO 及其变种。
- 复杂度：
  - 训练时间通常介于 $O(m^2)$ 与 $O(m^3)$ 之间，与支持向量数量高度相关；
  - 内存复杂度约为 $O(m^2)$，因为需要存储核矩阵或其近似。
- 这也是为什么在大样本场景下更推荐线性 SVM（`LinearSVC` 或 `SGDClassifier`）。

---

## 5.5 实战与工程细节

### 5.5.1 多分类策略总结

- `LinearSVC` 与 `SGDClassifier`：
  - 使用一对多 (OvR) 策略；
  - `coef_` 形状为 `(n_classes, n_features)`，便于分析每个类别的线性权重。
- `SVC`：
  - 默认使用一对一 (OvO)；
  - `decision_function_shape` 可设为 `"ovr"` 以改用一对多形式。

### 5.5.2 类别不平衡与 class_weight

- 类别严重不平衡时，SVM 容易偏向多数类。
- 处理方式：
  - 在 `LinearSVC`、`SVC`、`SGDClassifier` 中使用 `class_weight="balanced"`；
  - 或手动指定各类别权重，如 `{0: 1.0, 1: 10.0}`。
- 本质上是对不同类别使用不同的 $C$，增加少数类样本的“话语权”。

### 5.5.3 概率输出与校准

- 标准 SVM 输出的是决策函数值 $f(x)$，不是概率。
- 若需要概率：
  - `SVC(probability=True)`：内部采用 Platt Scaling（在决策函数上再拟合一个逻辑回归），训练会额外做交叉验证，速度明显变慢。
  - 对 `LinearSVC` / `SGDClassifier`，推荐使用 `CalibratedClassifierCV` 对 `decision_function` 的输出进行后验校准。
- 在许多只需要排序或阈值的任务（如排序学习、异常检测）中，直接使用 $f(x)$ 即可，无需概率。

### 5.5.4 调参与常见陷阱

- 调参建议：
  - 线性 SVM：重点调 `C`，通常在对数尺度上搜索（如 $10^{-4}$ 到 $10^3$）。
  - RBF SVM：同时调 `C` 与 `gamma`。
- 常见陷阱：
  - **忘记缩放特征**：导致训练困难或边界极不合理。
  - **在大样本上使用核 SVM**：训练时间和内存难以承受，应优先尝试线性 SVM 或树模型。
  - **没有在交叉验证中一起调参和缩放**：应使用 `Pipeline` 确保缩放参数只在训练 Fold 上拟合，避免数据泄露。
  - **忽视类别不平衡**：准确率看似很高，但召回率极差，需检查各类指标。

---

## 5.6 模型比较与选型

### 5.6.1 与逻辑回归的比较

- 相似之处：
  - 都是**线性分类器**；
  - 都在特征空间中学习一个超平面。
- 差异：
  - 损失函数不同：SVM 使用 hinge / squared hinge，逻辑回归使用 log-loss；
  - 逻辑回归天然输出**概率**，参数易解释；SVM 更偏向“间隔最大化”，更注重决策边界的位置。
- 经验：
  - 高维稀疏特征、样本很多时，逻辑回归往往是更简单稳健的起点；
  - 样本量中等、噪声不太大时，线性或核 SVM 有时能略优。

### 5.6.2 与树模型 / 集成方法比较

- 树模型（决策树、随机森林、梯度提升树）：
  - 优点：对特征缩放不敏感，可处理离散特征和复杂非线性关系；
  - 缺点：在高维稀疏空间（文本、推荐系统）中往往不如线性模型和 SVM。
- 经验：
  - 中低维结构化数据：树模型和提升方法（如 XGBoost、LightGBM）往往更强；
  - 高维稀疏数据或需要凸优化保证的场景（如某些安全关键任务）：线性 SVM / 逻辑回归是首选。

### 5.6.3 不同场景下的典型选择

- 文本分类、大规模线性可分任务：`LinearSVC` 或 `SGDClassifier(loss="hinge")`。
- 中小规模、边界明显非线性的任务：`SVC(kernel="rbf")`。
- 强调概率输出与可解释性：逻辑回归或概率校准后的 SVM。
- 表达能力极强、数据量很大：深度神经网络往往优于核 SVM。

---

## 总结

- **几何层面**：SVM 追求的是“最大间隔”的超平面，只有少量支持向量真正决定边界。
- **优化层面**：通过凸二次规划与对偶问题，保证了全局最优解，并为核技巧提供了入口。
- **核方法层面**：通过核函数在隐式高维空间中构造线性分类器，从而实现强大的非线性建模能力。
- **工程层面**：线性 SVM 适合大规模高维数据，核 SVM 适合中小数据的复杂边界建模；合理的特征缩放、调参和多分类策略是成功应用的关键。

