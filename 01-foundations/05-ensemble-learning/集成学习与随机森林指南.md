# ç¬¬ä¸ƒç« æ·±åº¦ç¬”è®°ï¼šé›†æˆå­¦ä¹ ä¸éšæœºæ£®æ—

> **æ ¸å¿ƒä¸»é¢˜**ï¼šåå·®â€“æ–¹å·®åˆ†è§£ã€Bagging / Pastingã€éšæœºæ£®æ—ã€Boostingã€Stacking  
> **å‰ç½®çŸ¥è¯†**ï¼šå†³ç­–æ ‘ã€åŸºæœ¬æ¦‚ç‡ä¸æœŸæœ›ã€äº¤å‰éªŒè¯

---

## ğŸ“‹ æœ¬ç« çŸ¥è¯†å›¾è°±

```
é›†æˆå­¦ä¹ 
â”œâ”€â”€ é›†æˆæ€æƒ³
â”‚   â”œâ”€â”€ åå·®â€“æ–¹å·®ä¸â€œå¤šæ•°è¡¨å†³â€
â”‚   â””â”€â”€ åå·®å‹ vs æ–¹å·®å‹åŸºæ¨¡å‹
â”œâ”€â”€ Bagging / Pasting
â”‚   â”œâ”€â”€ è‡ªåŠ©é‡‡æ · (Bootstrap)
â”‚   â”œâ”€â”€ Out-of-Bag è¯„ä¼°
â”‚   â””â”€â”€ éšæœºè¡¥ä¸ / éšæœºå­ç©ºé—´
â”œâ”€â”€ éšæœºæ£®æ—
â”‚   â”œâ”€â”€ æ ·æœ¬é‡‡æ · + ç‰¹å¾é‡‡æ ·
â”‚   â”œâ”€â”€ Extra Trees (æç«¯éšæœºæ ‘)
â”‚   â””â”€â”€ ç‰¹å¾é‡è¦æ€§ä¸ OOB åˆ†æ•°
â”œâ”€â”€ Boosting
â”‚   â”œâ”€â”€ AdaBoost (æ ·æœ¬åŠ æƒ)
â”‚   â”œâ”€â”€ æ¢¯åº¦æå‡æ ‘ (GBDT)
â”‚   â””â”€â”€ XGBoost ç­‰é«˜æ•ˆå®ç°
â””â”€â”€ æ¨¡å‹ç»„åˆ
    â”œâ”€â”€ Voting (ç¡¬/è½¯æŠ•ç¥¨)
    â””â”€â”€ Stacking (å…ƒå­¦ä¹ å™¨)
```

---

## 1. é›†æˆå­¦ä¹ çš„åŠ¨æœºä¸åå·®â€“æ–¹å·®

å•ä¸€æ¨¡å‹é€šå¸¸åœ¨ä»¥ä¸‹ä¸¤ä¸ªæç«¯ä¹‹é—´æƒè¡¡ï¼š

- **é«˜åå·®**ï¼šæ¨¡å‹è¿‡äºç®€å•ï¼ˆå¦‚æµ…çº¿æ€§æ¨¡å‹ï¼‰ï¼Œéš¾ä»¥æ•è·æ•°æ®ç»“æ„ï¼›
- **é«˜æ–¹å·®**ï¼šæ¨¡å‹è¿‡äºå¤æ‚ï¼ˆå¦‚æ·±å†³ç­–æ ‘ï¼‰ï¼Œæ˜“å¯¹è®­ç»ƒæ•°æ®æ‹Ÿåˆè¿‡åº¦ã€‚

é›†æˆå­¦ä¹ é€šè¿‡ç»„åˆå¤šä¸ªâ€œå¼±ä½†äº’è¡¥â€çš„æ¨¡å‹ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹èƒ½ï¼š

- é™ä½æ–¹å·®ï¼ˆBagging / éšæœºæ£®æ—ï¼‰ï¼›
- æˆ–é™ä½åå·®ï¼ˆBoostingï¼‰ï¼›
- æˆ–é€šè¿‡ç»¼åˆä¸åŒå½’çº³åå¥½æå‡é²æ£’æ€§ï¼ˆVoting / Stackingï¼‰ã€‚

---

## 2. Bagging ä¸ Pasting

### 2.1 åŸºæœ¬æ€æƒ³

- å¯¹åŒä¸€ç§åŸºæ¨¡å‹ï¼ˆå¸¸ç”¨å†³ç­–æ ‘ï¼‰ï¼š
  - ä»è®­ç»ƒé›†ä¸­æŠ½å–å¤šä¸ªæ ·æœ¬å­é›†ï¼Œè®­ç»ƒå¤šä¸ªåŸºæ¨¡å‹ï¼›
  - å¯¹åˆ†ç±»é—®é¢˜ç”¨æŠ•ç¥¨ï¼Œå¯¹å›å½’é—®é¢˜ç”¨å¹³å‡ï¼›
- æ ¹æ®æ˜¯å¦**æœ‰æ”¾å›é‡‡æ ·**ï¼š
  - **Bagging**ï¼šè‡ªåŠ©é‡‡æ · (bootstrap)ï¼ŒåŒä¸€æ ·æœ¬å¯åœ¨å¤šä¸ªå­é›†ä¸­é‡å¤å‡ºç°ï¼›
  - **Pasting**ï¼šæ— æ”¾å›é‡‡æ ·ï¼Œæ¯ä¸ªæ ·æœ¬æœ€å¤šåªå±äºä¸€ä¸ªå­é›†ã€‚

Bagging çš„é¢å¤–éšæœºæ€§æ›´æœ‰åŠ©äºé™ä½æ–¹å·®ï¼Œé€šå¸¸ç•¥ä¼˜äº Pastingã€‚

### 2.2 `BaggingClassifier` å…³é”®ç”¨æ³•

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(
        max_depth=None,
        random_state=42
    ),
    n_estimators=200,
    max_samples=1.0,        # ä»¥è®­ç»ƒé›†å¤§å°ä¸ºåŸºå‡†
    bootstrap=True,         # True = Bagging, False = Pasting
    n_jobs=-1,
    oob_score=True,         # å¯ç”¨ Out-of-Bag è¯„ä¼°
    random_state=42
)

bag_clf.fit(X_train, y_train)
print("OOB score:", bag_clf.oob_score_)
```

### 2.3 éšæœºè¡¥ä¸ä¸éšæœºå­ç©ºé—´

- éšæœºè¡¥ä¸ (Random Patches)ï¼š
  - åŒæ—¶å¯¹å­æ ·æœ¬ (rows) ä¸å­ç‰¹å¾ (columns) æŠ½æ ·ï¼›
  - é€‚ç”¨äºæ ·æœ¬æ•°å’Œç‰¹å¾æ•°éƒ½å¾ˆå¤§çš„æƒ…å½¢ã€‚
- éšæœºå­ç©ºé—´ (Random Subspaces)ï¼š
  - ä¿ç•™å…¨éƒ¨æ ·æœ¬ï¼Œä»…å¯¹å­ç‰¹å¾æŠ½æ ·ï¼›
  - é€‚ç”¨äºç»´åº¦æé«˜è€Œæ ·æœ¬æœ‰é™çš„æƒ…å½¢ã€‚

åœ¨ `BaggingClassifier` ä¸­é€šè¿‡ `max_samples`ã€`max_features`ã€`bootstrap_features` ç­‰å‚æ•°æ§åˆ¶ã€‚

---

## 3. éšæœºæ£®æ—ä¸æç«¯éšæœºæ ‘

### 3.1 éšæœºæ£®æ—ï¼šBagging + ç‰¹å¾å­é‡‡æ ·

éšæœºæ£®æ— = Bagging(å†³ç­–æ ‘) + åœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ†è£‚æ—¶éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾ï¼š

- è®­ç»ƒæ¯æ£µæ ‘æ—¶é‡‡æ ·ä¸åŒè®­ç»ƒæ ·æœ¬ï¼ˆbootstrapï¼‰ï¼›
- æ ‘çš„æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹ï¼Œä»…åœ¨éšæœºé€‰å‡ºçš„ç‰¹å¾å­é›†ä¸­å¯»æ‰¾æœ€ä½³åˆ†è£‚ï¼›
- å¤§å¤§æå‡æ ‘ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°é™ä½æ–¹å·®ã€‚

å…¸å‹ç”¨æ³•ï¼š

```python
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(
    n_estimators=500,
    max_depth=None,
    max_features="sqrt",  # åˆ†ç±»é—®é¢˜å¸¸ç”¨ 'sqrt'ï¼Œå›å½’å¸¸ç”¨ 'auto' æˆ– 'log2'
    n_jobs=-1,
    oob_score=True,
    random_state=42
)
rf_clf.fit(X_train, y_train)
print("OOB score:", rf_clf.oob_score_)
```

**ä¼˜ç‚¹**ï¼š

- å‡ ä¹ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾ï¼›
- å¯¹å¼‚å¸¸å€¼ä¸ç±»åˆ«ä¸å¹³è¡¡ç›¸å¯¹é²æ£’ï¼›
- æä¾›ç‰¹å¾é‡è¦æ€§ä¼°è®¡ï¼›
- â€œå¼€ç®±å³ç”¨â€ï¼šå°‘é‡è°ƒå‚å³å¯å¾—åˆ°å¼ºåŸºçº¿ã€‚

### 3.2 æç«¯éšæœºæ ‘ (Extra Trees)

- ä¸éšæœºæ£®æ—çš„åŒºåˆ«ï¼š
  - éšæœºæ£®æ—ï¼šåœ¨ç»™å®šç‰¹å¾å­é›†å†…ï¼Œå¯»æ‰¾æœ€ä½³åˆ’åˆ†é˜ˆå€¼ï¼›
  - Extra Treesï¼šåœ¨ç»™å®šç‰¹å¾å­é›†å†…ï¼Œé˜ˆå€¼ä¹Ÿéšæœºé‡‡æ ·ï¼Œå†ä»è¿™äº›éšæœºé˜ˆå€¼ä¸­é€‰æœ€ä¼˜ã€‚
- ç»“æœï¼šåå·®ç•¥é«˜ï¼Œä½†æ–¹å·®æ›´ä½ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

```python
from sklearn.ensemble import ExtraTreesClassifier

extra_clf = ExtraTreesClassifier(
    n_estimators=500,
    max_depth=None,
    max_features="sqrt",
    n_jobs=-1,
    random_state=42
)
extra_clf.fit(X_train, y_train)
```

### 3.3 ç‰¹å¾é‡è¦æ€§

éšæœºæ£®æ—å’Œ Extra Trees æä¾›åŸºäº**å¹³å‡ä¸çº¯åº¦å‡å°‘**çš„ç‰¹å¾é‡è¦æ€§ï¼š

```python
import numpy as np

importances = rf_clf.feature_importances_
indices = np.argsort(importances)[::-1]

for idx in indices[:20]:
    print(f"{feature_names[idx]}: {importances[idx]:.4f}")
```

æ³¨æ„ï¼šä¸çº¯åº¦å‡å°‘å‹é‡è¦æ€§å€¾å‘äºé«˜åŸºæ•°ç‰¹å¾ï¼Œå®é™…åº”ç”¨ä¸­å¯é…åˆ**Permutation Importance** è¿›è¡ŒéªŒè¯ã€‚

---

## 4. Boostingï¼šä¸²è¡Œè¯¯å·®ä¿®æ­£

### 4.1 AdaBoost

æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æ ·æœ¬æƒé‡èšç„¦å‰ä¸€è½®æ¨¡å‹éš¾ä»¥æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬ã€‚

- åˆå§‹æƒé‡ç›¸ç­‰ï¼›
- æ¯è½®è®­ç»ƒå¼±å­¦ä¹ å™¨ï¼Œå¢åŠ è¢«è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œå‡å°è¢«æ­£ç¡®åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼›
- ä½¿ç”¨åŠ æƒæŠ•ç¥¨å¾—åˆ°æœ€ç»ˆé¢„æµ‹ã€‚

å…¸å‹ç”¨æ³•ï¼ˆä»¥å†³ç­–æ ‘æ¡©ä¸ºåŸºå­¦ä¹ å™¨ï¼‰ï¼š

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada_clf = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=200,
    learning_rate=0.5,
    algorithm="SAMME.R",
    random_state=42
)
ada_clf.fit(X_train, y_train)
```

æ³¨æ„ï¼š

- å¯¹å¼‚å¸¸å€¼è¾ƒæ•æ„Ÿï¼ˆå®¹æ˜“åå¤â€œæ­»ç£•â€å™ªå£°ç‚¹ï¼‰ï¼›
- éœ€ç»“åˆæ—©åœä¸äº¤å‰éªŒè¯æ§åˆ¶ `n_estimators`ã€‚

### 4.2 æ¢¯åº¦æå‡æ ‘ (Gradient Boosting)

ä»¥å›å½’æ ‘ä¸ºåŸºå­¦ä¹ å™¨ï¼Œé€è½®æ‹Ÿåˆå‰ä¸€è½®çš„æ®‹å·®ï¼š

- åˆå§‹æ¨¡å‹ $F_0(x)$ ä¸ºå¸¸æ•°ï¼ˆå¦‚ç›®æ ‡å‡å€¼ï¼‰ï¼›
- ç¬¬ $m$ è½®å­¦ä¹ ä¸€ä¸ªæ ‘ $h_m(x)$ï¼Œæ‹Ÿåˆå½“å‰æ®‹å·®ï¼›
- æ›´æ–°æ¨¡å‹ $F_m(x) = F_{m-1}(x) + \eta h_m(x)$ï¼Œå…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ ç‡ã€‚

Scikitâ€‘learn ä¸­çš„å…¸å‹ç”¨æ³•ï¼š

```python
from sklearn.ensemble import GradientBoostingRegressor

gbrt = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gbrt.fit(X_train, y_train)
```

è¦ç‚¹ï¼š

- `learning_rate` ä¸ `n_estimators` ä¹‹é—´å­˜åœ¨å¼ºè€¦åˆï¼šå­¦ä¹ ç‡è¶Šå°ï¼Œæ‰€éœ€è¿­ä»£è½®æ•°è¶Šå¤šï¼›
- å¸¸ç”¨ç­–ç•¥ï¼šå°å­¦ä¹ ç‡ + è¾ƒå¤šè¿­ä»£ + æ—©åœã€‚

### 4.3 XGBoost / LightGBMï¼ˆæ¡†æ¶çº§å®ç°ï¼‰

- åœ¨ GBDT æ€æƒ³ä¸Šå¼•å…¥ï¼š
  - æ›´ç²¾ç»†çš„æ­£åˆ™åŒ–ï¼ˆå¶èŠ‚ç‚¹ L1/L2 æƒ©ç½šï¼‰ï¼›
  - æ›´é«˜æ•ˆçš„æ ‘æ„å»ºä¸å¹¶è¡ŒåŒ–ï¼›
  - åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼ä¸ç±»åˆ«ç‰¹å¾ï¼ˆå…·ä½“å–å†³äºæ¡†æ¶ï¼‰ã€‚
- å¸¸ä½œä¸ºè¡¨æ ¼æ•°æ®ä»»åŠ¡çš„**æ€§èƒ½ä¸Šé™åŸºçº¿**ï¼Œä½†è°ƒå‚ä¸éƒ¨ç½²æˆæœ¬è¾ƒé«˜ã€‚

---

## 5. Voting ä¸ Stackingï¼šæ¨¡å‹ç»„åˆ

### 5.1 Votingï¼šç®€å•å¤šæ•°è¡¨å†³

- ç¡¬æŠ•ç¥¨ (hard voting)ï¼šç›´æ¥å¯¹é¢„æµ‹æ ‡ç­¾æŠ•ç¥¨ï¼›
- è½¯æŠ•ç¥¨ (soft voting)ï¼šå¯¹é¢„æµ‹æ¦‚ç‡æ±‚å¹³å‡åé€‰å–æ¦‚ç‡æœ€å¤§ç±»åˆ«ï¼Œé€šå¸¸æ›´ç¨³å¥ã€‚

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf = VotingClassifier(
    estimators=[
        ("lr", LogisticRegression(max_iter=1000)),
        ("svc", SVC(probability=True)),
        ("tree", DecisionTreeClassifier(max_depth=5))
    ],
    voting="soft"
)
voting_clf.fit(X_train, y_train)
```

é€‚åˆåœºæ™¯ï¼šå·²æœ‰å¤šä¸ªå•æ¨¡å‹åŸºçº¿ï¼Œæƒ³ç”¨æå°‘æ”¹åŠ¨è·å¾—â€œå…è´¹åŠ æˆâ€ã€‚

### 5.2 Stackingï¼šå­¦ä¹ å¦‚ä½•ç»„åˆæ¨¡å‹

ä¸¤å±‚ç»“æ„ï¼š

1. ç¬¬ä¸€å±‚ï¼šå¤šä¸ªåŸºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šé€šè¿‡äº¤å‰éªŒè¯ç”Ÿæˆ**åŒ…å¤–é¢„æµ‹**ï¼›
2. ç¬¬äºŒå±‚ï¼šä»¥è¿™äº›é¢„æµ‹ä½œä¸ºç‰¹å¾è®­ç»ƒä¸€ä¸ªå…ƒæ¨¡å‹ã€‚

Scikitâ€‘learn ç”¨æ³•ï¼š

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

base_estimators = [
    ("rf", RandomForestClassifier(n_estimators=200, random_state=42)),
    ("gb", GradientBoostingRegressor()),  # è‹¥ä¸ºå›å½’åˆ™ä½¿ç”¨ StackingRegressor
]

stack_clf = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(max_iter=1000),
    cv=5
)
stack_clf.fit(X_train, y_train)
```

æ³¨æ„ï¼š

- å¿…é¡»é€šè¿‡äº¤å‰éªŒè¯ç”Ÿæˆç¬¬ä¸€å±‚çš„è®­ç»ƒè¾“å…¥ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼›
- åœ¨å®è·µä¸­åº”é™åˆ¶åŸºæ¨¡å‹æ•°é‡å’Œå¤æ‚åº¦ï¼Œé¿å…è¿‡åº¦è†¨èƒ€ã€‚

---

## 6. å®éªŒå»ºè®®

åœ¨æœ¬ç« çš„å„ä¸ª Notebookï¼ˆBagging / RandomForest / ExtraTrees / GBRT / XGBoost / Vote Classifierï¼‰ä¸­ï¼Œå¯ä»¥è®¾è®¡å¦‚ä¸‹ç³»ç»Ÿå®éªŒï¼š

1. **å†³ç­–æ ‘ vs éšæœºæ£®æ—**ï¼š  
   - åœ¨åŒä¸€æ•°æ®é›†ä¸Šæ¯”è¾ƒå•æ ‘ä¸éšæœºæ£®æ—çš„è®­ç»ƒ/æµ‹è¯•æ€§èƒ½ï¼›  
   - è§‚å¯Ÿæ·±åº¦å˜åŒ–å¯¹ä¸¤è€…è¿‡æ‹Ÿåˆè¡Œä¸ºçš„å½±å“ã€‚

2. **Bagging å‚æ•°æ•æ„Ÿæ€§**ï¼š  
   - å›ºå®šåŸºå­¦ä¹ å™¨ï¼Œé€æ­¥å¢åŠ  `n_estimators`ï¼›  
   - è§‚å¯Ÿæ€§èƒ½ä¸æ–¹å·®ï¼ˆäº¤å‰éªŒè¯æ–¹å·®ï¼‰å¦‚ä½•æ”¶æ•›ã€‚

3. **RandomForest vs ExtraTrees**ï¼š  
   - åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„æ•°æ®é›†ä¸Šæ¯”è¾ƒä¸¤è€…æ€§èƒ½ä¸è®­ç»ƒæ—¶é—´ï¼›  
   - åˆ†æåå·®â€“æ–¹å·®å·®å¼‚ã€‚

4. **AdaBoost vs GBDT**ï¼š  
   - ç”¨æµ…æ ‘ (`max_depth=1` vs `max_depth=3`) æ¯”è¾ƒ AdaBoost ä¸ GBDTï¼›  
   - è§‚å¯Ÿåœ¨å™ªå£°è¾ƒå¤šä¸è¾ƒå°‘çš„æ•°æ®é›†ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚

5. **ç®€å• Voting ä¸ Stacking**ï¼š  
   - ç”¨é€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€çº¿æ€§ SVM æ„å»º Voting é›†æˆï¼›  
   - å†ç”¨ Stacking ç»„åˆåŒä¸€ç»„åŸºæ¨¡å‹ï¼Œæ¯”è¾ƒæ€§èƒ½å·®å¼‚ä¸è®­ç»ƒè¿ç»´æˆæœ¬ã€‚

é€šè¿‡ä¸Šè¿°å®éªŒï¼Œä½ å¯ä»¥æŠŠâ€œé›†æˆå­¦ä¹ æ˜¯é€šè¿‡å¯æ§çš„éšæœºæ€§ä¸æ¨¡å‹ç»„åˆåœ¨åå·®â€“æ–¹å·®ç©ºé—´ä¸­å¯»ä¼˜â€è¿™ä¸€æ ¸å¿ƒæ€æƒ³çœŸæ­£æŒæ¡ã€‚

