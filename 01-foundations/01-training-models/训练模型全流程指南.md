# 第四章深度笔记：训练模型 —— 从线性回归到正则化与优化

> **核心主题**：线性模型、凸优化、梯度下降族算法、偏差–方差权衡、正则化
> **前置知识**：线性代数（矩阵运算、范数）、微积分（偏导、梯度）、概率统计（高斯分布、最大似然）

---

## 📋 本章知识图谱

```
训练模型
├── 线性回归 (4.1)
│   ├── 最小二乘与极大似然
│   ├── 标准方程 (Normal Equation)
│   ├── SVD 与伪逆 (Pseudo‑Inverse)
│   └── 何时用解析解，何时放弃
├── 梯度下降 (4.2)
│   ├── 批量 GD / 随机 GD / Mini‑Batch
│   ├── 特征缩放的重要性
│   ├── 学习率与学习率调度
│   └── 线性模型中的偏差–方差直觉
├── 多项式回归 (4.3)
│   ├── 显式特征映射
│   ├── 特征组合爆炸与数值问题
│   └── 配合正则化控制复杂度
├── 学习曲线 (4.4)
│   ├── 高偏差 vs 高方差的可视化诊断
│   └── 数据量、模型复杂度与正则化的协同作用
└── 正则化线性模型 (4.5)
    ├── Ridge (L2) – 默认选择 ⭐
    ├── Lasso (L1) – 稀疏与特征选择
    ├── Elastic Net – 折中与稳定性
    └── Early Stopping – 迭代算法中的隐式正则化
```

---

## 4.1 线性回归：最小二乘的几何与统计视角

### 4.1.1 模型、损失与极大似然

- 模型假设（多变量线性回归）：
  $$
  \hat{y} = h_\theta(x) = \theta^\top x = \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n.
  $$
- 矩阵形式：
  $$
  \hat{\mathbf{y}} = X \theta,\quad X \in \mathbb{R}^{m\times (n+1)}.
  $$
- 均方误差 (MSE) 损失：
  $$
  J(\theta) = \frac{1}{2m}\|X\theta - y\|_2^2.
  $$

**统计含义**（误差高斯假设）：

若假设
$$
y^{(i)} = \theta^\top x^{(i)} + \varepsilon^{(i)},\quad
\varepsilon^{(i)} \sim \mathcal{N}(0,\sigma^2), \text{独立同分布},
$$
则最大化似然等价于最小化 MSE。  
因此最小二乘线性回归不仅“方便”，还有明确的概率意义。

### 4.1.2 标准方程 (Normal Equation)

对 MSE 求导并令梯度为 0：

$$
\nabla_\theta J(\theta) = \frac{1}{m} X^\top(X\theta - y) = 0,
$$
得到正规方程：

$$
X^\top X \theta = X^\top y,\quad
\theta = (X^\top X)^{-1} X^\top y.
$$

**特点与复杂度**：

- 解析解，一步到位，无需学习率、迭代次数。
- 时间复杂度由矩阵求逆主导，约为 $O(n^3)$（更精细地说 $O(n^{2.4}) \sim O(n^3)$）。
- 对样本数 $m$ 相对不敏感，但对特征数 $n$ 极度敏感：
  - $n \lesssim 10^3$：可行；
  - $n \gtrsim 10^4$：通常不可行。

**何时 $X^\top X$ 不可逆或病态？**

1. 特征线性相关（多重共线性）。
2. 特征数 $n$ 大于样本数 $m$。
3. 特征尺度差异巨大，导致条件数很大，数值不稳定。

可用条件数粗略检查：

```python
import numpy as np

cond_number = np.linalg.cond(X.T @ X)
if cond_number > 1e10:
    print("⚠️ X^T X 条件数过大，考虑使用 SVD 或加入正则化")
```

### 4.1.3 SVD 与伪逆：数值稳定的解法

奇异值分解：

$$
X = U \Sigma V^\top,
$$
其中 $U,V$ 正交，$\Sigma$ 为奇异值对角矩阵。

线性回归解可写为 Moore–Penrose 伪逆：

$$
X^+ = V \Sigma^+ U^\top,\quad
\theta = X^+ y.
$$

**优点**：

- 避免直接计算 $(X^\top X)^{-1}$，数值稳定性更好。
- 即使 $X^\top X$ 不可逆，也能给出最小二乘意义下的最小范数解。
- 奇异值谱揭示了特征方向的重要性，可用于降维（截断 SVD）。

实战中可直接使用 `np.linalg.lstsq` 或 `LinearRegression`（内部基于 SVD / QR）：

```python
import numpy as np
from sklearn.linear_model import LinearRegression

theta_svd, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)
print("矩阵秩:", rank, "奇异值:", s)

lin_reg = LinearRegression()  # 默认使用稳健的数值方法
lin_reg.fit(X, y)
```

**复杂度对比（粗略）**：

| 方法           | 复杂度         | 更适合的场景             |
|----------------|----------------|--------------------------|
| 标准方程       | $O(n^3)$       | $n \le 10^3$             |
| SVD / lstsq    | $O(mn^2)$      | $n \le 10^4$，更稳定     |
| 梯度下降族算法 | $O(kmn)$       | $n$ 很大或数据流式到达   |

---

## 4.2 梯度下降：迭代优化与特征缩放

### 4.2.1 梯度下降的统一视角

- 一般更新公式：
  $$
  \theta^{(t+1)} = \theta^{(t)} - \eta_t \nabla_\theta J(\theta^{(t)}),
  $$
  其中 $\eta_t$ 为第 $t$ 步学习率。
- 对线性回归：
  $$
  \nabla_\theta J(\theta) = \frac{1}{m} X^\top (X\theta - y).
  $$

**为什么线性回归要用梯度下降？**

- 当 $n$ 很大（例如 $10^5$ 以上）时，$O(n^3)$ 的解析解无法承受；
+- 当模型不是简单的线性回归（例如逻辑回归、神经网络）时，本身就没有解析解；
- 当需要**在线学习 / 流式学习**时，必须要能增量更新参数。

### 4.2.2 特征缩放：收敛速度的“优化前提”

若不同特征尺度差异巨大，则损失函数在参数空间的等高线呈细长椭圆，梯度下降会呈“之”字形缓慢收敛。  
解决方案就是对输入特征做缩放。

常见方法：

| 方法            | 公式                                             | 适用场景                |
|-----------------|--------------------------------------------------|-------------------------|
| StandardScaler  | $x' = \dfrac{x - \mu}{\sigma}$                   | 近似高斯分布、常规默认  |
| MinMaxScaler    | $x' = \dfrac{x - x_{\min}}{x_{\max}-x_{\min}}$   | 需要 [0,1] 范围         |
| RobustScaler    | $x' = \dfrac{x - Q_{50}}{Q_{75}-Q_{25}}$         | 有大量离群点时          |

使用时务必 **只在训练集上 `fit`，在验证/测试集上仅 `transform`**：

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

### 4.2.3 批量 / 随机 / Mini‑Batch 梯度下降

1. **批量梯度下降 (Batch GD)**  
   每次迭代使用全部样本计算精确梯度，复杂度 $O(mn)$，对大数据不友好，但在小数据、凸问题上收敛稳定。

2. **随机梯度下降 (SGD)**  
   每次只用一个样本更新参数，单步成本 $O(n)$，噪声大但速度快，可做在线学习，常配合学习率衰减：
   $$
   \eta_t = \frac{\eta_0}{1 + \gamma t}.
   $$

3. **Mini‑Batch 梯度下降** ⭐  
   每次使用大小为 $b$ 的小批量（如 32、64、128），综合了 Batch 的稳定与 SGD 的效率，是深度学习和大规模训练的主力方法。

常见经验：

- 小规模、凸优化问题：Batch 或小批量 GD 足够。
- 大规模数据 / 线上场景：SGD 或 Mini‑Batch + 学习率衰减。

### 4.2.4 学习率与学习率调度

- 学习率过大：可能导致发散或在最优点附近来回震荡。
- 学习率过小：收敛极慢，训练时间长。

典型策略：

- 固定学习率（小规模问题）。
- 分段衰减：训练若干 epoch 后将学习率乘以常数因子（如 0.1）。
- 反比例衰减：$\eta_t = \eta_0 / (1 + \gamma t)$。

在 `sklearn.linear_model.SGDRegressor` / `SGDClassifier` 中：

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(
    loss="squared_error",
    penalty="l2",
    alpha=1e-4,          # 正则化强度 ~ 1/C
    learning_rate="invscaling",
    eta0=0.01,
    random_state=42
)
```

---

## 4.3 多项式回归：用线性模型拟合非线性

### 4.3.1 核心思想：显式特征映射

对标量特征 $x$，考虑多项式：

$$
y \approx \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_d x^d.
$$

仍然是**对参数 $\theta$ 线性**的模型，只是我们在输入空间做了非线性变换：

$$
\phi(x) = [x, x^2, \dots, x^d]^\top.
$$

这允许线性回归拟合出弯曲的决策边界。

### 4.3.2 特征组合爆炸与数值问题

若有 $n$ 个原始特征，构造到 $d$ 阶的多项式特征，维度约为

$$
\binom{n+d}{d},
$$

增长非常快。例如：

- $n=10, d=5 \Rightarrow 1001$ 个特征；
- $n=100, d=3 \Rightarrow 176,851$ 个特征。

过多的高阶特征会带来：

- 极高的计算与存储成本；
- 严重的过拟合风险；
- 数值稳定性问题（特征尺度差异巨大）。

因此，多项式回归几乎总是要搭配**特征缩放 + 正则化**使用。

### 4.3.3 实战：Pipeline + PolynomialFeatures

```python
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

degree = 2

poly_reg = Pipeline([
    ("poly_features", PolynomialFeatures(degree=degree, include_bias=False)),
    ("scaler", StandardScaler()),
    ("lin_reg", LinearRegression())
])

poly_reg.fit(X_train, y_train)
y_pred = poly_reg.predict(X_test)
```

调参重点：

- `degree`：模型复杂度的主要控制量。
- 建议使用交叉验证选择合适的阶数，并结合学习曲线和验证集误差检查是否过拟合。

---

## 4.4 学习曲线：诊断欠拟合与过拟合

### 4.4.1 定义与用法

学习曲线通常绘制：

- x 轴：训练集大小（例如用 10%、20%、…、100% 的数据训练模型）。
- y 轴：对应训练集上的误差和验证集上的误差。

它是判断**模型容量、数据量、正则化强度**是否合适的关键工具。

### 4.4.2 高偏差 (High Bias) 场景

特征：

- 训练误差高；
- 验证误差也高；
- 两条曲线相距不大。

说明模型即使用完全部训练数据也“学不动”——模型太简单。

应对策略：

- 增加模型复杂度（更高阶多项式、更强模型）；
- 减小正则化强度；
- 添加有信息量的新特征。

单纯增加数据量通常**帮助有限**。

### 4.4.3 高方差 (High Variance) 场景

特征：

- 训练误差很低；
- 验证误差明显更高；
- 两条曲线之间存在明显间隙。

说明模型在训练集上过拟合。

应对策略：

- 增加训练数据（最根本的办法）；
- 降低模型复杂度（降低 `degree`、减少特征）；
- 增强正则化（增大 `alpha` 或使用更强的正则化形式）。

---

## 4.5 正则化线性模型：偏差–方差的控制阀

### 4.5.1 统一视角：惩罚项与约束形式

- 正则化的目标：控制模型容量，抑制过拟合。
- 典型形式（惩罚形式）：
  $$
  \min_\theta \frac{1}{2m}\|X\theta-y\|_2^2 + \lambda \,\Omega(\theta),
  $$
  其中 $\Omega(\theta)$ 是参数的某种范数（通常不惩罚截距）。
- 与之等价的约束形式：
  $$
  \min_\theta \frac{1}{2m}\|X\theta-y\|_2^2 \quad
  \text{s.t.}\, \Omega(\theta) \le C.
  $$

参数 $\lambda$（或库中常用的 `alpha`）控制“拟合数据 vs 保持解简单”之间的权衡。

### 4.5.2 岭回归 (Ridge Regression, L2 正则)

- 惩罚项：
  $$
  \Omega(\theta) = \frac{1}{2}\sum_{j=1}^n \theta_j^2 = \frac{1}{2}\|\theta_{1:n}\|_2^2.
  $$
- 闭式解：
  $$
  \theta = (X^\top X + \lambda I)^{-1} X^\top y.
  $$
- 好处：
  - 保证矩阵可逆，即便原本存在共线性；
  - 均匀缩小各特征权重，避免某些权重极大；
  - 数学上对应在高斯先验下的最大后验估计 (MAP)。

实战代码：

```python
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

ridge_reg = Pipeline([
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=1.0, solver="cholesky"))
])

ridge_reg.fit(X_train, y_train)
```

### 4.5.3 Lasso 回归 (Lasso Regression, L1 正则)

- 惩罚项：
  $$
  \Omega(\theta) = \sum_{j=1}^n |\theta_j|.
  $$
- 效果：
  - 倾向于将一部分权重压到**精确的 0**，产生稀疏解；
  - 等价于在拉普拉斯先验下的 MAP 估计。

优点：

- 自动特征选择：无用特征的权重被置零。
- 模型可解释性好。

缺点：

- 高度相关特征之间“择一其一”，不够稳定；
- 当 $n \gg m$ 时可能表现不佳。

实战代码：

```python
from sklearn.linear_model import Lasso

lasso_reg = Pipeline([
    ("scaler", StandardScaler()),
    ("lasso", Lasso(alpha=0.1))
])

lasso_reg.fit(X_train, y_train)
coeffs = lasso_reg.named_steps["lasso"].coef_
```

### 4.5.4 弹性网络 (Elastic Net)

结合 L1 与 L2 正则：

$$
J(\theta) = \frac{1}{2m}\|X\theta-y\|_2^2 +
\lambda\left( \rho\|\theta\|_1 + \frac{1-\rho}{2}\|\theta\|_2^2 \right),
$$

其中 $\rho = \text{l1_ratio} \in [0,1]$ 控制 L1 与 L2 的权重。

- $\rho = 1$：退化为 Lasso；
- $\rho = 0$：退化为 Ridge。

与 Lasso 相比，Elastic Net 在特征高度相关或 $n \gg m$ 时更稳定，是实践中常用折中方案。

```python
from sklearn.linear_model import ElasticNet

elastic_reg = Pipeline([
    ("scaler", StandardScaler()),
    ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5))
])

elastic_reg.fit(X_train, y_train)
```

### 4.5.5 提前停止 (Early Stopping)：隐式正则化

对于使用梯度下降族算法训练的模型（包括线性模型、神经网络），**训练轮数本身就成为一个正则化参数**。

典型现象：

- 训练误差随迭代单调下降；
- 验证误差先下降后上升（过拟合开始）。

策略：

- 在验证误差达到最小值附近提前终止训练；
- 保留对应迭代的参数，丢弃后续迭代结果。

在一定条件下，提前停止的效果与 L2 正则非常接近（可视为一种“时间维度上的正则化”）。

`SGDRegressor` 中可直接启用：

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    penalty="l2",
    alpha=1e-4,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=5,
    random_state=42
)

sgd_reg.fit(X_train_scaled, y_train)
```

---

## 4.6 与后续章节的衔接：从线性模型到广义线性模型与深度学习

- 本章的所有思想——**损失函数、梯度下降、特征缩放、学习曲线、正则化**——在后续模型（逻辑回归、SVM、神经网络）中都会反复出现；
- 线性回归的 MSE + L2 正则，是理解“广义线性模型 + 正则化”最重要的起点；
- 梯度下降族与提前停止，是理解深度学习优化与泛化的桥梁。

掌握本章内容，可以把后续更复杂模型都看作：

> “换一个损失函数 + 换一个模型结构，但仍然在做：  
>  **最小化损失 + 加上合适的正则化，用梯度下降族优化**。”

